{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5e9993-618c-4449-a90a-cdba5a81dfd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run it once on Llama-index Docs directory\n",
    "# only keep .md and .rst files, and remove files of other types\n",
    "import os\n",
    "\n",
    "dir_path = './lidocs'\n",
    "for root, dirs, files in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        if file.endswith(('.md', '.rst')):\n",
    "            continue\n",
    "        else:\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56be021-21a8-4028-a2de-378ed51e5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dda7fcc-16b5-4272-9165-5ccdb318763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n",
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 194\n",
      "> [SimpleDirectoryReader] Total files added: 194\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"./lidocs\", recursive=True)\n",
    "lidocs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6174ce-44f1-4860-9526-33aa333e2949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Documentation Guide...\n",
      "> Adding chunk: Documentation Guide...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A guide for docs contributors\n",
      "\n",
      "The `docs` direc...\n",
      "> Adding chunk: A guide for docs contributors\n",
      "\n",
      "The `docs` direc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Build Docs\n",
      "\n",
      "If you haven't already, clone the L...\n",
      "> Adding chunk: Build Docs\n",
      "\n",
      "If you haven't already, clone the L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Watch Docs\n",
      "\n",
      "We recommend using sphinx-autobuild...\n",
      "> Adding chunk: Watch Docs\n",
      "\n",
      "We recommend using sphinx-autobuild...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "Callbacks\n",
      "=================\n",
      "\n",
      ".. ...\n",
      "> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "Callbacks\n",
      "=================\n",
      "\n",
      ".. ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Composability:\n",
      "\n",
      "Composability\n",
      "=========...\n",
      "> Adding chunk: .. _Ref-Composability:\n",
      "\n",
      "Composability\n",
      "=========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Example-Notebooks:\n",
      "\n",
      "Example Notebooks\n",
      "=...\n",
      "> Adding chunk: .. _Ref-Example-Notebooks:\n",
      "\n",
      "Example Notebooks\n",
      "=...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Finetuning:\n",
      "\n",
      "Finetuning\n",
      "=============\n",
      "\n",
      "...\n",
      "> Adding chunk: .. _Ref-Finetuning:\n",
      "\n",
      "Finetuning\n",
      "=============\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-API_Reference:\n",
      "\n",
      "API Reference\n",
      "=========...\n",
      "> Adding chunk: .. _Ref-API_Reference:\n",
      "\n",
      "API Reference\n",
      "=========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-Empty:\n",
      "\n",
      "Empty Index\n",
      "===========...\n",
      "> Adding chunk: .. _Ref-Indices-Empty:\n",
      "\n",
      "Empty Index\n",
      "===========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-Knowledge-Graph:\n",
      "\n",
      "Knowledge Gra...\n",
      "> Adding chunk: .. _Ref-Indices-Knowledge-Graph:\n",
      "\n",
      "Knowledge Gra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-List:\n",
      "\n",
      "List Index\n",
      "==========\n",
      "\n",
      "B...\n",
      "> Adding chunk: .. _Ref-Indices-List:\n",
      "\n",
      "List Index\n",
      "==========\n",
      "\n",
      "B...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-StructStore:\n",
      "\n",
      "Structured Store ...\n",
      "> Adding chunk: .. _Ref-Indices-StructStore:\n",
      "\n",
      "Structured Store ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-Table:\n",
      "\n",
      "Table Index\n",
      "===========...\n",
      "> Adding chunk: .. _Ref-Indices-Table:\n",
      "\n",
      "Table Index\n",
      "===========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-Tree:\n",
      "\n",
      "Tree Index\n",
      "==========\n",
      "\n",
      "B...\n",
      "> Adding chunk: .. _Ref-Indices-Tree:\n",
      "\n",
      "Tree Index\n",
      "==========\n",
      "\n",
      "B...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-VectorStore:\n",
      "\n",
      "Vector Store Inde...\n",
      "> Adding chunk: .. _Ref-Indices-VectorStore:\n",
      "\n",
      "Vector Store Inde...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices:\n",
      "\n",
      "Indices\n",
      "=======\n",
      "\n",
      "This doc sho...\n",
      "> Adding chunk: .. _Ref-Indices:\n",
      "\n",
      "Indices\n",
      "=======\n",
      "\n",
      "This doc sho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Langchain-Integrations:\n",
      "\n",
      "Langchain Inte...\n",
      "> Adding chunk: .. _Ref-Langchain-Integrations:\n",
      "\n",
      "Langchain Inte...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "LLM Predictors\n",
      "=================...\n",
      "> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "LLM Predictors\n",
      "=================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Anthropic\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "> Adding chunk: Anthropic\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Azure OpenAI\n",
      "============\n",
      "\n",
      ".. autopydantic_mode...\n",
      "> Adding chunk: Azure OpenAI\n",
      "============\n",
      "\n",
      ".. autopydantic_mode...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: HuggingFaceLLM\n",
      "==============\n",
      "\n",
      ".. autopydantic_...\n",
      "> Adding chunk: HuggingFaceLLM\n",
      "==============\n",
      "\n",
      ".. autopydantic_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LangChainLLM\n",
      "============\n",
      "\n",
      ".. autopydantic_mode...\n",
      "> Adding chunk: LangChainLLM\n",
      "============\n",
      "\n",
      ".. autopydantic_mode...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LlamaCPP\n",
      "========\n",
      "\n",
      ".. autopydantic_model:: llam...\n",
      "> Adding chunk: LlamaCPP\n",
      "========\n",
      "\n",
      ".. autopydantic_model:: llam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OpenAI\n",
      "======\n",
      "\n",
      ".. autopydantic_model:: llama_in...\n",
      "> Adding chunk: OpenAI\n",
      "======\n",
      "\n",
      ".. autopydantic_model:: llama_in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: PaLM\n",
      "====\n",
      "\n",
      ".. autopydantic_model:: llama_index....\n",
      "> Adding chunk: PaLM\n",
      "====\n",
      "\n",
      ".. autopydantic_model:: llama_index....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Predibase\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "> Adding chunk: Predibase\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Replicate\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "> Adding chunk: Replicate\n",
      "=========\n",
      "\n",
      ".. autopydantic_model:: ll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: XOrbits Xinference\n",
      "==================\n",
      "\n",
      ".. autop...\n",
      "> Adding chunk: XOrbits Xinference\n",
      "==================\n",
      "\n",
      ".. autop...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-LLMs:\n",
      "\n",
      "\n",
      "LLMs\n",
      "====\n",
      "\n",
      "A large language mod...\n",
      "> Adding chunk: .. _Ref-LLMs:\n",
      "\n",
      "\n",
      "LLMs\n",
      "====\n",
      "\n",
      "A large language mod...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Memory\n",
      "\n",
      "Memory\n",
      "======\n",
      "\n",
      ".. automodule:: ...\n",
      "> Adding chunk: .. _Ref-Memory\n",
      "\n",
      "Memory\n",
      "======\n",
      "\n",
      ".. automodule:: ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "Node\n",
      "=================\n",
      "\n",
      ".. autom...\n",
      "> Adding chunk: .. _Ref-Node:\n",
      "\n",
      "Node\n",
      "=================\n",
      "\n",
      ".. autom...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Node-Postprocessor:\n",
      "\n",
      "Node Postprocessor...\n",
      "> Adding chunk: .. _Ref-Node-Postprocessor:\n",
      "\n",
      "Node Postprocessor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Playground:\n",
      "\n",
      "Playground\n",
      "===============...\n",
      "> Adding chunk: .. _Ref-Playground:\n",
      "\n",
      "Playground\n",
      "===============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Prompt-Templates:\n",
      "\n",
      "Prompt Templates\n",
      "=======...\n",
      "> Adding chunk: .. _Prompt-Templates:\n",
      "\n",
      "Prompt Templates\n",
      "=======...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Condense Question Chat Engine\n",
      "=================...\n",
      "> Adding chunk: Condense Question Chat Engine\n",
      "=================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Simple Chat Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "> Adding chunk: Simple Chat Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Chat-Engines:\n",
      "\n",
      "Chat Engines\n",
      "===========...\n",
      "> Adding chunk: .. _Ref-Chat-Engines:\n",
      "\n",
      "Chat Engines\n",
      "===========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Bundle\n",
      "============\n",
      "\n",
      ".. automodule:: llam...\n",
      "> Adding chunk: Query Bundle\n",
      "============\n",
      "\n",
      ".. automodule:: llam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Citation Query Engine\n",
      "=======================\n",
      "\n",
      "...\n",
      "> Adding chunk: Citation Query Engine\n",
      "=======================\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Flare Query Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "> Adding chunk: Flare Query Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Graph Query Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "> Adding chunk: Graph Query Engine\n",
      "=======================\n",
      "\n",
      ".. ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graph Query Engine\n",
      "==================...\n",
      "> Adding chunk: Knowledge Graph Query Engine\n",
      "==================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graph RAG Query Engine\n",
      "==============...\n",
      "> Adding chunk: Knowledge Graph RAG Query Engine\n",
      "==============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Multistep Query Engine\n",
      "=======================\n",
      "...\n",
      "> Adding chunk: Multistep Query Engine\n",
      "=======================\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pandas Query Engine\n",
      "=======================\n",
      "\n",
      ".....\n",
      "> Adding chunk: Pandas Query Engine\n",
      "=======================\n",
      "\n",
      ".....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retriever Query Engine\n",
      "=======================\n",
      "...\n",
      "> Adding chunk: Retriever Query Engine\n",
      "=======================\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retriever Router Query Engine\n",
      "=================...\n",
      "> Adding chunk: Retriever Router Query Engine\n",
      "=================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Router Query Engine\n",
      "=======================\n",
      "\n",
      ".....\n",
      "> Adding chunk: Router Query Engine\n",
      "=======================\n",
      "\n",
      ".....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SQL Join Query Engine\n",
      "=======================\n",
      "\n",
      "...\n",
      "> Adding chunk: SQL Join Query Engine\n",
      "=======================\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SQL Query Engine\n",
      "=======================\n",
      "\n",
      ".. au...\n",
      "> Adding chunk: SQL Query Engine\n",
      "=======================\n",
      "\n",
      ".. au...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Sub Question Query Engine\n",
      "=====================...\n",
      "> Adding chunk: Sub Question Query Engine\n",
      "=====================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Transform Query Engine\n",
      "=======================\n",
      "...\n",
      "> Adding chunk: Transform Query Engine\n",
      "=======================\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Query-Engines:\n",
      "\n",
      "Query Engines\n",
      "=========...\n",
      "> Adding chunk: .. _Ref-Query-Engines:\n",
      "\n",
      "Query Engines\n",
      "=========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Transform\n",
      "===============\n",
      "\n",
      ".. automodule:...\n",
      "> Adding chunk: Query Transform\n",
      "===============\n",
      "\n",
      ".. automodule:...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Response-Synthesizer:\n",
      "\n",
      "Response Synthes...\n",
      "> Adding chunk: .. _Ref-Response-Synthesizer:\n",
      "\n",
      "Response Synthes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Empty Index Retriever\n",
      "=======================\n",
      "\n",
      "...\n",
      "> Adding chunk: Empty Index Retriever\n",
      "=======================\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graph Retriever\n",
      "=====================...\n",
      "> Adding chunk: Knowledge Graph Retriever\n",
      "=====================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: List Retriever\n",
      "=======================\n",
      "\n",
      ".. auto...\n",
      "> Adding chunk: List Retriever\n",
      "=======================\n",
      "\n",
      ".. auto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Keyword Table Retrievers\n",
      "======================...\n",
      "> Adding chunk: Keyword Table Retrievers\n",
      "======================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Transform Retriever\n",
      "=======================\n",
      "\n",
      ".....\n",
      "> Adding chunk: Transform Retriever\n",
      "=======================\n",
      "\n",
      ".....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tree Retrievers\n",
      "=======================\n",
      "\n",
      ".. aut...\n",
      "> Adding chunk: Tree Retrievers\n",
      "=======================\n",
      "\n",
      ".. aut...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Store Retrievers\n",
      "=======================...\n",
      "> Adding chunk: Vector Store Retrievers\n",
      "=======================...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Retrievers:\n",
      "\n",
      "Retrievers\n",
      "===============...\n",
      "> Adding chunk: .. _Ref-Retrievers:\n",
      "\n",
      "Retrievers\n",
      "===============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Query:\n",
      "\n",
      "Querying an Index\n",
      "=============...\n",
      "> Adding chunk: .. _Ref-Query:\n",
      "\n",
      "Querying an Index\n",
      "=============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Connectors\n",
      "===============\n",
      "\n",
      "NOTE: Our data...\n",
      "> Adding chunk: Data Connectors\n",
      "===============\n",
      "\n",
      "NOTE: Our data...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Response:\n",
      "\n",
      "Response\n",
      "=================\n",
      "\n",
      "...\n",
      "> Adding chunk: .. _Ref-Response:\n",
      "\n",
      "Response\n",
      "=================\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Embeddings:\n",
      "\n",
      "Embeddings\n",
      "===============...\n",
      "> Adding chunk: .. _Ref-Embeddings:\n",
      "\n",
      "Embeddings\n",
      "===============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Node Parser\n",
      "===========\n",
      "\n",
      ".. automodule:: llama_...\n",
      "> Adding chunk: Node Parser\n",
      "===========\n",
      "\n",
      ".. automodule:: llama_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Prompt-Helper:\n",
      "\n",
      "PromptHelper\n",
      "==========...\n",
      "> Adding chunk: .. _Ref-Prompt-Helper:\n",
      "\n",
      "PromptHelper\n",
      "==========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Service-Context:\n",
      "\n",
      "Service Context\n",
      "=====...\n",
      "> Adding chunk: .. _Ref-Service-Context:\n",
      "\n",
      "Service Context\n",
      "=====...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Storage-Docstore:\n",
      "\n",
      "Document Store\n",
      "=====...\n",
      "> Adding chunk: .. _Ref-Storage-Docstore:\n",
      "\n",
      "Document Store\n",
      "=====...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Storage-Index-Store:\n",
      "\n",
      "Index Store\n",
      "=====...\n",
      "> Adding chunk: .. _Ref-Storage-Index-Store:\n",
      "\n",
      "Index Store\n",
      "=====...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Indices-SaveLoad:\n",
      "\n",
      "Loading Indices\n",
      "====...\n",
      "> Adding chunk: .. _Ref-Indices-SaveLoad:\n",
      "\n",
      "Loading Indices\n",
      "====...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Storage-KVStore:\n",
      "\n",
      "\n",
      "KV Storage\n",
      "=========...\n",
      "> Adding chunk: .. _Ref-Storage-KVStore:\n",
      "\n",
      "\n",
      "KV Storage\n",
      "=========...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Storage-Vector-Store:\n",
      "\n",
      "Vector Store\n",
      "===...\n",
      "> Adding chunk: .. _Ref-Storage-Vector-Store:\n",
      "\n",
      "Vector Store\n",
      "===...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Storage:\n",
      "\n",
      "\n",
      "Storage Context\n",
      "============...\n",
      "> Adding chunk: .. _Ref-Storage:\n",
      "\n",
      "\n",
      "Storage Context\n",
      "============...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. _Ref-Struct-Store:\n",
      "\n",
      "Structured Index Configu...\n",
      "> Adding chunk: .. _Ref-Struct-Store:\n",
      "\n",
      "Structured Index Configu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: App Showcase\n",
      "\n",
      "Here is a sample of some of the i...\n",
      "> Adding chunk: App Showcase\n",
      "\n",
      "Here is a sample of some of the i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Meru - Dense Data Retrieval API\n",
      "\n",
      "Hosted API ser...\n",
      "> Adding chunk: Meru - Dense Data Retrieval API\n",
      "\n",
      "Hosted API ser...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Algovera\n",
      "\n",
      "Build AI workflows using building blo...\n",
      "> Adding chunk: Algovera\n",
      "\n",
      "Build AI workflows using building blo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SlideSpeak\n",
      "\n",
      "Summarize PowerPoint files and othe...\n",
      "> Adding chunk: SlideSpeak\n",
      "\n",
      "Summarize PowerPoint files and othe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT LlamaIndex\n",
      "\n",
      "Interface that allows users...\n",
      "> Adding chunk: ChatGPT LlamaIndex\n",
      "\n",
      "Interface that allows users...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: AgentHQ\n",
      "\n",
      "A web tool to build agents, interactin...\n",
      "> Adding chunk: AgentHQ\n",
      "\n",
      "A web tool to build agents, interactin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SiteChatAI\n",
      "\n",
      "SiteChatAi is ChatGPT powered that ...\n",
      "> Adding chunk: SiteChatAI\n",
      "\n",
      "SiteChatAi is ChatGPT powered that ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: PapersGPT\n",
      "\n",
      "Feed any of the following content in...\n",
      "> Adding chunk: PapersGPT\n",
      "\n",
      "Feed any of the following content in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: VideoQues + DocsQues\n",
      "\n",
      "**VideoQues**: A tool tha...\n",
      "> Adding chunk: VideoQues + DocsQues\n",
      "\n",
      "**VideoQues**: A tool tha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: PaperBrain\n",
      "\n",
      "A platform to access/understand res...\n",
      "> Adding chunk: PaperBrain\n",
      "\n",
      "A platform to access/understand res...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CACTUS\n",
      "Contextual search on top of LinkedIn sea...\n",
      "> Adding chunk: CACTUS\n",
      "Contextual search on top of LinkedIn sea...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Personal Note Chatbot\n",
      "A chatbot that can answer...\n",
      "> Adding chunk: Personal Note Chatbot\n",
      "A chatbot that can answer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: RHOBH AMA\n",
      "\n",
      "Ask questions about the Real Housewi...\n",
      "> Adding chunk: RHOBH AMA\n",
      "\n",
      "Ask questions about the Real Housewi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Mynd\n",
      "\n",
      "A journaling app that uses AI to uncover ...\n",
      "> Adding chunk: Mynd\n",
      "\n",
      "A journaling app that uses AI to uncover ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CoFounder\n",
      "The First AI Co-Founder for Your Star...\n",
      "> Adding chunk: CoFounder\n",
      "The First AI Co-Founder for Your Star...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Al-X by OpenExO\n",
      "\n",
      "Your Digital Transformation Co...\n",
      "> Adding chunk: Al-X by OpenExO\n",
      "\n",
      "Your Digital Transformation Co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: AnySummary\n",
      "\n",
      "Summarize any document, audio or vi...\n",
      "> Adding chunk: AnySummary\n",
      "\n",
      "Summarize any document, audio or vi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Blackmaria\n",
      "\n",
      "Python package for webscraping in N...\n",
      "> Adding chunk: Blackmaria\n",
      "\n",
      "Python package for webscraping in N...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT Plugin Integrations\n",
      "\n",
      "**NOTE**: This is ...\n",
      "> Adding chunk: ChatGPT Plugin Integrations\n",
      "\n",
      "**NOTE**: This is ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT Retrieval Plugin Integrations\n",
      "\n",
      "The Open...\n",
      "> Adding chunk: ChatGPT Retrieval Plugin Integrations\n",
      "\n",
      "The Open...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Loading Data from LlamaHub into the ChatGPT Ret...\n",
      "> Adding chunk: Loading Data from LlamaHub into the ChatGPT Ret...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: download loader, load documents\n",
      "SimpleWebPageRe...\n",
      "> Adding chunk: download loader, load documents\n",
      "SimpleWebPageRe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Convert LlamaIndex Documents to JSON format\n",
      "def...\n",
      "> Adding chunk: Convert LlamaIndex Documents to JSON format\n",
      "def...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT Retrieval Plugin Data Loader\n",
      "\n",
      "The ChatG...\n",
      "> Adding chunk: ChatGPT Retrieval Plugin Data Loader\n",
      "\n",
      "The ChatG...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents\n",
      "bearer_token = os.getenv(\"BEARER...\n",
      "> Adding chunk: load documents\n",
      "bearer_token = os.getenv(\"BEARER...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build and query index\n",
      "from llama_index import L...\n",
      "> Adding chunk: build and query index\n",
      "from llama_index import L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set Logging to DEBUG for more detailed outputs\n",
      "...\n",
      "> Adding chunk: set Logging to DEBUG for more detailed outputs\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT Retrieval Plugin Index\n",
      "\n",
      "The ChatGPT Ret...\n",
      "> Adding chunk: ChatGPT Retrieval Plugin Index\n",
      "\n",
      "The ChatGPT Ret...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "bearer_token = os.getenv(\"BEARER_TO...\n",
      "> Adding chunk: build index\n",
      "bearer_token = os.getenv(\"BEARER_TO...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize without metadata filter\n",
      "index = Chat...\n",
      "> Adding chunk: initialize without metadata filter\n",
      "index = Chat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using Graph Stores...\n",
      "> Adding chunk: Using Graph Stores...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: `Neo4jGraphStore`\n",
      "\n",
      "`Neo4j` is supported as a gr...\n",
      "> Adding chunk: `Neo4jGraphStore`\n",
      "\n",
      "`Neo4j` is supported as a gr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: `NebulaGraphStore`\n",
      "\n",
      "We support a `NebulaGraphSt...\n",
      "> Adding chunk: `NebulaGraphStore`\n",
      "\n",
      "We support a `NebulaGraphSt...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: `KuzuGraphStore`\n",
      "\n",
      "We support a `KuzuGraphStore`...\n",
      "> Adding chunk: `KuzuGraphStore`\n",
      "\n",
      "We support a `KuzuGraphStore`...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: `FalkorDBGraphStore`\n",
      "\n",
      "We support a `FalkorDBGra...\n",
      "> Adding chunk: `FalkorDBGraphStore`\n",
      "\n",
      "We support a `FalkorDBGra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tracing with Graphsignal\n",
      "\n",
      "Graphsignal provides ...\n",
      "> Adding chunk: Tracing with Graphsignal\n",
      "\n",
      "Graphsignal provides ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Installation and Setup\n",
      "\n",
      "Adding Graphsignal trac...\n",
      "> Adding chunk: Installation and Setup\n",
      "\n",
      "Adding Graphsignal trac...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Provide an API key directly or via GRAPHSIGNAL_...\n",
      "> Adding chunk: Provide an API key directly or via GRAPHSIGNAL_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tracing Other Functions\n",
      "\n",
      "To additionally trace ...\n",
      "> Adding chunk: Tracing Other Functions\n",
      "\n",
      "To additionally trace ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Useful Links\n",
      "\n",
      "* Tracing and Monitoring LlamaInd...\n",
      "> Adding chunk: Useful Links\n",
      "\n",
      "* Tracing and Monitoring LlamaInd...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guidance\n",
      "\n",
      "Guidance is a guidance language for c...\n",
      "> Adding chunk: Guidance\n",
      "\n",
      "Guidance is a guidance language for c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Structured Output\n",
      "One particularly exciting asp...\n",
      "> Adding chunk: Structured Output\n",
      "One particularly exciting asp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a guidance program to generate pydanti...\n",
      "> Adding chunk: Creating a guidance program to generate pydanti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using guidance to improve the robustness of our...\n",
      "> Adding chunk: Using guidance to improve the robustness of our...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define guidance based question generator\n",
      "questi...\n",
      "> Adding chunk: define guidance based question generator\n",
      "questi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query engine tools\n",
      "query_engine_tools = ...\n",
      "> Adding chunk: define query engine tools\n",
      "query_engine_tools = ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct sub-question query engine\n",
      "s_engine = ...\n",
      "> Adding chunk: construct sub-question query engine\n",
      "s_engine = ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using Managed Indices\n",
      "\n",
      "LlamaIndex offers multip...\n",
      "> Adding chunk: Using Managed Indices\n",
      "\n",
      "LlamaIndex offers multip...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using a Managed Index\n",
      "\n",
      "Similar to any other ind...\n",
      "> Adding chunk: Using a Managed Index\n",
      "\n",
      "Similar to any other ind...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load documents and build index\n",
      "vectara_customer...\n",
      "> Adding chunk: Load documents and build index\n",
      "vectara_customer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluating and Tracking with TruLens\n",
      "\n",
      "This page...\n",
      "> Adding chunk: Evaluating and Tracking with TruLens\n",
      "\n",
      "This page...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: What is TruLens?\n",
      "\n",
      "TruLens is an opensource pack...\n",
      "> Adding chunk: What is TruLens?\n",
      "\n",
      "TruLens is an opensource pack...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Installation and Setup\n",
      "\n",
      "Adding TruLens is simpl...\n",
      "> Adding chunk: Installation and Setup\n",
      "\n",
      "Adding TruLens is simpl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Try it out!\n",
      "\n",
      "llama_index_quickstart.ipynb\n",
      "\n",
      "![Op...\n",
      "> Adding chunk: Try it out!\n",
      "\n",
      "llama_index_quickstart.ipynb\n",
      "\n",
      "![Op...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Read more\n",
      "\n",
      "* Build and Evaluate LLM Apps with L...\n",
      "> Adding chunk: Read more\n",
      "\n",
      "* Build and Evaluate LLM Apps with L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with Langchain 🦜🔗\n",
      "\n",
      "LlamaIndex provides bo...\n",
      "> Adding chunk: Using with Langchain 🦜🔗\n",
      "\n",
      "LlamaIndex provides bo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use any data loader as a Langchain Tool\n",
      "\n",
      "LlamaI...\n",
      "> Adding chunk: Use any data loader as a Langchain Tool\n",
      "\n",
      "LlamaI...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use a query engine as a Langchain Tool\n",
      "LlamaInd...\n",
      "> Adding chunk: Use a query engine as a Langchain Tool\n",
      "LlamaInd...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Llama Demo Notebook: Tool + Memory module\n",
      "\n",
      "We p...\n",
      "> Adding chunk: Llama Demo Notebook: Tool + Memory module\n",
      "\n",
      "We p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using Vector Stores\n",
      "\n",
      "LlamaIndex offers multiple...\n",
      "> Adding chunk: Using Vector Stores\n",
      "\n",
      "LlamaIndex offers multiple...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using a Vector Store as an Index\n",
      "\n",
      "LlamaIndex al...\n",
      "> Adding chunk: Using a Vector Store as an Index\n",
      "\n",
      "LlamaIndex al...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store and customize storage co...\n",
      "> Adding chunk: construct vector store and customize storage co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "> Adding chunk: Load documents and build index\n",
      "documents = Simp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "> Adding chunk: Query index\n",
      "query_engine = index.as_query_engin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = DeepLakeV...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = DeepLakeV...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create faiss index\n",
      "d = 1536\n",
      "faiss_index = faiss...\n",
      "> Adding chunk: create faiss index\n",
      "d = 1536\n",
      "faiss_index = faiss...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = FaissVect...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = FaissVect...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: since faiss index is in-memory, we need t...\n",
      "> Adding chunk: NOTE: since faiss index is in-memory, we need t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: creating a Weaviate client\n",
      "resource_owner_confi...\n",
      "> Adding chunk: creating a Weaviate client\n",
      "resource_owner_confi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = WeaviateV...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = WeaviateV...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query index using both a text query and metadat...\n",
      "> Adding chunk: Query index using both a text query and metadat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a Pinecone index\n",
      "api_key = \"api_key\"\n",
      "p...\n",
      "> Adding chunk: Creating a Pinecone index\n",
      "api_key = \"api_key\"\n",
      "p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: can define filters specific to this vector inde...\n",
      "> Adding chunk: can define filters specific to this vector inde...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = PineconeV...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = PineconeV...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a Qdrant vector store\n",
      "client = qdrant_...\n",
      "> Adding chunk: Creating a Qdrant vector store\n",
      "client = qdrant_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = QdrantVec...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = QdrantVec...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: for a Cassandra cluster:\n",
      "cluster = Cluster([\"12...\n",
      "> Adding chunk: for a Cassandra cluster:\n",
      "cluster = Cluster([\"12...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: for an Astra DB cloud instance:\n",
      "cluster = Clust...\n",
      "> Adding chunk: for an Astra DB cloud instance:\n",
      "cluster = Clust...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a Chroma client\n",
      "chroma_client = chroma...\n",
      "> Adding chunk: Creating a Chroma client\n",
      "chroma_client = chroma...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = ChromaVec...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = ChromaVec...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = MilvusVec...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = MilvusVec...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = MilvusVec...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = MilvusVec...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a MyScale client\n",
      "client = clickhouse_c...\n",
      "> Adding chunk: Creating a MyScale client\n",
      "client = clickhouse_c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = MyScaleVe...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = MyScaleVe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = DocArrayH...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = DocArrayH...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: alternatively, construct the in-memory vector s...\n",
      "> Adding chunk: alternatively, construct the in-memory vector s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Provide URI to constructor, or use environment ...\n",
      "> Adding chunk: Provide URI to constructor, or use environment ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: mongo_uri = os.environ[\"MONGO_URI\"]\n",
      "mongo_uri =...\n",
      "> Adding chunk: mongo_uri = os.environ[\"MONGO_URI\"]\n",
      "mongo_uri =...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct store\n",
      "store = MongoDBAtlasVectorSearc...\n",
      "> Adding chunk: construct store\n",
      "store = MongoDBAtlasVectorSearc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct index\n",
      "index = VectorStoreIndex.from_d...\n",
      "> Adding chunk: construct index\n",
      "index = VectorStoreIndex.from_d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Loading Data from Vector Stores using Data Conn...\n",
      "> Adding chunk: Loading Data from Vector Stores using Data Conn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The chroma reader loads data from a persisted C...\n",
      "> Adding chunk: The chroma reader loads data from a persisted C...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: the query_vector is an embedding representation...\n",
      "> Adding chunk: the query_vector is an embedding representation...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: Required args are collection_name, query_...\n",
      "> Adding chunk: NOTE: Required args are collection_name, query_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: option 1: specify class_name and properties...\n",
      "> Adding chunk: option 1: specify class_name and properties...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 1) load data using class_name and properties\n",
      "do...\n",
      "> Adding chunk: 1) load data using class_name and properties\n",
      "do...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 2) example GraphQL query\n",
      "query = \"\"\"\n",
      "{\n",
      "    Get ...\n",
      "> Adding chunk: 2) example GraphQL query\n",
      "query = \"\"\"\n",
      "{\n",
      "    Get ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Integrations\n",
      "\n",
      "LlamaIndex has a number of commun...\n",
      "> Adding chunk: Integrations\n",
      "\n",
      "LlamaIndex has a number of commun...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Loaders\n",
      "\n",
      "The full set of data loaders are ...\n",
      "> Adding chunk: Data Loaders\n",
      "\n",
      "The full set of data loaders are ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Agent Tools\n",
      "The full set of agent tools are fou...\n",
      "> Adding chunk: Agent Tools\n",
      "The full set of agent tools are fou...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLMs\n",
      "The full set of supported LLMs are found h...\n",
      "> Adding chunk: LLMs\n",
      "The full set of supported LLMs are found h...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Observability/Tracing\n",
      "\n",
      "Check out our one-click ...\n",
      "> Adding chunk: Observability/Tracing\n",
      "\n",
      "Check out our one-click ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Structured Outputs\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "> Adding chunk: Structured Outputs\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Storage\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "integr...\n",
      "> Adding chunk: Storage\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "integr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Application Frameworks\n",
      "```{toctree}\n",
      "---\n",
      "maxdept...\n",
      "> Adding chunk: Application Frameworks\n",
      "```{toctree}\n",
      "---\n",
      "maxdept...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Distributed Compute\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "> Adding chunk: Distributed Compute\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Other\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "integrat...\n",
      "> Adding chunk: Other\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "integrat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides\n",
      "\n",
      "These guide provide an overview ...\n",
      "> Adding chunk: Module Guides\n",
      "\n",
      "These guide provide an overview ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OpenAI Agent\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "> Adding chunk: OpenAI Agent\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ReAct Agent\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/e...\n",
      "> Adding chunk: ReAct Agent\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/e...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Agents...\n",
      "> Adding chunk: Data Agents...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Data Agents are LLM-powered knowledge w...\n",
      "> Adding chunk: Concept\n",
      "Data Agents are LLM-powered knowledge w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Reasoning Loop\n",
      "The reasoning loop depends on th...\n",
      "> Adding chunk: Reasoning Loop\n",
      "The reasoning loop depends on th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tool Abstractions\n",
      "\n",
      "You can learn more about our...\n",
      "> Adding chunk: Tool Abstractions\n",
      "\n",
      "You can learn more about our...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Blog Post\n",
      "\n",
      "For full details, please check out o...\n",
      "> Adding chunk: Blog Post\n",
      "\n",
      "For full details, please check out o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Data agents can be used in the f...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Data agents can be used in the f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: import and define tools\n",
      "......\n",
      "> Adding chunk: import and define tools\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize llm\n",
      "llm = OpenAI(model=\"gpt-3.5-turb...\n",
      "> Adding chunk: initialize llm\n",
      "llm = OpenAI(model=\"gpt-3.5-turb...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize openai agent\n",
      "agent = OpenAIAgent.fro...\n",
      "> Adding chunk: initialize openai agent\n",
      "agent = OpenAIAgent.fro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "Learn more about our different agent t...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "Learn more about our different agent t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "\n",
      "An agent is initialized from a set...\n",
      "> Adding chunk: Get Started\n",
      "\n",
      "An agent is initialized from a set...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define sample Tool\n",
      "def multiply(a: int, b: int)...\n",
      "> Adding chunk: define sample Tool\n",
      "def multiply(a: int, b: int)...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize llm\n",
      "llm = OpenAI(model=\"gpt-3.5-turb...\n",
      "> Adding chunk: initialize llm\n",
      "llm = OpenAI(model=\"gpt-3.5-turb...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize ReAct agent\n",
      "agent = ReActAgent.from_...\n",
      "> Adding chunk: initialize ReAct agent\n",
      "agent = ReActAgent.from_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Engine Tools\n",
      "\n",
      "It is easy to wrap query en...\n",
      "> Adding chunk: Query Engine Tools\n",
      "\n",
      "It is easy to wrap query en...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: lyft_index and uber_index are both Simple...\n",
      "> Adding chunk: NOTE: lyft_index and uber_index are both Simple...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize ReAct agent\n",
      "agent = ReActAgent.from_...\n",
      "> Adding chunk: initialize ReAct agent\n",
      "agent = ReActAgent.from_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use other agents as Tools\n",
      "\n",
      "A nifty feature of o...\n",
      "> Adding chunk: Use other agents as Tools\n",
      "\n",
      "A nifty feature of o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced Concepts (for `OpenAIAgent`, in beta)\n",
      "...\n",
      "> Adding chunk: Advanced Concepts (for `OpenAIAgent`, in beta)\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Function Retrieval Agents\n",
      "\n",
      "If the set of Tools ...\n",
      "> Adding chunk: Function Retrieval Agents\n",
      "\n",
      "If the set of Tools ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define an \"object\" index over these tools\n",
      "from ...\n",
      "> Adding chunk: define an \"object\" index over these tools\n",
      "from ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Context Retrieval Agents\n",
      "\n",
      "Our context-augmented...\n",
      "> Adding chunk: Context Retrieval Agents\n",
      "\n",
      "Our context-augmented...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: toy index - stores a list of abbreviations\n",
      "text...\n",
      "> Adding chunk: toy index - stores a list of abbreviations\n",
      "text...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: add context agent\n",
      "context_agent = ContextRetrie...\n",
      "> Adding chunk: add context agent\n",
      "context_agent = ContextRetrie...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Planning\n",
      "\n",
      "OpenAI Function Agents can be c...\n",
      "> Adding chunk: Query Planning\n",
      "\n",
      "OpenAI Function Agents can be c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query plan tool\n",
      "from llama_index.tools i...\n",
      "> Adding chunk: define query plan tool\n",
      "from llama_index.tools i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize agent\n",
      "agent = OpenAIAgent.from_tools...\n",
      "> Adding chunk: initialize agent\n",
      "agent = OpenAIAgent.from_tools...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: should output a query plan to call march, june,...\n",
      "> Adding chunk: should output a query plan to call march, june,...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LlamaHub Tools Guide\n",
      "\n",
      "We offer a rich set of To...\n",
      "> Adding chunk: LlamaHub Tools Guide\n",
      "\n",
      "We offer a rich set of To...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tool Specs\n",
      "\n",
      "Coming soon!...\n",
      "> Adding chunk: Tool Specs\n",
      "\n",
      "Coming soon!...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Utility Tools\n",
      "\n",
      "Oftentimes, directly querying an...\n",
      "> Adding chunk: Utility Tools\n",
      "\n",
      "Oftentimes, directly querying an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OnDemandLoaderTool\n",
      "\n",
      "This tool turns any existin...\n",
      "> Adding chunk: OnDemandLoaderTool\n",
      "\n",
      "This tool turns any existin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LoadAndSearchToolSpec\n",
      "\n",
      "The LoadAndSearchToolSpe...\n",
      "> Adding chunk: LoadAndSearchToolSpec\n",
      "\n",
      "The LoadAndSearchToolSpe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get the search wikipedia tool\n",
      "tool = wiki_spec....\n",
      "> Adding chunk: Get the search wikipedia tool\n",
      "tool = wiki_spec....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Create the Agent with load/search tools\n",
      "agent =...\n",
      "> Adding chunk: Create the Agent with load/search tools\n",
      "agent =...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tools...\n",
      "> Adding chunk: Tools...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "Having proper tool abstractions is at ...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "Having proper tool abstractions is at ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Blog Post\n",
      "\n",
      "For full details, please check out o...\n",
      "> Adding chunk: Blog Post\n",
      "\n",
      "For full details, please check out o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Our Tool Specs and Tools can be ...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Our Tool Specs and Tools can be ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LlamaHub Tools Guide 🛠️\n",
      "\n",
      "Check out our guide fo...\n",
      "> Adding chunk: LlamaHub Tools Guide 🛠️\n",
      "\n",
      "Check out our guide fo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tool Example Notebooks\n",
      "\n",
      "Coming soon!  -->...\n",
      "> Adding chunk: Tool Example Notebooks\n",
      "\n",
      "Coming soon!  -->...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "You can create custom LlamaHub T...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "You can create custom LlamaHub T...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with our Agents\n",
      "\n",
      "To use with our OpenAIAg...\n",
      "> Adding chunk: Using with our Agents\n",
      "\n",
      "To use with our OpenAIAg...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use a tool spec from Llama-Hub\n",
      "tool_spec = Gmai...\n",
      "> Adding chunk: Use a tool spec from Llama-Hub\n",
      "tool_spec = Gmai...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Create a custom tool. Type annotations and docs...\n",
      "> Adding chunk: Create a custom tool. Type annotations and docs...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: use agent\n",
      "agent.chat(\"Can you create a new emai...\n",
      "> Adding chunk: use agent\n",
      "agent.chat(\"Can you create a new emai...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with LangChain\n",
      "To use with a LangChain ag...\n",
      "> Adding chunk: Using with LangChain\n",
      "To use with a LangChain ag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: plug into LangChain agent\n",
      "from langchain.agents...\n",
      "> Adding chunk: plug into LangChain agent\n",
      "from langchain.agents...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides\n",
      "\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "--...\n",
      "> Adding chunk: Module Guides\n",
      "\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "--...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Connectors (LlamaHub)...\n",
      "> Adding chunk: Data Connectors (LlamaHub)...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "A data connector (i.e. `Reader`) ingest...\n",
      "> Adding chunk: Concept\n",
      "A data connector (i.e. `Reader`) ingest...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LlamaHub\n",
      "Our data connectors are offered throug...\n",
      "> Adding chunk: LlamaHub\n",
      "Our data connectors are offered throug...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "from ...\n",
      "> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "from ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "Some sample data connectors:\n",
      "- local f...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "Some sample data connectors:\n",
      "- local f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "Each data loader contains a \"Usage\"...\n",
      "> Adding chunk: Get Started\n",
      "Each data loader contains a \"Usage\"...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Documents / Nodes...\n",
      "> Adding chunk: Documents / Nodes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "Document and Node objects are core abs...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "Document and Node objects are core abs...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Here are some simple snippets to...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Here are some simple snippets to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Documents\n",
      "\n",
      "```python\n",
      "from llama_index import Do...\n",
      "> Adding chunk: Documents\n",
      "\n",
      "```python\n",
      "from llama_index import Do...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Nodes\n",
      "```python\n",
      "\n",
      "from llama_index.node_parser i...\n",
      "> Adding chunk: Nodes\n",
      "```python\n",
      "\n",
      "from llama_index.node_parser i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents\n",
      "......\n",
      "> Adding chunk: load documents\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: parse nodes\n",
      "parser = SimpleNodeParser.from_defa...\n",
      "> Adding chunk: parse nodes\n",
      "parser = SimpleNodeParser.from_defa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes)\n",
      "\n",
      "``...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes)\n",
      "\n",
      "``...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Document/Node Usage\n",
      "\n",
      "Take a look at our in-dept...\n",
      "> Adding chunk: Document/Node Usage\n",
      "\n",
      "Take a look at our in-dept...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining and Customizing Documents...\n",
      "> Adding chunk: Defining and Customizing Documents...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining Documents\n",
      "\n",
      "Documents can either be cre...\n",
      "> Adding chunk: Defining Documents\n",
      "\n",
      "Documents can either be cre...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Documents\n",
      "\n",
      "This section covers vari...\n",
      "> Adding chunk: Customizing Documents\n",
      "\n",
      "This section covers vari...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Metadata\n",
      "\n",
      "Documents also offer the chance to in...\n",
      "> Adding chunk: Metadata\n",
      "\n",
      "Documents also offer the chance to in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: automatically sets the metadata of each documen...\n",
      "> Adding chunk: automatically sets the metadata of each documen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing the id\n",
      "\n",
      "As detailed in the section ...\n",
      "> Adding chunk: Customizing the id\n",
      "\n",
      "As detailed in the section ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced - Metadata Customization\n",
      "\n",
      "A key detail...\n",
      "> Adding chunk: Advanced - Metadata Customization\n",
      "\n",
      "A key detail...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing LLM Metadata Text\n",
      "\n",
      "Typically, a doc...\n",
      "> Adding chunk: Customizing LLM Metadata Text\n",
      "\n",
      "Typically, a doc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Embedding Metadata Text\n",
      "\n",
      "Similar to...\n",
      "> Adding chunk: Customizing Embedding Metadata Text\n",
      "\n",
      "Similar to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Metadata Format\n",
      "\n",
      "As you know by now...\n",
      "> Adding chunk: Customizing Metadata Format\n",
      "\n",
      "As you know by now...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Summary\n",
      "\n",
      "Knowing all this, let's create a short...\n",
      "> Adding chunk: Summary\n",
      "\n",
      "Knowing all this, let's create a short...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced - Automatic Metadata Extraction\n",
      "\n",
      "We ha...\n",
      "> Adding chunk: Advanced - Automatic Metadata Extraction\n",
      "\n",
      "We ha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Automated Metadata Extraction for Nodes\n",
      "\n",
      "You ca...\n",
      "> Adding chunk: Automated Metadata Extraction for Nodes\n",
      "\n",
      "You ca...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: assume documents are defined -> extract nodes\n",
      "n...\n",
      "> Adding chunk: assume documents are defined -> extract nodes\n",
      "n...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining and Customizing Nodes\n",
      "\n",
      "Nodes represent...\n",
      "> Adding chunk: Defining and Customizing Nodes\n",
      "\n",
      "Nodes represent...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set relationships\n",
      "node1.relationships[NodeRelat...\n",
      "> Adding chunk: set relationships\n",
      "node1.relationships[NodeRelat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Composability\n",
      "\n",
      "\n",
      "LlamaIndex offers **composabili...\n",
      "> Adding chunk: Composability\n",
      "\n",
      "\n",
      "LlamaIndex offers **composabili...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining Subindices\n",
      "To see how this works, imag...\n",
      "> Adding chunk: Defining Subindices\n",
      "To see how this works, imag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining Summary Text\n",
      "\n",
      "You then need to explici...\n",
      "> Adding chunk: Defining Summary Text\n",
      "\n",
      "You then need to explici...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a Graph with a Top-Level Index\n",
      "\n",
      "We can...\n",
      "> Adding chunk: Creating a Graph with a Top-Level Index\n",
      "\n",
      "We can...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying the Graph\n",
      "\n",
      "During a query, we would st...\n",
      "> Adding chunk: Querying the Graph\n",
      "\n",
      "During a query, we would st...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set custom retrievers. An example is provided b...\n",
      "> Adding chunk: set custom retrievers. An example is provided b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: [Optional] Persisting the Graph\n",
      "\n",
      "The graph can ...\n",
      "> Adding chunk: [Optional] Persisting the Graph\n",
      "\n",
      "The graph can ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set the ID\n",
      "graph.root_index.set_index_id(\"my_id...\n",
      "> Adding chunk: set the ID\n",
      "graph.root_index.set_index_id(\"my_id...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: persist to storage\n",
      "graph.root_index.storage_con...\n",
      "> Adding chunk: persist to storage\n",
      "graph.root_index.storage_con...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load\n",
      "from llama_index import StorageContext, lo...\n",
      "> Adding chunk: load\n",
      "from llama_index import StorageContext, lo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Document Management\n",
      "\n",
      "Most LlamaIndex index stru...\n",
      "> Adding chunk: Document Management\n",
      "\n",
      "Most LlamaIndex index stru...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Insertion\n",
      "\n",
      "You can \"insert\" a new Document into...\n",
      "> Adding chunk: Insertion\n",
      "\n",
      "You can \"insert\" a new Document into...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: insert\n",
      "for doc_chunk in doc_chunks:\n",
      "    index.i...\n",
      "> Adding chunk: insert\n",
      "for doc_chunk in doc_chunks:\n",
      "    index.i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Deletion\n",
      "\n",
      "You can \"delete\" a Document from most...\n",
      "> Adding chunk: Deletion\n",
      "\n",
      "You can \"delete\" a Document from most...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Update\n",
      "\n",
      "If a Document is already present within...\n",
      "> Adding chunk: Update\n",
      "\n",
      "If a Document is already present within...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: the document has a `doc_id` specified\n",
      "doc...\n",
      "> Adding chunk: NOTE: the document has a `doc_id` specified\n",
      "doc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Refresh\n",
      "\n",
      "If you set the doc `id_` of each docum...\n",
      "> Adding chunk: Refresh\n",
      "\n",
      "If you set the doc `id_` of each docum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: modify first document, with the same doc_id\n",
      "doc...\n",
      "> Adding chunk: modify first document, with the same doc_id\n",
      "doc...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: add a new document\n",
      "doc_chunks.append(Document(t...\n",
      "> Adding chunk: add a new document\n",
      "doc_chunks.append(Document(t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: refresh the index\n",
      "refreshed_docs = index.refres...\n",
      "> Adding chunk: refresh the index\n",
      "refreshed_docs = index.refres...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: refreshed_docs[0] and refreshed_docs[-1] should...\n",
      "> Adding chunk: refreshed_docs[0] and refreshed_docs[-1] should...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Document Tracking\n",
      "\n",
      "Any index that uses the docs...\n",
      "> Adding chunk: Document Tracking\n",
      "\n",
      "Any index that uses the docs...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: How Each Index Works\n",
      "\n",
      "This guide describes how ...\n",
      "> Adding chunk: How Each Index Works\n",
      "\n",
      "This guide describes how ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: List Index\n",
      "\n",
      "The list index simply stores Nodes ...\n",
      "> Adding chunk: List Index\n",
      "\n",
      "The list index simply stores Nodes ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying\n",
      "\n",
      "During query time, if no other query ...\n",
      "> Adding chunk: Querying\n",
      "\n",
      "During query time, if no other query ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Store Index\n",
      "\n",
      "The vector store index stor...\n",
      "> Adding chunk: Vector Store Index\n",
      "\n",
      "The vector store index stor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying\n",
      "\n",
      "Querying a vector store index involve...\n",
      "> Adding chunk: Querying\n",
      "\n",
      "Querying a vector store index involve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tree Index\n",
      "\n",
      "The tree index builds a hierarchica...\n",
      "> Adding chunk: Tree Index\n",
      "\n",
      "The tree index builds a hierarchica...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying\n",
      "\n",
      "Querying a tree index involves traver...\n",
      "> Adding chunk: Querying\n",
      "\n",
      "Querying a tree index involves traver...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Keyword Table Index\n",
      "\n",
      "The keyword table index ex...\n",
      "> Adding chunk: Keyword Table Index\n",
      "\n",
      "The keyword table index ex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying\n",
      "\n",
      "During query time, we extract relevan...\n",
      "> Adding chunk: Querying\n",
      "\n",
      "During query time, we extract relevan...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Metadata Extraction...\n",
      "> Adding chunk: Metadata Extraction...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Introduction\n",
      "In many cases, especially with lon...\n",
      "> Adding chunk: Introduction\n",
      "In many cases, especially with lon...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage\n",
      "\n",
      "First, we define a metadata extractor th...\n",
      "> Adding chunk: Usage\n",
      "\n",
      "First, we define a metadata extractor th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Custom Extractors\n",
      "\n",
      "If the provided extractors d...\n",
      "> Adding chunk: Custom Extractors\n",
      "\n",
      "If the provided extractors d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---...\n",
      "> Adding chunk: Module Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Indexes...\n",
      "> Adding chunk: Indexes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "An `Index` is a data structure that all...\n",
      "> Adding chunk: Concept\n",
      "An `Index` is a data structure that all...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "from ...\n",
      "> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "from ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2\n",
      "---\n",
      "modul...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2\n",
      "---\n",
      "modul...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced Concepts\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "> Adding chunk: Advanced Concepts\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "\n",
      "Build an index from documents:\n",
      "\n",
      "``...\n",
      "> Adding chunk: Get Started\n",
      "\n",
      "Build an index from documents:\n",
      "\n",
      "``...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: What is happening under the hood?\n",
      "\n",
      "1. Documents...\n",
      "> Adding chunk: What is happening under the hood?\n",
      "\n",
      "1. Documents...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring Document Parsing\n",
      "\n",
      "The most common c...\n",
      "> Adding chunk: Configuring Document Parsing\n",
      "\n",
      "The most common c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High-Level API\n",
      "\n",
      "We can configure our service co...\n",
      "> Adding chunk: High-Level API\n",
      "\n",
      "We can configure our service co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-Level API\n",
      "\n",
      "You can use the low-level compos...\n",
      "> Adding chunk: Low-Level API\n",
      "\n",
      "You can use the low-level compos...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Handling Document Update\n",
      "\n",
      "Read more about how t...\n",
      "> Adding chunk: Handling Document Update\n",
      "\n",
      "Read more about how t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Node Parser...\n",
      "> Adding chunk: Node Parser...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "Node parsers are a simple abstraction ...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "Node parsers are a simple abstraction ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "from llama_index.node_...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "from llama_index.node_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Getting Started\n",
      "\n",
      "Node parsers can be used on th...\n",
      "> Adding chunk: Getting Started\n",
      "\n",
      "Node parsers can be used on th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customization\n",
      "\n",
      "There are several options availa...\n",
      "> Adding chunk: Customization\n",
      "\n",
      "There are several options availa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Text Splitter Customization\n",
      "\n",
      "If you do customiz...\n",
      "> Adding chunk: Text Splitter Customization\n",
      "\n",
      "If you do customiz...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SentenceWindowNodeParser\n",
      "\n",
      "The `SentenceWindowNo...\n",
      "> Adding chunk: SentenceWindowNodeParser\n",
      "\n",
      "The `SentenceWindowNo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Storage\n",
      "\n",
      "By default, LlamaIndex hid...\n",
      "> Adding chunk: Customizing Storage\n",
      "\n",
      "By default, LlamaIndex hid...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-Level API\n",
      "To do this, instead of the high-l...\n",
      "> Adding chunk: Low-Level API\n",
      "To do this, instead of the high-l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context using default stores\n",
      "sto...\n",
      "> Adding chunk: create storage context using default stores\n",
      "sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) docstore and add nodes\n",
      "storage...\n",
      "> Adding chunk: create (or load) docstore and add nodes\n",
      "storage...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: save index\n",
      "index.storage_context.persist(persis...\n",
      "> Adding chunk: save index\n",
      "index.storage_context.persist(persis...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: can also set index_id to save multiple indexes ...\n",
      "> Adding chunk: can also set index_id to save multiple indexes ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: to load index later, make sure you setup the st...\n",
      "> Adding chunk: to load index later, make sure you setup the st...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: then load the index object\n",
      "from llama_index imp...\n",
      "> Adding chunk: then load the index object\n",
      "from llama_index imp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: if loading an index from a persist_dir containi...\n",
      "> Adding chunk: if loading an index from a persist_dir containi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: if loading multiple indexes from a persist dir\n",
      "...\n",
      "> Adding chunk: if loading multiple indexes from a persist dir\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Store Integrations and Storage\n",
      "\n",
      "Most of ...\n",
      "> Adding chunk: Vector Store Integrations and Storage\n",
      "\n",
      "Most of ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Creating a Pinecone index\n",
      "api_key = \"api_key\"\n",
      "p...\n",
      "> Adding chunk: Creating a Pinecone index\n",
      "api_key = \"api_key\"\n",
      "p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store\n",
      "vector_store = PineconeV...\n",
      "> Adding chunk: construct vector store\n",
      "vector_store = PineconeV...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create index, which will insert documents/vecto...\n",
      "> Adding chunk: create index, which will insert documents/vecto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Document Stores\n",
      "Document stores contain ingeste...\n",
      "> Adding chunk: Document Stores\n",
      "Document stores contain ingeste...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Simple Document Store\n",
      "By default, the `SimpleDo...\n",
      "> Adding chunk: Simple Document Store\n",
      "By default, the `SimpleDo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: MongoDB Document Store\n",
      "We support MongoDB as an...\n",
      "> Adding chunk: MongoDB Document Store\n",
      "We support MongoDB as an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Redis Document Store\n",
      "\n",
      "We support Redis as an al...\n",
      "> Adding chunk: Redis Document Store\n",
      "\n",
      "We support Redis as an al...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Firestore Document Store\n",
      "\n",
      "We support Firestore ...\n",
      "> Adding chunk: Firestore Document Store\n",
      "\n",
      "We support Firestore ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "> Adding chunk: create parser and parse document into nodes\n",
      "par...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "> Adding chunk: create (or load) docstore and add nodes\n",
      "docstor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Index Stores\n",
      "\n",
      "Index stores contains lightweight...\n",
      "> Adding chunk: Index Stores\n",
      "\n",
      "Index stores contains lightweight...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Simple Index Store\n",
      "By default, LlamaIndex uses ...\n",
      "> Adding chunk: Simple Index Store\n",
      "By default, LlamaIndex uses ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: MongoDB Index Store\n",
      "Similarly to document store...\n",
      "> Adding chunk: MongoDB Index Store\n",
      "Similarly to document store...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) index store\n",
      "index_store = Mong...\n",
      "> Adding chunk: create (or load) index store\n",
      "index_store = Mong...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: or alternatively, load index\n",
      "from llama_index i...\n",
      "> Adding chunk: or alternatively, load index\n",
      "from llama_index i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Redis Index Store\n",
      "\n",
      "We support Redis as an alter...\n",
      "> Adding chunk: Redis Index Store\n",
      "\n",
      "We support Redis as an alter...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create (or load) docstore and add nodes\n",
      "index_s...\n",
      "> Adding chunk: create (or load) docstore and add nodes\n",
      "index_s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "> Adding chunk: create storage context\n",
      "storage_context = Storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex(nodes, sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: or alternatively, load index\n",
      "from llama_index i...\n",
      "> Adding chunk: or alternatively, load index\n",
      "from llama_index i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Key-Value Stores\n",
      "\n",
      "Key-Value stores are the unde...\n",
      "> Adding chunk: Key-Value Stores\n",
      "\n",
      "Key-Value stores are the unde...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Storage...\n",
      "> Adding chunk: Storage...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "LlamaIndex provides a high-level inter...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "LlamaIndex provides a high-level inter...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Many vector stores (except FAISS...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Many vector stores (except FAISS...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build a new index\n",
      "from llama_index import Vecto...\n",
      "> Adding chunk: build a new index\n",
      "from llama_index import Vecto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct vector store and customize storage co...\n",
      "> Adding chunk: construct vector store and customize storage co...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load documents and build index\n",
      "index = VectorSt...\n",
      "> Adding chunk: Load documents and build index\n",
      "index = VectorSt...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: reload an existing one\n",
      "index = VectorStoreIndex...\n",
      "> Adding chunk: reload an existing one\n",
      "index = VectorStoreIndex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create storage context using default stores\n",
      "sto...\n",
      "> Adding chunk: create storage context using default stores\n",
      "sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "We offer in-depth guides on the differ...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "We offer in-depth guides on the differ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Persisting & Loading Data...\n",
      "> Adding chunk: Persisting & Loading Data...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Persisting Data\n",
      "By default, LlamaIndex stores d...\n",
      "> Adding chunk: Persisting Data\n",
      "By default, LlamaIndex stores d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Loading Data\n",
      "To load data, user simply needs to...\n",
      "> Adding chunk: Loading Data\n",
      "To load data, user simply needs to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load a single index\n",
      "index = load_index_from_sto...\n",
      "> Adding chunk: load a single index\n",
      "index = load_index_from_sto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: don't need to specify index_id if there's only ...\n",
      "> Adding chunk: don't need to specify index_id if there's only ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load multiple indices\n",
      "indices = load_indices_fr...\n",
      "> Adding chunk: load multiple indices\n",
      "indices = load_indices_fr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load composable graph\n",
      "graph = load_graph_from_s...\n",
      "> Adding chunk: load composable graph\n",
      "graph = load_graph_from_s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using a remote backend\n",
      "\n",
      "By default, LlamaIndex ...\n",
      "> Adding chunk: Using a remote backend\n",
      "\n",
      "By default, LlamaIndex ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "> Adding chunk: load documents\n",
      "documents = SimpleDirectoryReade...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set up s3fs\n",
      "AWS_KEY = os.environ['AWS_ACCESS_KE...\n",
      "> Adding chunk: set up s3fs\n",
      "AWS_KEY = os.environ['AWS_ACCESS_KE...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: save index to remote blob storage\n",
      "index.set_ind...\n",
      "> Adding chunk: save index to remote blob storage\n",
      "index.set_ind...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: this is {bucket_name}/{index_name}\n",
      "index.storag...\n",
      "> Adding chunk: this is {bucket_name}/{index_name}\n",
      "index.storag...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load index from s3\n",
      "sc = StorageContext.from_def...\n",
      "> Adding chunk: load index from s3\n",
      "sc = StorageContext.from_def...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Stores\n",
      "\n",
      "Vector stores contain embedding ...\n",
      "> Adding chunk: Vector Stores\n",
      "\n",
      "Vector stores contain embedding ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Simple Vector Store\n",
      "\n",
      "By default, LlamaIndex use...\n",
      "> Adding chunk: Simple Vector Store\n",
      "\n",
      "By default, LlamaIndex use...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Store Options & Feature Support\n",
      "\n",
      "LlamaIn...\n",
      "> Adding chunk: Vector Store Options & Feature Support\n",
      "\n",
      "LlamaIn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ✓      | ✓               |       |\n",
      "| Redis     ...\n",
      "> Adding chunk: ✓      | ✓               |       |\n",
      "| Redis     ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: |                 |       |\n",
      "| ChatGPT Retrieval...\n",
      "> Adding chunk: |                 |       |\n",
      "| ChatGPT Retrieval...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Embeddings...\n",
      "> Adding chunk: Embeddings...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Embeddings are used in LlamaIndex to re...\n",
      "> Adding chunk: Concept\n",
      "Embeddings are used in LlamaIndex to re...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Most commonly in LlamaIndex, emb...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Most commonly in LlamaIndex, emb...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Getting Started\n",
      "\n",
      "The most common usage for an e...\n",
      "> Adding chunk: Getting Started\n",
      "\n",
      "The most common usage for an e...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: optionally set a global service context to avoi...\n",
      "> Adding chunk: optionally set a global service context to avoi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customization...\n",
      "> Adding chunk: Customization...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Batch Size\n",
      "\n",
      "By default, embeddings requests are...\n",
      "> Adding chunk: Batch Size\n",
      "\n",
      "By default, embeddings requests are...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set the batch size to 42\n",
      "embed_model = OpenAIEm...\n",
      "> Adding chunk: set the batch size to 42\n",
      "embed_model = OpenAIEm...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Local Embedding Models\n",
      "\n",
      "The easiest way to use ...\n",
      "> Adding chunk: Local Embedding Models\n",
      "\n",
      "The easiest way to use ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Embedding Model Integrations\n",
      "\n",
      "We also support a...\n",
      "> Adding chunk: Embedding Model Integrations\n",
      "\n",
      "We also support a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Custom Embedding Model\n",
      "\n",
      "If you wanted to use em...\n",
      "> Adding chunk: Custom Embedding Model\n",
      "\n",
      "If you wanted to use em...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Standalone Usage\n",
      "\n",
      "You can also use embeddings a...\n",
      "> Adding chunk: Standalone Usage\n",
      "\n",
      "You can also use embeddings a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, A...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OpenAI\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "> Adding chunk: OpenAI\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Anthropic\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exa...\n",
      "> Adding chunk: Anthropic\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Hugging Face\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "> Adding chunk: Hugging Face\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: PaLM\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/example...\n",
      "> Adding chunk: PaLM\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/example...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Predibase\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "> Adding chunk: Predibase\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Replicate\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "> Adding chunk: Replicate\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LangChain\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "> Adding chunk: LangChain\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Llama API\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exa...\n",
      "> Adding chunk: Llama API\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Llama CPP\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "> Adding chunk: Llama CPP\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/ex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Xorbits Inference\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "> Adding chunk: Xorbits Inference\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: MonsterAPI\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/e...\n",
      "> Adding chunk: MonsterAPI\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/e...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM...\n",
      "> Adding chunk: LLM...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Picking the proper Large Language Model...\n",
      "> Adding chunk: Concept\n",
      "Picking the proper Large Language Model...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "The following code snippet shows...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "The following code snippet shows...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: non-streaming\n",
      "resp = OpenAI().complete('Paul Gr...\n",
      "> Adding chunk: non-streaming\n",
      "resp = OpenAI().complete('Paul Gr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, H...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "We support integrations with OpenAI, H...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing LLMs within LlamaIndex Abstractions...\n",
      "> Adding chunk: Customizing LLMs within LlamaIndex Abstractions...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example: Changing the underlying LLM\n",
      "\n",
      "An exampl...\n",
      "> Adding chunk: Example: Changing the underlying LLM\n",
      "\n",
      "An exampl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: alternatively\n",
      "\n",
      "documents = SimpleDirectoryReade...\n",
      "> Adding chunk: alternatively\n",
      "\n",
      "documents = SimpleDirectoryReade...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define LLM\n",
      "llm = OpenAI(temperature=0.1, model=...\n",
      "> Adding chunk: define LLM\n",
      "llm = OpenAI(temperature=0.1, model=...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = KeywordTableIndex.from_docu...\n",
      "> Adding chunk: build index\n",
      "index = KeywordTableIndex.from_docu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: get response from query\n",
      "query_engine = index.as...\n",
      "> Adding chunk: get response from query\n",
      "query_engine = index.as...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example: Changing the number of output tokens (...\n",
      "> Adding chunk: Example: Changing the number of output tokens (...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define LLM\n",
      "llm = OpenAI(temperature=0, model=\"t...\n",
      "> Adding chunk: define LLM\n",
      "llm = OpenAI(temperature=0, model=\"t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example: Explicitly configure `context_window` ...\n",
      "> Adding chunk: Example: Explicitly configure `context_window` ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: alternatively\n",
      "\n",
      "documents = SimpleDirectoryReade...\n",
      "> Adding chunk: alternatively\n",
      "\n",
      "documents = SimpleDirectoryReade...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set context window\n",
      "context_window = 4096...\n",
      "> Adding chunk: set context window\n",
      "context_window = 4096...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set number of output tokens\n",
      "num_output = 256...\n",
      "> Adding chunk: set number of output tokens\n",
      "num_output = 256...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define LLM\n",
      "llm = OpenAI(\n",
      "    temperature=0, \n",
      "  ...\n",
      "> Adding chunk: define LLM\n",
      "llm = OpenAI(\n",
      "    temperature=0, \n",
      "  ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example: Using a HuggingFace LLM\n",
      "\n",
      "LlamaIndex su...\n",
      "> Adding chunk: Example: Using a HuggingFace LLM\n",
      "\n",
      "LlamaIndex su...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: This will wrap the default prompts that are int...\n",
      "> Adding chunk: This will wrap the default prompts that are int...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example: Using a Custom LLM Model - Advanced\n",
      "\n",
      "T...\n",
      "> Adding chunk: Example: Using a Custom LLM Model - Advanced\n",
      "\n",
      "T...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set context window size\n",
      "context_window = 2048...\n",
      "> Adding chunk: set context window size\n",
      "context_window = 2048...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set number of output tokens\n",
      "num_output = 256...\n",
      "> Adding chunk: set number of output tokens\n",
      "num_output = 256...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: store the pipeline/model outisde of the LLM cla...\n",
      "> Adding chunk: store the pipeline/model outisde of the LLM cla...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define our LLM\n",
      "llm = OurLLM()\n",
      "\n",
      "service_context ...\n",
      "> Adding chunk: define our LLM\n",
      "llm = OurLLM()\n",
      "\n",
      "service_context ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load the your data\n",
      "documents = SimpleDirectoryR...\n",
      "> Adding chunk: Load the your data\n",
      "documents = SimpleDirectoryR...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query and print response\n",
      "query_engine = index.a...\n",
      "> Adding chunk: Query and print response\n",
      "query_engine = index.a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using LLMs as standalone modules\n",
      "\n",
      "You can use o...\n",
      "> Adding chunk: Using LLMs as standalone modules\n",
      "\n",
      "You can use o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Text Completion Example\n",
      "\n",
      "```python\n",
      "from llama_i...\n",
      "> Adding chunk: Text Completion Example\n",
      "\n",
      "```python\n",
      "from llama_i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: non-streaming\n",
      "resp = OpenAI().complete('Paul Gr...\n",
      "> Adding chunk: non-streaming\n",
      "resp = OpenAI().complete('Paul Gr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: using streaming endpoint\n",
      "from llama_index.llms ...\n",
      "> Adding chunk: using streaming endpoint\n",
      "from llama_index.llms ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Chat Example\n",
      "\n",
      "```python\n",
      "from llama_index.llms i...\n",
      "> Adding chunk: Chat Example\n",
      "\n",
      "```python\n",
      "from llama_index.llms i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prompts...\n",
      "> Adding chunk: Prompts...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "Prompting is the fundamental input tha...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "Prompting is the fundamental input tha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining a custom prompt\n",
      "\n",
      "Defining a custom pro...\n",
      "> Adding chunk: Defining a custom prompt\n",
      "\n",
      "Defining a custom pro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: you can create text prompt (for completion API)...\n",
      "> Adding chunk: you can create text prompt (for completion API)...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: or easily convert to message prompts (for chat ...\n",
      "> Adding chunk: or easily convert to message prompts (for chat ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: you can create message prompts (for chat API)\n",
      "m...\n",
      "> Adding chunk: you can create message prompts (for chat API)\n",
      "m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: or easily convert to text prompt (for completio...\n",
      "> Adding chunk: or easily convert to text prompt (for completio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Passing custom prompts into the pipeline\n",
      "\n",
      "Since...\n",
      "> Adding chunk: Passing custom prompts into the pipeline\n",
      "\n",
      "Since...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modify prompts used in index construction\n",
      "Diffe...\n",
      "> Adding chunk: Modify prompts used in index construction\n",
      "Diffe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modify prompts used in query engine\n",
      "More common...\n",
      "> Adding chunk: Modify prompts used in query engine\n",
      "More common...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides\n",
      "\n",
      "We provide a few simple implemen...\n",
      "> Adding chunk: Module Guides\n",
      "\n",
      "We provide a few simple implemen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Chat Engine...\n",
      "> Adding chunk: Chat Engine...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Chat engine is a high-level interface f...\n",
      "> Adding chunk: Concept\n",
      "Chat engine is a high-level interface f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "chat_...\n",
      "> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "chat_...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "Below you can find corresponding tutori...\n",
      "> Adding chunk: Modules\n",
      "Below you can find corresponding tutori...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "\n",
      "Build a chat engine from index:\n",
      "``...\n",
      "> Adding chunk: Get Started\n",
      "\n",
      "Build a chat engine from index:\n",
      "``...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring a Chat Engine\n",
      "Configuring a chat en...\n",
      "> Adding chunk: Configuring a Chat Engine\n",
      "Configuring a chat en...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High-Level API\n",
      "You can directly build and confi...\n",
      "> Adding chunk: High-Level API\n",
      "You can directly build and confi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Available Chat Modes\n",
      "\n",
      "- `best` - Turn the query...\n",
      "> Adding chunk: Available Chat Modes\n",
      "\n",
      "- `best` - Turn the query...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-Level Composition API\n",
      "\n",
      "You can use the low-...\n",
      "> Adding chunk: Low-Level Composition API\n",
      "\n",
      "You can use the low-...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: list of `ChatMessage` objects\n",
      "custom_chat_histo...\n",
      "> Adding chunk: list of `ChatMessage` objects\n",
      "custom_chat_histo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Streaming\n",
      "To enable streaming, you simply need ...\n",
      "> Adding chunk: Streaming\n",
      "To enable streaming, you simply need ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules...\n",
      "> Adding chunk: Modules...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SimilarityPostprocessor\n",
      "\n",
      "Used to remove nodes t...\n",
      "> Adding chunk: SimilarityPostprocessor\n",
      "\n",
      "Used to remove nodes t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: KeywordNodePostprocessor\n",
      "\n",
      "Used to ensure certai...\n",
      "> Adding chunk: KeywordNodePostprocessor\n",
      "\n",
      "Used to ensure certai...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: MetadataReplacementPostProcessor\n",
      "\n",
      "Used to repla...\n",
      "> Adding chunk: MetadataReplacementPostProcessor\n",
      "\n",
      "Used to repla...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SentenceEmbeddingOptimizer\n",
      "\n",
      "This postprocessor ...\n",
      "> Adding chunk: SentenceEmbeddingOptimizer\n",
      "\n",
      "This postprocessor ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: CohereRerank\n",
      "\n",
      "Uses the \"Cohere ReRank\" function...\n",
      "> Adding chunk: CohereRerank\n",
      "\n",
      "Uses the \"Cohere ReRank\" function...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SentenceTransformerRerank\n",
      "\n",
      "Uses the cross-encod...\n",
      "> Adding chunk: SentenceTransformerRerank\n",
      "\n",
      "Uses the cross-encod...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: We choose a model with relatively high speed an...\n",
      "> Adding chunk: We choose a model with relatively high speed an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Rerank\n",
      "\n",
      "Uses a LLM to re-order nodes by ask...\n",
      "> Adding chunk: LLM Rerank\n",
      "\n",
      "Uses a LLM to re-order nodes by ask...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: FixedRecencyPostprocessor\n",
      "\n",
      "This postproccesor r...\n",
      "> Adding chunk: FixedRecencyPostprocessor\n",
      "\n",
      "This postproccesor r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: EmbeddingRecencyPostprocessor\n",
      "\n",
      "This postprocces...\n",
      "> Adding chunk: EmbeddingRecencyPostprocessor\n",
      "\n",
      "This postprocces...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: TimeWeightedPostprocessor\n",
      "\n",
      "This postproccesor r...\n",
      "> Adding chunk: TimeWeightedPostprocessor\n",
      "\n",
      "This postproccesor r...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (Beta) PIINodePostprocessor\n",
      "\n",
      "The PII (Personal ...\n",
      "> Adding chunk: (Beta) PIINodePostprocessor\n",
      "\n",
      "The PII (Personal ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Version\n",
      "\n",
      "```python\n",
      "from llama_index.indices...\n",
      "> Adding chunk: LLM Version\n",
      "\n",
      "```python\n",
      "from llama_index.indices...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NER Version\n",
      "\n",
      "This version uses the default loca...\n",
      "> Adding chunk: NER Version\n",
      "\n",
      "This version uses the default loca...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (Beta) PrevNextNodePostprocessor\n",
      "\n",
      "Uses pre-defi...\n",
      "> Adding chunk: (Beta) PrevNextNodePostprocessor\n",
      "\n",
      "Uses pre-defi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: (Beta) AutoPrevNextNodePostprocessor\n",
      "\n",
      "The same ...\n",
      "> Adding chunk: (Beta) AutoPrevNextNodePostprocessor\n",
      "\n",
      "The same ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All Notebooks\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---...\n",
      "> Adding chunk: All Notebooks\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Node Postprocessor...\n",
      "> Adding chunk: Node Postprocessor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Node postprocessors are a set of module...\n",
      "> Adding chunk: Concept\n",
      "Node postprocessors are a set of module...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "An example of using a node postp...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "An example of using a node postp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "Below you can find guides for each node...\n",
      "> Adding chunk: Modules\n",
      "Below you can find guides for each node...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Most commonly, node-postprocesso...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Most commonly, node-postprocesso...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with a Query Engine\n",
      "\n",
      "```python\n",
      "from llama...\n",
      "> Adding chunk: Using with a Query Engine\n",
      "\n",
      "```python\n",
      "from llama...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: all node post-processors will be applied during...\n",
      "> Adding chunk: all node post-processors will be applied during...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with Retrieved Nodes\n",
      "\n",
      "Or used as a standa...\n",
      "> Adding chunk: Using with Retrieved Nodes\n",
      "\n",
      "Or used as a standa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using with your own nodes\n",
      "\n",
      "As you may have noti...\n",
      "> Adding chunk: Using with your own nodes\n",
      "\n",
      "As you may have noti...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "> Adding chunk: filter nodes below 0.75 similarity score\n",
      "proces...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Custom Node PostProcessor\n",
      "\n",
      "The base class is `B...\n",
      "> Adding chunk: Custom Node PostProcessor\n",
      "\n",
      "The base class is `B...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Transformations\n",
      "\n",
      "\n",
      "LlamaIndex allows you t...\n",
      "> Adding chunk: Query Transformations\n",
      "\n",
      "\n",
      "LlamaIndex allows you t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use Cases\n",
      "Query transformations have multiple u...\n",
      "> Adding chunk: Use Cases\n",
      "Query transformations have multiple u...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: HyDE (Hypothetical Document Embeddings)\n",
      "\n",
      "HyDE i...\n",
      "> Adding chunk: HyDE (Hypothetical Document Embeddings)\n",
      "\n",
      "HyDE i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: run query with HyDE query transform\n",
      "query_str =...\n",
      "> Adding chunk: run query with HyDE query transform\n",
      "query_str =...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Single-Step Query Decomposition\n",
      "\n",
      "Some recent ap...\n",
      "> Adding chunk: Single-Step Query Decomposition\n",
      "\n",
      "Some recent ap...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting: a list index composed over multiple ve...\n",
      "> Adding chunk: Setting: a list index composed over multiple ve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize indexes and graph\n",
      "......\n",
      "> Adding chunk: initialize indexes and graph\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure retrievers\n",
      "vector_query_engine = vect...\n",
      "> Adding chunk: configure retrievers\n",
      "vector_query_engine = vect...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query\n",
      "query_str = (\n",
      "    \"Compare and contrast t...\n",
      "> Adding chunk: query\n",
      "query_str = (\n",
      "    \"Compare and contrast t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Multi-Step Query Transformations\n",
      "\n",
      "Multi-step qu...\n",
      "> Adding chunk: Multi-Step Query Transformations\n",
      "\n",
      "Multi-step qu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: gpt-4\n",
      "step_decompose_transform = StepDecomposeQ...\n",
      "> Adding chunk: gpt-4\n",
      "step_decompose_transform = StepDecomposeQ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides...\n",
      "> Adding chunk: Module Guides...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Basic\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "Retrieve...\n",
      "> Adding chunk: Basic\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "Retrieve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Structured & Semi-Structured Data\n",
      "```{toctree}\n",
      "...\n",
      "> Adding chunk: Structured & Semi-Structured Data\n",
      "```{toctree}\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "> Adding chunk: Advanced\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Experimental\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "> Adding chunk: Experimental\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Response Modes\n",
      "\n",
      "Right now, we support the follo...\n",
      "> Adding chunk: Response Modes\n",
      "\n",
      "Right now, we support the follo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Engine...\n",
      "> Adding chunk: Query Engine...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Query engine is a generic interface tha...\n",
      "> Adding chunk: Concept\n",
      "Query engine is a generic interface tha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "query...\n",
      "> Adding chunk: Usage Pattern\n",
      "Get started with:\n",
      "```python\n",
      "query...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 3\n",
      "---\n",
      "module...\n",
      "> Adding chunk: Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 3\n",
      "---\n",
      "module...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Supporting Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2...\n",
      "> Adding chunk: Supporting Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Streaming\n",
      "\n",
      "LlamaIndex supports streaming the re...\n",
      "> Adding chunk: Streaming\n",
      "\n",
      "LlamaIndex supports streaming the re...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setup\n",
      "To enable streaming, you need to use an L...\n",
      "> Adding chunk: Setup\n",
      "To enable streaming, you need to use an L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Streaming Response\n",
      "After properly configuring b...\n",
      "> Adding chunk: Streaming Response\n",
      "After properly configuring b...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Supporting Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "> Adding chunk: Supporting Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "Build a query engine from index:\n",
      "``...\n",
      "> Adding chunk: Get Started\n",
      "Build a query engine from index:\n",
      "``...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring a Query Engine\n",
      "You can directly bui...\n",
      "> Adding chunk: Configuring a Query Engine\n",
      "You can directly bui...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-Level Composition API\n",
      "\n",
      "You can use the low-...\n",
      "> Adding chunk: Low-Level Composition API\n",
      "\n",
      "You can use the low-...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure retriever\n",
      "retriever = VectorIndexRetr...\n",
      "> Adding chunk: configure retriever\n",
      "retriever = VectorIndexRetr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure response synthesizer\n",
      "response_synthes...\n",
      "> Adding chunk: configure response synthesizer\n",
      "response_synthes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: assemble query engine\n",
      "query_engine = RetrieverQ...\n",
      "> Adding chunk: assemble query engine\n",
      "query_engine = RetrieverQ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query\n",
      "response = query_engine.query(\"What did t...\n",
      "> Adding chunk: query\n",
      "response = query_engine.query(\"What did t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Streaming\n",
      "To enable streaming, you simply need ...\n",
      "> Adding chunk: Streaming\n",
      "To enable streaming, you simply need ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guide\n",
      "\n",
      "Detailed inputs/outputs for each ...\n",
      "> Adding chunk: Module Guide\n",
      "\n",
      "Detailed inputs/outputs for each ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: API Example\n",
      "\n",
      "The following shows the setup for ...\n",
      "> Adding chunk: API Example\n",
      "\n",
      "The following shows the setup for ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: synchronous\n",
      "response = response_synthesizer.syn...\n",
      "> Adding chunk: synchronous\n",
      "response = response_synthesizer.syn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: asynchronous\n",
      "response = await response_synthesi...\n",
      "> Adding chunk: asynchronous\n",
      "response = await response_synthesi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Example Notebooks\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "> Adding chunk: Example Notebooks\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Response Synthesizer...\n",
      "> Adding chunk: Response Synthesizer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "A `Response Synthesizer` is what genera...\n",
      "> Adding chunk: Concept\n",
      "A `Response Synthesizer` is what genera...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "Use a response synthesizer on it'...\n",
      "> Adding chunk: Usage Pattern\n",
      "Use a response synthesizer on it'...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "Below you can find detailed API informa...\n",
      "> Adding chunk: Modules\n",
      "Below you can find detailed API informa...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "\n",
      "Configuring the response synthesiz...\n",
      "> Adding chunk: Get Started\n",
      "\n",
      "Configuring the response synthesiz...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring the Response Mode\n",
      "Response synthesi...\n",
      "> Adding chunk: Configuring the Response Mode\n",
      "Response synthesi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Custom Response Synthesizers\n",
      "\n",
      "Each response syn...\n",
      "> Adding chunk: Custom Response Synthesizers\n",
      "\n",
      "Each response syn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using Structured Answer Filtering\n",
      "When using ei...\n",
      "> Adding chunk: Using Structured Answer Filtering\n",
      "When using ei...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Module Guides\n",
      "We are actively adding more tailo...\n",
      "> Adding chunk: Module Guides\n",
      "We are actively adding more tailo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Index Retrievers\n",
      "\n",
      "Please see the retriever mode...\n",
      "> Adding chunk: Index Retrievers\n",
      "\n",
      "Please see the retriever mode...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced Retriever Guides\n",
      "\n",
      "Check out our compre...\n",
      "> Adding chunk: Advanced Retriever Guides\n",
      "\n",
      "Check out our compre...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: External Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "> Adding chunk: External Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graph Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "max...\n",
      "> Adding chunk: Knowledge Graph Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "max...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Composed Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "> Adding chunk: Composed Retrievers\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retriever Modes\n",
      "Here we show the mapping from `...\n",
      "> Adding chunk: Retriever Modes\n",
      "Here we show the mapping from `...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector Index\n",
      "Specifying `retriever_mode` has no...\n",
      "> Adding chunk: Vector Index\n",
      "Specifying `retriever_mode` has no...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: List Index\n",
      "* `default`: ListIndexRetriever \n",
      "* `...\n",
      "> Adding chunk: List Index\n",
      "* `default`: ListIndexRetriever \n",
      "* `...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tree Index\n",
      "* `select_leaf`: TreeSelectLeafRetri...\n",
      "> Adding chunk: Tree Index\n",
      "* `select_leaf`: TreeSelectLeafRetri...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Keyword Table Index\n",
      "* `default`: KeywordTableGP...\n",
      "> Adding chunk: Keyword Table Index\n",
      "* `default`: KeywordTableGP...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graph Index\n",
      "* `keyword`: KGTableRetri...\n",
      "> Adding chunk: Knowledge Graph Index\n",
      "* `keyword`: KGTableRetri...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Document Summary Index\n",
      "* `default`: DocumentSum...\n",
      "> Adding chunk: Document Summary Index\n",
      "* `default`: DocumentSum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retriever...\n",
      "> Adding chunk: Retriever...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "Retrievers are responsible for fetchin...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "Retrievers are responsible for fetchin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Get started with:\n",
      "```python\n",
      "retr...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Get started with:\n",
      "```python\n",
      "retr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2\n",
      "---\n",
      "module...\n",
      "> Adding chunk: Modules\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 2\n",
      "---\n",
      "module...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Get Started\n",
      "Get a retriever from index:\n",
      "```pyth...\n",
      "> Adding chunk: Get Started\n",
      "Get a retriever from index:\n",
      "```pyth...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High-Level API...\n",
      "> Adding chunk: High-Level API...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Selecting a Retriever\n",
      "\n",
      "You can select the index...\n",
      "> Adding chunk: Selecting a Retriever\n",
      "\n",
      "You can select the index...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring a Retriever\n",
      "In the same way, you ca...\n",
      "> Adding chunk: Configuring a Retriever\n",
      "In the same way, you ca...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-Level Composition API\n",
      "You can use the low-l...\n",
      "> Adding chunk: Low-Level Composition API\n",
      "You can use the low-l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "Defi...\n",
      "> Adding chunk: Advanced\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "Defi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Routers...\n",
      "> Adding chunk: Routers...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Routers are modules that take in a user...\n",
      "> Adding chunk: Concept\n",
      "Routers are modules that take in a user...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "A simple example of using our ro...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "A simple example of using our ro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "Below you can find extensive guides usi...\n",
      "> Adding chunk: Modules\n",
      "Below you can find extensive guides usi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Defining a \"selector\" is at the ...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Defining a \"selector\" is at the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining a selector\n",
      "\n",
      "Some examples are given be...\n",
      "> Adding chunk: Defining a selector\n",
      "\n",
      "Some examples are given be...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: pydantic selectors feed in pydantic objects to ...\n",
      "> Adding chunk: pydantic selectors feed in pydantic objects to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: multi selector (pydantic)\n",
      "selector = PydanticMu...\n",
      "> Adding chunk: multi selector (pydantic)\n",
      "selector = PydanticMu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM selectors use text completion endpoints\n",
      "sel...\n",
      "> Adding chunk: LLM selectors use text completion endpoints\n",
      "sel...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: multi selector (LLM)\n",
      "selector = LLMMultiSelecto...\n",
      "> Adding chunk: multi selector (LLM)\n",
      "selector = LLMMultiSelecto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using as a Query Engine\n",
      "\n",
      "A `RouterQueryEngine` ...\n",
      "> Adding chunk: Using as a Query Engine\n",
      "\n",
      "A `RouterQueryEngine` ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query engines\n",
      "......\n",
      "> Adding chunk: define query engines\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize tools\n",
      "list_tool = QueryEngineTool.fr...\n",
      "> Adding chunk: initialize tools\n",
      "list_tool = QueryEngineTool.fr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize router query engine (single selectio...\n",
      "> Adding chunk: initialize router query engine (single selectio...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using as a Retriever\n",
      "\n",
      "Similarly, a `RouterRetri...\n",
      "> Adding chunk: Using as a Retriever\n",
      "\n",
      "Similarly, a `RouterRetri...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define indices\n",
      "......\n",
      "> Adding chunk: define indices\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define retrievers\n",
      "vector_retriever = vector_ind...\n",
      "> Adding chunk: define retrievers\n",
      "vector_retriever = vector_ind...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize tools\n",
      "vector_tool = RetrieverTool.fr...\n",
      "> Adding chunk: initialize tools\n",
      "vector_tool = RetrieverTool.fr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define retriever\n",
      "retriever = RouterRetriever(\n",
      " ...\n",
      "> Adding chunk: define retriever\n",
      "retriever = RouterRetriever(\n",
      " ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using selector as a standalone module\n",
      "\n",
      "You can ...\n",
      "> Adding chunk: Using selector as a standalone module\n",
      "\n",
      "You can ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: choices as a list of tool metadata\n",
      "choices = [\n",
      "...\n",
      "> Adding chunk: choices as a list of tool metadata\n",
      "choices = [\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: choices as a list of strings\n",
      "choices = [\"choice...\n",
      "> Adding chunk: choices as a list of strings\n",
      "choices = [\"choice...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Output Parsing\n",
      "\n",
      "LlamaIndex supports integration...\n",
      "> Adding chunk: Output Parsing\n",
      "\n",
      "LlamaIndex supports integration...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guardrails\n",
      "\n",
      "Guardrails is an open-source Python...\n",
      "> Adding chunk: Guardrails\n",
      "\n",
      "Guardrails is an open-source Python...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: specify StructuredLLMPredictor...\n",
      "> Adding chunk: specify StructuredLLMPredictor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query / output spec\n",
      "rail_spec = (\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "> Adding chunk: define query / output spec\n",
      "rail_spec = (\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define output parser\n",
      "output_parser = Guardrails...\n",
      "> Adding chunk: define output parser\n",
      "output_parser = Guardrails...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: format each prompt with output parser instructi...\n",
      "> Adding chunk: format each prompt with output parser instructi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: obtain a structured response\n",
      "query_engine = ind...\n",
      "> Adding chunk: obtain a structured response\n",
      "query_engine = ind...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Langchain\n",
      "\n",
      "Langchain also offers output parsing...\n",
      "> Adding chunk: Langchain\n",
      "\n",
      "Langchain also offers output parsing...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "> Adding chunk: load documents, build index\n",
      "documents = SimpleD...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define output schema\n",
      "response_schemas = [\n",
      "    R...\n",
      "> Adding chunk: define output schema\n",
      "response_schemas = [\n",
      "    R...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define output parser\n",
      "lc_output_parser = Structu...\n",
      "> Adding chunk: define output parser\n",
      "lc_output_parser = Structu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: format each prompt with output parser instructi...\n",
      "> Adding chunk: format each prompt with output parser instructi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = index.as_query_engin...\n",
      "> Adding chunk: query index\n",
      "query_engine = index.as_query_engin...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "caption: Examples\n",
      "maxd...\n",
      "> Adding chunk: Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "caption: Examples\n",
      "maxd...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pydantic Program\n",
      "\n",
      "A pydantic program is a gener...\n",
      "> Adding chunk: Pydantic Program\n",
      "\n",
      "A pydantic program is a gener...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Text Completion Pydantic Programs\n",
      "TODO: Com...\n",
      "> Adding chunk: LLM Text Completion Pydantic Programs\n",
      "TODO: Com...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Function Calling Pydantic Programs\n",
      "```{toct...\n",
      "> Adding chunk: LLM Function Calling Pydantic Programs\n",
      "```{toct...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prepackaged Pydantic Programs\n",
      "```{toctree}\n",
      "---\n",
      "...\n",
      "> Adding chunk: Prepackaged Pydantic Programs\n",
      "```{toctree}\n",
      "---\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Structured Outputs\n",
      "\n",
      "The ability of LLMs to prod...\n",
      "> Adding chunk: Structured Outputs\n",
      "\n",
      "The ability of LLMs to prod...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 🔬 Anatomy of a Structured Output Function\n",
      "\n",
      "Here...\n",
      "> Adding chunk: 🔬 Anatomy of a Structured Output Function\n",
      "\n",
      "Here...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Output Parser Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdept...\n",
      "> Adding chunk: Output Parser Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdept...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pydantic Program Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxd...\n",
      "> Adding chunk: Pydantic Program Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxd...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Callbacks...\n",
      "> Adding chunk: Callbacks...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "LlamaIndex provides callbacks to help d...\n",
      "> Adding chunk: Concept\n",
      "LlamaIndex provides callbacks to help d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "Currently supported callbacks are as f...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "Currently supported callbacks are as f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Token Counting - Migration Guide\n",
      "\n",
      "The existing ...\n",
      "> Adding chunk: Token Counting - Migration Guide\n",
      "\n",
      "The existing ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: you can set a tokenizer directly, or optionally...\n",
      "> Adding chunk: you can set a tokenizer directly, or optionally...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: if verbose is turned on, you will see embedding...\n",
      "> Adding chunk: if verbose is turned on, you will see embedding...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: otherwise, you can access the count directly\n",
      "pr...\n",
      "> Adding chunk: otherwise, you can access the count directly\n",
      "pr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: reset the counts at your discretion!\n",
      "token_coun...\n",
      "> Adding chunk: reset the counts at your discretion!\n",
      "token_coun...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: also track prompt, completion, and total LLM to...\n",
      "> Adding chunk: also track prompt, completion, and total LLM to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Cost Analysis...\n",
      "> Adding chunk: Cost Analysis...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Each call to an LLM will cost some amou...\n",
      "> Adding chunk: Concept\n",
      "Each call to an LLM will cost some amou...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Overview of Cost Structure...\n",
      "> Adding chunk: Overview of Cost Structure...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Indices with no LLM calls\n",
      "The following indices...\n",
      "> Adding chunk: Indices with no LLM calls\n",
      "The following indices...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Indices with LLM calls\n",
      "The following indices do...\n",
      "> Adding chunk: Indices with LLM calls\n",
      "The following indices do...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Query Time\n",
      "\n",
      "There will always be >= 1 LLM call ...\n",
      "> Adding chunk: Query Time\n",
      "\n",
      "There will always be >= 1 LLM call ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "LlamaIndex offers token **predic...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "LlamaIndex offers token **predic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using MockLLM\n",
      "\n",
      "To predict token usage of LLM ca...\n",
      "> Adding chunk: Using MockLLM\n",
      "\n",
      "To predict token usage of LLM ca...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: optionally set a global service context\n",
      "set_glo...\n",
      "> Adding chunk: optionally set a global service context\n",
      "set_glo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using MockEmbedding\n",
      "\n",
      "You may also predict the t...\n",
      "> Adding chunk: Using MockEmbedding\n",
      "\n",
      "You may also predict the t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: specify a MockLLMPredictor\n",
      "embed_model = MockEm...\n",
      "> Adding chunk: specify a MockLLMPredictor\n",
      "embed_model = MockEm...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: optionally set a global service context\n",
      "set_glo...\n",
      "> Adding chunk: optionally set a global service context\n",
      "set_glo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "Read about the full usage patter...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "Read about the full usage patter...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Estimating LLM and Embedding Token Counts\n",
      "\n",
      "In o...\n",
      "> Adding chunk: Estimating LLM and Embedding Token Counts\n",
      "\n",
      "In o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: reset counts\n",
      "token_counter.reset_counts()\n",
      "```\n",
      "\n",
      "...\n",
      "> Adding chunk: reset counts\n",
      "token_counter.reset_counts()\n",
      "```\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "Notebooks with usage of these componen...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "Notebooks with usage of these componen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluation...\n",
      "> Adding chunk: Evaluation...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "Evaluation in generative AI and retriev...\n",
      "> Adding chunk: Concept\n",
      "Evaluation in generative AI and retriev...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluation of the Response + Context\n",
      "\n",
      "Each resp...\n",
      "> Adding chunk: Evaluation of the Response + Context\n",
      "\n",
      "Each resp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluation of the Query + Response + Source Con...\n",
      "> Adding chunk: Evaluation of the Query + Response + Source Con...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Question Generation\n",
      "\n",
      "In addition to evaluating ...\n",
      "> Adding chunk: Question Generation\n",
      "\n",
      "In addition to evaluating ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "For full usage details, see the ...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "For full usage details, see the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "Notebooks with usage of these componen...\n",
      "> Adding chunk: Modules\n",
      "\n",
      "Notebooks with usage of these componen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluating Response for Hallucination...\n",
      "> Adding chunk: Evaluating Response for Hallucination...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Binary Evaluation\n",
      "\n",
      "This mode of evaluation will...\n",
      "> Adding chunk: Binary Evaluation\n",
      "\n",
      "This mode of evaluation will...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "......\n",
      "> Adding chunk: build index\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define evaluator\n",
      "evaluator = ResponseEvaluator(...\n",
      "> Adding chunk: define evaluator\n",
      "evaluator = ResponseEvaluator(...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Sources Evaluation\n",
      "\n",
      "This mode of evaluation wil...\n",
      "> Adding chunk: Sources Evaluation\n",
      "\n",
      "This mode of evaluation wil...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "......\n",
      "> Adding chunk: build index\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define evaluator\n",
      "evaluator = ResponseEvaluator(...\n",
      "> Adding chunk: define evaluator\n",
      "evaluator = ResponseEvaluator(...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluting Query + Response for Answer Quality...\n",
      "> Adding chunk: Evaluting Query + Response for Answer Quality...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Binary Evaluation\n",
      "\n",
      "This mode of evaluation will...\n",
      "> Adding chunk: Binary Evaluation\n",
      "\n",
      "This mode of evaluation will...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "......\n",
      "> Adding chunk: build index\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define evaluator\n",
      "evaluator = QueryResponseEvalu...\n",
      "> Adding chunk: define evaluator\n",
      "evaluator = QueryResponseEvalu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Sources Evaluation\n",
      "\n",
      "This mode of evaluation wil...\n",
      "> Adding chunk: Sources Evaluation\n",
      "\n",
      "This mode of evaluation wil...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "......\n",
      "> Adding chunk: build index\n",
      "......\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define evaluator\n",
      "evaluator = QueryResponseEvalu...\n",
      "> Adding chunk: define evaluator\n",
      "evaluator = QueryResponseEvalu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "> Adding chunk: query index\n",
      "query_engine = vector_index.as_quer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Question Generation\n",
      "\n",
      "LlamaIndex can also genera...\n",
      "> Adding chunk: Question Generation\n",
      "\n",
      "LlamaIndex can also genera...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "> Adding chunk: build service context\n",
      "llm = OpenAI(model=\"gpt-4...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build documents\n",
      "documents = SimpleDirectoryRead...\n",
      "> Adding chunk: build documents\n",
      "documents = SimpleDirectoryRead...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define genertor, generate questions\n",
      "data_genera...\n",
      "> Adding chunk: define genertor, generate questions\n",
      "data_genera...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Playground...\n",
      "> Adding chunk: Playground...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "\n",
      "The Playground module in LlamaIndex is...\n",
      "> Adding chunk: Concept\n",
      "\n",
      "The Playground module in LlamaIndex is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "A sample usage is given below.\n",
      "\n",
      "...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "A sample usage is given below.\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load data\n",
      "WikipediaReader = download_loader(\"Wi...\n",
      "> Adding chunk: load data\n",
      "WikipediaReader = download_loader(\"Wi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define multiple index data structures (vector i...\n",
      "> Adding chunk: define multiple index data structures (vector i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize playground\n",
      "playground = Playground(i...\n",
      "> Adding chunk: initialize playground\n",
      "playground = Playground(i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: playground compare\n",
      "playground.compare(\"What is ...\n",
      "> Adding chunk: playground compare\n",
      "playground.compare(\"What is ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "../.....\n",
      "> Adding chunk: Modules\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "../.....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ServiceContext...\n",
      "> Adding chunk: ServiceContext...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concept\n",
      "The `ServiceContext` is a bundle of com...\n",
      "> Adding chunk: Concept\n",
      "The `ServiceContext` is a bundle of com...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern...\n",
      "> Adding chunk: Usage Pattern...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring the service context\n",
      "The `ServiceCon...\n",
      "> Adding chunk: Configuring the service context\n",
      "The `ServiceCon...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting global configuration\n",
      "You can set a serv...\n",
      "> Adding chunk: Setting global configuration\n",
      "You can set a serv...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting local configuration\n",
      "You can pass in a s...\n",
      "> Adding chunk: Setting local configuration\n",
      "You can pass in a s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Deprecated Terms\n",
      "\n",
      "As LlamaIndex continues to ev...\n",
      "> Adding chunk: Deprecated Terms\n",
      "\n",
      "As LlamaIndex continues to ev...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: GPTSimpleVectorIndex\n",
      "\n",
      "This has been renamed to ...\n",
      "> Adding chunk: GPTSimpleVectorIndex\n",
      "\n",
      "This has been renamed to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: GPTVectorStoreIndex\n",
      "\n",
      "This has been renamed to `...\n",
      "> Adding chunk: GPTVectorStoreIndex\n",
      "\n",
      "This has been renamed to `...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLMPredictor\n",
      "\n",
      "The `LLMPredictor` object is no l...\n",
      "> Adding chunk: LLMPredictor\n",
      "\n",
      "The `LLMPredictor` object is no l...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: PromptHelper and max_input_size/\n",
      "\n",
      "The `max_inpu...\n",
      "> Adding chunk: PromptHelper and max_input_size/\n",
      "\n",
      "The `max_inpu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. mdinclude:: ../../CHANGELOG.md...\n",
      "> Adding chunk: .. mdinclude:: ../../CHANGELOG.md...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. mdinclude:: ../../CONTRIBUTING.md...\n",
      "> Adding chunk: .. mdinclude:: ../../CONTRIBUTING.md...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. mdinclude:: ../DOCS_README.md...\n",
      "> Adding chunk: .. mdinclude:: ../DOCS_README.md...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Privacy and Security\n",
      "By default, LLamaIndex sen...\n",
      "> Adding chunk: Privacy and Security\n",
      "By default, LLamaIndex sen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Privacy\n",
      "Regarding data privacy, when using...\n",
      "> Adding chunk: Data Privacy\n",
      "Regarding data privacy, when using...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Vector stores\n",
      "LLamaIndex offers modules to conn...\n",
      "> Adding chunk: Vector stores\n",
      "LLamaIndex offers modules to conn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Agents...\n",
      "> Adding chunk: Agents...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Context\n",
      "An \"agent\" is an automated reasoning an...\n",
      "> Adding chunk: Context\n",
      "An \"agent\" is an automated reasoning an...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Agents + LlamaIndex\n",
      "\n",
      "LlamaIndex provides some a...\n",
      "> Adding chunk: Agents + LlamaIndex\n",
      "\n",
      "LlamaIndex provides some a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: \"Agent-like\" Components within LlamaIndex\n",
      "\n",
      "Llam...\n",
      "> Adding chunk: \"Agent-like\" Components within LlamaIndex\n",
      "\n",
      "Llam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using LlamaIndex as as Tool within an Agent Fra...\n",
      "> Adding chunk: Using LlamaIndex as as Tool within an Agent Fra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LangChain\n",
      "\n",
      "We have deep integrations with LangC...\n",
      "> Adding chunk: LangChain\n",
      "\n",
      "We have deep integrations with LangC...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ChatGPT\n",
      "\n",
      "LlamaIndex can be used as a ChatGPT re...\n",
      "> Adding chunk: ChatGPT\n",
      "\n",
      "LlamaIndex can be used as a ChatGPT re...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Native OpenAIAgent\n",
      "\n",
      "With the new OpenAI API tha...\n",
      "> Adding chunk: Native OpenAIAgent\n",
      "\n",
      "With the new OpenAI API tha...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A Guide to Building a Full-Stack Web App with L...\n",
      "> Adding chunk: A Guide to Building a Full-Stack Web App with L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Flask Backend\n",
      "\n",
      "For this guide, our backend will...\n",
      "> Adding chunk: Flask Backend\n",
      "\n",
      "For this guide, our backend will...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Basic Flask - Handling User Index Queries\n",
      "\n",
      "Now,...\n",
      "> Adding chunk: Basic Flask - Handling User Index Queries\n",
      "\n",
      "Now,...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: for local testing only, do NOT deploy wit...\n",
      "> Adding chunk: NOTE: for local testing only, do NOT deploy wit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Advanced Flask - Handling User Document Uploads...\n",
      "> Adding chunk: Advanced Flask - Handling User Document Uploads...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: for local testing only, do NOT deploy wit...\n",
      "> Adding chunk: NOTE: for local testing only, do NOT deploy wit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize manager connection\n",
      "manager = BaseMan...\n",
      "> Adding chunk: initialize manager connection\n",
      "manager = BaseMan...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Let's implement that inside `index_server.py`:\n",
      "...\n",
      "> Adding chunk: Let's implement that inside `index_server.py`:\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: React Frontend\n",
      "\n",
      "Generally, React and Typescript...\n",
      "> Adding chunk: React Frontend\n",
      "\n",
      "Generally, React and Typescript...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: fetchDocuments.tsx\n",
      "\n",
      "This file contains the func...\n",
      "> Adding chunk: fetchDocuments.tsx\n",
      "\n",
      "This file contains the func...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: queryIndex.tsx\n",
      "\n",
      "This file sends the user query ...\n",
      "> Adding chunk: queryIndex.tsx\n",
      "\n",
      "This file sends the user query ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: insertDocument.tsx\n",
      "\n",
      "Probably the most complex A...\n",
      "> Adding chunk: insertDocument.tsx\n",
      "\n",
      "Probably the most complex A...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All the Other Frontend Good-ness\n",
      "\n",
      "And that pret...\n",
      "> Adding chunk: All the Other Frontend Good-ness\n",
      "\n",
      "And that pret...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Conclusion\n",
      "\n",
      "This guide has covered a ton of inf...\n",
      "> Adding chunk: Conclusion\n",
      "\n",
      "This guide has covered a ton of inf...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A Guide to Building a Full-Stack LlamaIndex Web...\n",
      "> Adding chunk: A Guide to Building a Full-Stack LlamaIndex Web...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: What We're Building\n",
      "\n",
      "Here's a quick demo of the...\n",
      "> Adding chunk: What We're Building\n",
      "\n",
      "Here's a quick demo of the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Architectural Overview\n",
      "\n",
      "Delphic leverages the L...\n",
      "> Adding chunk: Architectural Overview\n",
      "\n",
      "Delphic leverages the L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: System Requirements\n",
      "\n",
      "Celery doesn't work on Win...\n",
      "> Adding chunk: System Requirements\n",
      "\n",
      "Celery doesn't work on Win...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Django Backend...\n",
      "> Adding chunk: Django Backend...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Project Directory Overview\n",
      "\n",
      "The Delphic applica...\n",
      "> Adding chunk: Project Directory Overview\n",
      "\n",
      "The Delphic applica...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Database Models\n",
      "\n",
      "The Delphic application has tw...\n",
      "> Adding chunk: Database Models\n",
      "\n",
      "The Delphic application has tw...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Django Ninja API\n",
      "\n",
      "Django Ninja is a web framewo...\n",
      "> Adding chunk: Django Ninja API\n",
      "\n",
      "Django Ninja is a web framewo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ```python\n",
      "@collections_router.get(\"/available\",...\n",
      "> Adding chunk: ```python\n",
      "@collections_router.get(\"/available\",...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Intro to Websockets\n",
      "\n",
      "WebSockets are a communica...\n",
      "> Adding chunk: Intro to Websockets\n",
      "\n",
      "WebSockets are a communica...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Websocket Handler\n",
      "\n",
      "The `CollectionQueryConsumer...\n",
      "> Adding chunk: Websocket Handler\n",
      "\n",
      "The `CollectionQueryConsumer...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Websocket connect listener\n",
      "\n",
      "The `connect` metho...\n",
      "> Adding chunk: Websocket connect listener\n",
      "\n",
      "The `connect` metho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Websocket disconnect listener\n",
      "\n",
      "The `disconnect`...\n",
      "> Adding chunk: Websocket disconnect listener\n",
      "\n",
      "The `disconnect`...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Websocket receive listener\n",
      "\n",
      "The `receive` metho...\n",
      "> Adding chunk: Websocket receive listener\n",
      "\n",
      "The `receive` metho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: # Retrieve the Collection object\n",
      "    collection...\n",
      "> Adding chunk: # Retrieve the Collection object\n",
      "    collection...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: React Frontend...\n",
      "> Adding chunk: React Frontend...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Overview\n",
      "\n",
      "We chose to use TypeScript, React and...\n",
      "> Adding chunk: Overview\n",
      "\n",
      "We chose to use TypeScript, React and...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Frontend Project Structure\n",
      "\n",
      "The frontend can be...\n",
      "> Adding chunk: Frontend Project Structure\n",
      "\n",
      "The frontend can be...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Grabbing Collections from the Backend\n",
      "\n",
      "The coll...\n",
      "> Adding chunk: Grabbing Collections from the Backend\n",
      "\n",
      "The coll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Displaying Collections\n",
      "\n",
      "The latest collectios a...\n",
      "> Adding chunk: Displaying Collections\n",
      "\n",
      "The latest collectios a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Chat View Component\n",
      "\n",
      "The `ChatView` component i...\n",
      "> Adding chunk: Chat View Component\n",
      "\n",
      "The `ChatView` component i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Chat Websocket Client\n",
      "\n",
      "The WebSocket connection...\n",
      "> Adding chunk: Chat Websocket Client\n",
      "\n",
      "The WebSocket connection...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: In the callback, the component checks for a spe...\n",
      "> Adding chunk: In the callback, the component checks for a spe...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Rendering our Chat Messages\n",
      "\n",
      "In the `ChatView` ...\n",
      "> Adding chunk: Rendering our Chat Messages\n",
      "\n",
      "In the `ChatView` ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Deployment...\n",
      "> Adding chunk: Deployment...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Prerequisites\n",
      "\n",
      "To deploy the app, you're going ...\n",
      "> Adding chunk: Prerequisites\n",
      "\n",
      "To deploy the app, you're going ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Build and Deploy\n",
      "\n",
      "The project is based on djang...\n",
      "> Adding chunk: Build and Deploy\n",
      "\n",
      "The project is based on djang...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using the Application...\n",
      "> Adding chunk: Using the Application...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setup Users\n",
      "\n",
      "In order to actually use the appli...\n",
      "> Adding chunk: Setup Users\n",
      "\n",
      "In order to actually use the appli...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Full-Stack Web Application\n",
      "\n",
      "LlamaIndex can be i...\n",
      "> Adding chunk: Full-Stack Web Application\n",
      "\n",
      "LlamaIndex can be i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 💬🤖 How to Build a Chatbot\n",
      "\n",
      "LlamaIndex is an int...\n",
      "> Adding chunk: 💬🤖 How to Build a Chatbot\n",
      "\n",
      "LlamaIndex is an int...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Context\n",
      "\n",
      "In this tutorial, we build an \"10-K Ch...\n",
      "> Adding chunk: Context\n",
      "\n",
      "In this tutorial, we build an \"10-K Ch...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Ingest Data\n",
      "\n",
      "Let's first download the raw 10-k ...\n",
      "> Adding chunk: Ingest Data\n",
      "\n",
      "Let's first download the raw 10-k ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: the code examples assume you're operating...\n",
      "> Adding chunk: NOTE: the code examples assume you're operating...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting up Vector Indices for each year\n",
      "\n",
      "We fir...\n",
      "> Adding chunk: Setting up Vector Indices for each year\n",
      "\n",
      "We fir...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: initialize simple vector indices + global vecto...\n",
      "> Adding chunk: initialize simple vector indices + global vecto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load indices from disk\n",
      "index_set = {}\n",
      "for year ...\n",
      "> Adding chunk: Load indices from disk\n",
      "index_set = {}\n",
      "for year ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Composing a Graph to Synthesize Answers Across ...\n",
      "> Adding chunk: Composing a Graph to Synthesize Answers Across ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: describe each index to help traversal of compos...\n",
      "> Adding chunk: describe each index to help traversal of compos...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define an LLMPredictor set number of output tok...\n",
      "> Adding chunk: define an LLMPredictor set number of output tok...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define a list index over the vector indices\n",
      "gra...\n",
      "> Adding chunk: define a list index over the vector indices\n",
      "gra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: [optional] save to disk\n",
      "storage_context.persist...\n",
      "> Adding chunk: [optional] save to disk\n",
      "storage_context.persist...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: [optional] load from disk, so you don't need to...\n",
      "> Adding chunk: [optional] load from disk, so you don't need to...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting up the Tools + Langchain Chatbot Agent\n",
      "...\n",
      "> Adding chunk: Setting up the Tools + Langchain Chatbot Agent\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: do imports\n",
      "from langchain.chains.conversation.m...\n",
      "> Adding chunk: do imports\n",
      "from langchain.chains.conversation.m...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define a decompose transform\n",
      "from llama_index.i...\n",
      "> Adding chunk: define a decompose transform\n",
      "from llama_index.i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define custom retrievers\n",
      "from llama_index.query...\n",
      "> Adding chunk: define custom retrievers\n",
      "from llama_index.query...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: construct query engine\n",
      "graph_query_engine = gra...\n",
      "> Adding chunk: construct query engine\n",
      "graph_query_engine = gra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: tool config\n",
      "graph_config = IndexToolConfig(\n",
      "   ...\n",
      "> Adding chunk: tool config\n",
      "graph_config = IndexToolConfig(\n",
      "   ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define toolkit\n",
      "index_configs = []\n",
      "for y in rang...\n",
      "> Adding chunk: define toolkit\n",
      "index_configs = []\n",
      "for y in rang...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Testing the Agent\n",
      "\n",
      "We can now test the agent wi...\n",
      "> Adding chunk: Testing the Agent\n",
      "\n",
      "We can now test the agent wi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: INFO:llama_index.token_counter.token_counter:> ...\n",
      "> Adding chunk: INFO:llama_index.token_counter.token_counter:> ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting up the Chatbot Loop\n",
      "\n",
      "Now that we have t...\n",
      "> Adding chunk: Setting up the Chatbot Loop\n",
      "\n",
      "Now that we have t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Notebook\n",
      "\n",
      "Take a look at our corresponding note...\n",
      "> Adding chunk: Notebook\n",
      "\n",
      "Take a look at our corresponding note...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Chatbots\n",
      "\n",
      "Chatbots are an incredibly popular us...\n",
      "> Adding chunk: Chatbots\n",
      "\n",
      "Chatbots are an incredibly popular us...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Component Wise Evaluation\n",
      "To do more in-depth e...\n",
      "> Adding chunk: Component Wise Evaluation\n",
      "To do more in-depth e...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Utilizing public benchmarks\n",
      "When doing initial ...\n",
      "> Adding chunk: Utilizing public benchmarks\n",
      "When doing initial ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluating Retrieval\n",
      "\n",
      "BEIR is useful for benchm...\n",
      "> Adding chunk: Evaluating Retrieval\n",
      "\n",
      "BEIR is useful for benchm...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluating the Query Engine Components (e.g. Wi...\n",
      "> Adding chunk: Evaluating the Query Engine Components (e.g. Wi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: HotpotQA Dataset\n",
      "\n",
      "The HotpotQA dataset is usefu...\n",
      "> Adding chunk: HotpotQA Dataset\n",
      "\n",
      "The HotpotQA dataset is usefu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The Development Pathway\n",
      "\n",
      "In your journey to dev...\n",
      "> Adding chunk: The Development Pathway\n",
      "\n",
      "In your journey to dev...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The Challenges of Building a Production-Ready L...\n",
      "> Adding chunk: The Challenges of Building a Production-Ready L...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Quality and User Interaction\n",
      "On the tamer side,...\n",
      "> Adding chunk: Quality and User Interaction\n",
      "On the tamer side,...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Tradeoffs in LLM Application Development\n",
      "There ...\n",
      "> Adding chunk: Tradeoffs in LLM Application Development\n",
      "There ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Development Hurdles\n",
      "\n",
      "Here are some potential pr...\n",
      "> Adding chunk: Development Hurdles\n",
      "\n",
      "Here are some potential pr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retrieval\n",
      "\n",
      "1. **Out of Domain:**\n",
      "If your data i...\n",
      "> Adding chunk: Retrieval\n",
      "\n",
      "1. **Out of Domain:**\n",
      "If your data i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: End-to-End Evaluation\n",
      "End-to-End evaluation sho...\n",
      "> Adding chunk: End-to-End Evaluation\n",
      "End-to-End evaluation sho...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting up an Evaluation Set\n",
      "\n",
      "It is helpful to ...\n",
      "> Adding chunk: Setting up an Evaluation Set\n",
      "\n",
      "It is helpful to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: The Spectrum of Evaluation Options\n",
      "\n",
      "Quantitativ...\n",
      "> Adding chunk: The Spectrum of Evaluation Options\n",
      "\n",
      "Quantitativ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Discovery - Sensitivity Testing\n",
      "\n",
      "With a complex...\n",
      "> Adding chunk: Discovery - Sensitivity Testing\n",
      "\n",
      "With a complex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Metrics Ensembling\n",
      "\n",
      "It may be expensive to use ...\n",
      "> Adding chunk: Metrics Ensembling\n",
      "\n",
      "It may be expensive to use ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Evaluation...\n",
      "> Adding chunk: Evaluation...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setting the Stage\n",
      "\n",
      "LlamaIndex is meant to conne...\n",
      "> Adding chunk: Setting the Stage\n",
      "\n",
      "LlamaIndex is meant to conne...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: General Strategy\n",
      "\n",
      "When developing your LLM appl...\n",
      "> Adding chunk: General Strategy\n",
      "\n",
      "When developing your LLM appl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: E2E or Component-Wise - Which Do I Start With?\n",
      "...\n",
      "> Adding chunk: E2E or Component-Wise - Which Do I Start With?\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Diving Deeper into Evaluation\n",
      "Evaluation is a c...\n",
      "> Adding chunk: Diving Deeper into Evaluation\n",
      "Evaluation is a c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Standard Metrics\n",
      "\n",
      "Against annotated datasets, w...\n",
      "> Adding chunk: Standard Metrics\n",
      "\n",
      "Against annotated datasets, w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Case Studies and Resources\n",
      "1. (Course) Data-Cen...\n",
      "> Adding chunk: Case Studies and Resources\n",
      "1. (Course) Data-Cen...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Monitoring\n",
      "When developing your LLM application...\n",
      "> Adding chunk: Monitoring\n",
      "When developing your LLM application...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Observability\n",
      "\n",
      "In a complex LLM application wit...\n",
      "> Adding chunk: Observability\n",
      "\n",
      "In a complex LLM application wit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: How to Set Up Observability\n",
      "\n",
      "You may refer to o...\n",
      "> Adding chunk: How to Set Up Observability\n",
      "\n",
      "You may refer to o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Discover LlamaIndex Video Series\n",
      "\n",
      "This page con...\n",
      "> Adding chunk: Discover LlamaIndex Video Series\n",
      "\n",
      "This page con...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: SubQuestionQueryEngine + 10K Analysis\n",
      "\n",
      "This vid...\n",
      "> Adding chunk: SubQuestionQueryEngine + 10K Analysis\n",
      "\n",
      "This vid...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Discord Document Management\n",
      "\n",
      "This video covers ...\n",
      "> Adding chunk: Discord Document Management\n",
      "\n",
      "This video covers ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Joint Text to SQL and Semantic Search\n",
      "\n",
      "This vid...\n",
      "> Adding chunk: Joint Text to SQL and Semantic Search\n",
      "\n",
      "This vid...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finetuning...\n",
      "> Adding chunk: Finetuning...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Overview\n",
      "\n",
      "Finetuning a model means updating the...\n",
      "> Adding chunk: Overview\n",
      "\n",
      "Finetuning a model means updating the...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Embedding Finetuning Benefits\n",
      "- Finetuning the ...\n",
      "> Adding chunk: Embedding Finetuning Benefits\n",
      "- Finetuning the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Finetuning Benefits\n",
      "- Allow it to learn a s...\n",
      "> Adding chunk: LLM Finetuning Benefits\n",
      "- Allow it to learn a s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Integrations with LlamaIndex\n",
      "\n",
      "This is an evolvi...\n",
      "> Adding chunk: Integrations with LlamaIndex\n",
      "\n",
      "This is an evolvi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finetuning Embeddings for Better Retrieval Perf...\n",
      "> Adding chunk: Finetuning Embeddings for Better Retrieval Perf...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finetuning GPT-3.5 to distill GPT-4\n",
      "\n",
      "We release...\n",
      "> Adding chunk: Finetuning GPT-3.5 to distill GPT-4\n",
      "\n",
      "We release...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Finetuning Llama 2 for Better Text-to-SQL\n",
      "\n",
      "In t...\n",
      "> Adding chunk: Finetuning Llama 2 for Better Text-to-SQL\n",
      "\n",
      "In t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Knowledge Graphs\n",
      "\n",
      "LlamaIndex contains some fant...\n",
      "> Adding chunk: Knowledge Graphs\n",
      "\n",
      "LlamaIndex contains some fant...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: One-Click Observability\n",
      "\n",
      "LlamaIndex provides **...\n",
      "> Adding chunk: One-Click Observability\n",
      "\n",
      "LlamaIndex provides **...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "To toggle, you will generally ju...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "To toggle, you will generally ju...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: general usage\n",
      "set_global_handler(\"\", **kwargs)...\n",
      "> Adding chunk: general usage\n",
      "set_global_handler(\"\", **kwargs)...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: W&B example\n",
      "\n",
      "```\n",
      "\n",
      "Note that all `kwargs` to `se...\n",
      "> Adding chunk: W&B example\n",
      "\n",
      "```\n",
      "\n",
      "Note that all `kwargs` to `se...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Partners\n",
      "\n",
      "We offer a rich set of integrations w...\n",
      "> Adding chunk: Partners\n",
      "\n",
      "We offer a rich set of integrations w...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Weights and Biases Prompts\n",
      "\n",
      "Prompts allows user...\n",
      "> Adding chunk: Weights and Biases Prompts\n",
      "\n",
      "Prompts allows user...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "from llama_index impor...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "from llama_index impor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: No need to do the following...\n",
      "> Adding chunk: NOTE: No need to do the following...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: access additional methods on handler to persist...\n",
      "> Adding chunk: access additional methods on handler to persist...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: persist index\n",
      "llama_index.global_handler.persis...\n",
      "> Adding chunk: persist index\n",
      "llama_index.global_handler.persis...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load storage context\n",
      "storage_context = llama_in...\n",
      "> Adding chunk: load storage context\n",
      "storage_context = llama_in...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guides\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "> Adding chunk: Guides\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OpenInference\n",
      "\n",
      "OpenInference is an open standar...\n",
      "> Adding chunk: OpenInference\n",
      "\n",
      "OpenInference is an open standar...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "import llama_index\n",
      "\n",
      "ll...\n",
      "> Adding chunk: Usage Pattern\n",
      "\n",
      "```python\n",
      "import llama_index\n",
      "\n",
      "ll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: NOTE: No need to do the following...\n",
      "> Adding chunk: NOTE: No need to do the following...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Run your LlamaIndex application here...\n",
      "for que...\n",
      "> Adding chunk: Run your LlamaIndex application here...\n",
      "for que...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: View your LLM app data as a dataframe in OpenIn...\n",
      "> Adding chunk: View your LLM app data as a dataframe in OpenIn...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guides\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "> Adding chunk: Guides\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/exampl...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: TruEra TruLens\n",
      "\n",
      "TruLens allows users to instrum...\n",
      "> Adding chunk: TruEra TruLens\n",
      "\n",
      "TruLens allows users to instrum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Usage Pattern + Guides\n",
      "\n",
      "```python...\n",
      "> Adding chunk: Usage Pattern + Guides\n",
      "\n",
      "```python...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: use trulens\n",
      "from trulens_eval import TruLlama\n",
      "t...\n",
      "> Adding chunk: use trulens\n",
      "from trulens_eval import TruLlama\n",
      "t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query\n",
      "tru_query_engine.query(\"What did the auth...\n",
      "> Adding chunk: query\n",
      "tru_query_engine.query(\"What did the auth...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/commu...\n",
      "> Adding chunk: Guides\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/commu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Principled Development Practices\n",
      "\n",
      "In order to d...\n",
      "> Adding chunk: Principled Development Practices\n",
      "\n",
      "In order to d...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Contribute Your Insights!\n",
      "If you have thoughts ...\n",
      "> Adding chunk: Contribute Your Insights!\n",
      "If you have thoughts ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Private Setup\n",
      "\n",
      "Relevant Resources:\n",
      "- Using Llam...\n",
      "> Adding chunk: Private Setup\n",
      "\n",
      "Relevant Resources:\n",
      "- Using Llam...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A Guide to Extracting Terms and Definitions\n",
      "\n",
      "Ll...\n",
      "> Adding chunk: A Guide to Extracting Terms and Definitions\n",
      "\n",
      "Ll...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Uploading Text\n",
      "\n",
      "Step one is giving users a way ...\n",
      "> Adding chunk: Uploading Text\n",
      "\n",
      "Step one is giving users a way ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: LLM Settings\n",
      "\n",
      "This next step introduces some ta...\n",
      "> Adding chunk: LLM Settings\n",
      "\n",
      "This next step introduces some ta...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Extracting and Storing Terms\n",
      "\n",
      "Now that we are a...\n",
      "> Adding chunk: Extracting and Storing Terms\n",
      "\n",
      "Now that we are a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A list index will read every single piece of te...\n",
      "> Adding chunk: A list index will read every single piece of te...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Saving Extracted Terms\n",
      "\n",
      "Now that we can extract...\n",
      "> Adding chunk: Saving Extracted Terms\n",
      "\n",
      "Now that we can extract...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Then, after extracting terms from the input tex...\n",
      "> Adding chunk: Then, after extracting terms from the input tex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying for Extracted Terms/Definitions\n",
      "\n",
      "With ...\n",
      "> Adding chunk: Querying for Extracted Terms/Definitions\n",
      "\n",
      "With ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Dry Run Test\n",
      "\n",
      "Well, actually I hope you've been...\n",
      "> Adding chunk: Dry Run Test\n",
      "\n",
      "Well, actually I hope you've been...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Improvement 1 - Create a Starting Index\n",
      "\n",
      "With o...\n",
      "> Adding chunk: Improvement 1 - Create a Starting Index\n",
      "\n",
      "With o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Improvement 2 - (Refining) Better Prompts\n",
      "\n",
      "If y...\n",
      "> Adding chunk: Improvement 2 - (Refining) Better Prompts\n",
      "\n",
      "If y...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Text QA templates\n",
      "DEFAULT_TEXT_QA_PROMPT_TMPL =...\n",
      "> Adding chunk: Text QA templates\n",
      "DEFAULT_TEXT_QA_PROMPT_TMPL =...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Refine templates\n",
      "DEFAULT_REFINE_PROMPT_TMPL = (...\n",
      "> Adding chunk: Refine templates\n",
      "DEFAULT_REFINE_PROMPT_TMPL = (...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: refine prompt selector\n",
      "REFINE_TEMPLATE = Select...\n",
      "> Adding chunk: refine prompt selector\n",
      "REFINE_TEMPLATE = Select...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Improvement 3 - Image Support\n",
      "\n",
      "Llama index also...\n",
      "> Adding chunk: Improvement 3 - Image Support\n",
      "\n",
      "Llama index also...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: )\n",
      "                    )\n",
      "                if uplo...\n",
      "> Adding chunk: )\n",
      "                    )\n",
      "                if uplo...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Conclusion/TLDR\n",
      "\n",
      "In this tutorial, we covered a...\n",
      "> Adding chunk: Conclusion/TLDR\n",
      "\n",
      "In this tutorial, we covered a...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A Guide to Creating a Unified Query Framework o...\n",
      "> Adding chunk: A Guide to Creating a Unified Query Framework o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setup\n",
      "\n",
      "In this example, we will analyze Wikiped...\n",
      "> Adding chunk: Setup\n",
      "\n",
      "In this example, we will analyze Wikiped...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Load all wiki documents\n",
      "city_docs = {}\n",
      "for wiki...\n",
      "> Adding chunk: Load all wiki documents\n",
      "city_docs = {}\n",
      "for wiki...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining the Set of Indexes\n",
      "\n",
      "We will now define...\n",
      "> Adding chunk: Defining the Set of Indexes\n",
      "\n",
      "We will now define...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set service context\n",
      "llm_gpt4 = OpenAI(temperatu...\n",
      "> Adding chunk: set service context\n",
      "llm_gpt4 = OpenAI(temperatu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Build city document index\n",
      "vector_indices = {}\n",
      "f...\n",
      "> Adding chunk: Build city document index\n",
      "vector_indices = {}\n",
      "f...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining a Graph for Compare/Contrast Queries\n",
      "\n",
      "...\n",
      "> Adding chunk: Defining a Graph for Compare/Contrast Queries\n",
      "\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: get root index\n",
      "root_index = graph.get_index(gra...\n",
      "> Adding chunk: get root index\n",
      "root_index = graph.get_index(gra...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set id of root index\n",
      "root_index.set_index_id(\"c...\n",
      "> Adding chunk: set id of root index\n",
      "root_index.set_index_id(\"c...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define decompose_transform\n",
      "from llama_index imp...\n",
      "> Adding chunk: define decompose_transform\n",
      "from llama_index imp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define custom query engines\n",
      "from llama_index.qu...\n",
      "> Adding chunk: define custom query engines\n",
      "from llama_index.qu...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query engine\n",
      "query_engine = graph.as_que...\n",
      "> Adding chunk: define query engine\n",
      "query_engine = graph.as_que...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query the graph\n",
      "query_str = (\n",
      "    \"Compare and ...\n",
      "> Adding chunk: query the graph\n",
      "query_str = (\n",
      "    \"Compare and ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Defining the Unified Query Interface\n",
      "\n",
      "Now that ...\n",
      "> Adding chunk: Defining the Unified Query Interface\n",
      "\n",
      "Now that ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: add vector index tools\n",
      "for wiki_title in wiki_t...\n",
      "> Adding chunk: add vector index tools\n",
      "for wiki_title in wiki_t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: add graph tool\n",
      "graph_description = (\n",
      "    \"This ...\n",
      "> Adding chunk: add graph tool\n",
      "graph_description = (\n",
      "    \"This ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying our Unified Interface\n",
      "\n",
      "The advantage o...\n",
      "> Adding chunk: Querying our Unified Interface\n",
      "\n",
      "The advantage o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: ask a compare/contrast question\n",
      "response = rout...\n",
      "> Adding chunk: ask a compare/contrast question\n",
      "response = rout...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Q&A over Documents\n",
      "\n",
      "At a high-level, LlamaIndex...\n",
      "> Adding chunk: Q&A over Documents\n",
      "\n",
      "At a high-level, LlamaIndex...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Semantic Search\n",
      "\n",
      "The most basic example usage o...\n",
      "> Adding chunk: Semantic Search\n",
      "\n",
      "The most basic example usage o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Summarization\n",
      "\n",
      "A summarization query requires t...\n",
      "> Adding chunk: Summarization\n",
      "\n",
      "A summarization query requires t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Queries over Structured Data\n",
      "\n",
      "LlamaIndex suppor...\n",
      "> Adding chunk: Queries over Structured Data\n",
      "\n",
      "LlamaIndex suppor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Synthesis over Heterogeneous Data\n",
      "\n",
      "LlamaIndex s...\n",
      "> Adding chunk: Synthesis over Heterogeneous Data\n",
      "\n",
      "LlamaIndex s...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Routing over Heterogeneous Data\n",
      "\n",
      "LlamaIndex als...\n",
      "> Adding chunk: Routing over Heterogeneous Data\n",
      "\n",
      "LlamaIndex als...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define sub-indices\n",
      "index1 = VectorStoreIndex.fr...\n",
      "> Adding chunk: define sub-indices\n",
      "index1 = VectorStoreIndex.fr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define query engines and tools\n",
      "tool1 = QueryEng...\n",
      "> Adding chunk: define query engines and tools\n",
      "tool1 = QueryEng...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Compare/Contrast Queries\n",
      "You can explicitly per...\n",
      "> Adding chunk: Compare/Contrast Queries\n",
      "You can explicitly per...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Multi-Document Queries\n",
      "\n",
      "Besides the explicit sy...\n",
      "> Adding chunk: Multi-Document Queries\n",
      "\n",
      "Besides the explicit sy...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Multi-Step Queries\n",
      "\n",
      "LlamaIndex can also support...\n",
      "> Adding chunk: Multi-Step Queries\n",
      "\n",
      "LlamaIndex can also support...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Temporal Queries\n",
      "\n",
      "LlamaIndex can support querie...\n",
      "> Adding chunk: Temporal Queries\n",
      "\n",
      "LlamaIndex can support querie...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Additional Resources\n",
      "- A Guide to Creating a Un...\n",
      "> Adding chunk: Additional Resources\n",
      "- A Guide to Creating a Un...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: A Guide to LlamaIndex + Structured Data\n",
      "\n",
      "A lot ...\n",
      "> Adding chunk: A Guide to LlamaIndex + Structured Data\n",
      "\n",
      "A lot ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Setup\n",
      "\n",
      "First, we use SQLAlchemy to setup a simp...\n",
      "> Adding chunk: Setup\n",
      "\n",
      "First, we use SQLAlchemy to setup a simp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: create city SQL table\n",
      "table_name = \"city_stats\"...\n",
      "> Adding chunk: create city SQL table\n",
      "table_name = \"city_stats\"...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Natural language SQL\n",
      "Once we have constructed o...\n",
      "> Adding chunk: Natural language SQL\n",
      "Once we have constructed o...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Building our Table Index\n",
      "If we don't know ahead...\n",
      "> Adding chunk: Building our Table Index\n",
      "If we don't know ahead...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: manually set extra context text\n",
      "city_stats_text...\n",
      "> Adding chunk: manually set extra context text\n",
      "city_stats_text...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Using natural language SQL queries\n",
      "Once we have...\n",
      "> Adding chunk: Using natural language SQL queries\n",
      "Once we have...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Concluding Thoughts\n",
      "\n",
      "This is it for now! We're ...\n",
      "> Adding chunk: Concluding Thoughts\n",
      "\n",
      "This is it for now! We're ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Structured Data\n",
      "\n",
      "Relevant Resources:\n",
      "- A Guide ...\n",
      "> Adding chunk: Structured Data\n",
      "\n",
      "Relevant Resources:\n",
      "- A Guide ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Basic Usage Pattern\n",
      "\n",
      "The general usage pattern ...\n",
      "> Adding chunk: Basic Usage Pattern\n",
      "\n",
      "The general usage pattern ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 1. Load in Documents\n",
      "\n",
      "The first step is to load...\n",
      "> Adding chunk: 1. Load in Documents\n",
      "\n",
      "The first step is to load...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 2. Parse the Documents into Nodes\n",
      "\n",
      "The next ste...\n",
      "> Adding chunk: 2. Parse the Documents into Nodes\n",
      "\n",
      "The next ste...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: set relationships\n",
      "node1.relationships[NodeRelat...\n",
      "> Adding chunk: set relationships\n",
      "node1.relationships[NodeRelat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 3. Index Construction\n",
      "\n",
      "We can now build an inde...\n",
      "> Adding chunk: 3. Index Construction\n",
      "\n",
      "We can now build an inde...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Reusing Nodes across Index Structures\n",
      "\n",
      "If you h...\n",
      "> Adding chunk: Reusing Nodes across Index Structures\n",
      "\n",
      "If you h...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Inserting Documents or Nodes\n",
      "\n",
      "You can also take...\n",
      "> Adding chunk: Inserting Documents or Nodes\n",
      "\n",
      "You can also take...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: nodes: Sequence[Node]\n",
      "index = VectorStoreIndex(...\n",
      "> Adding chunk: nodes: Sequence[Node]\n",
      "index = VectorStoreIndex(...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Documents\n",
      "\n",
      "When creating documents,...\n",
      "> Adding chunk: Customizing Documents\n",
      "\n",
      "When creating documents,...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing LLM's\n",
      "\n",
      "By default, we use OpenAI's ...\n",
      "> Adding chunk: Customizing LLM's\n",
      "\n",
      "By default, we use OpenAI's ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: define LLM\n",
      "llm = OpenAI(model=\"gpt-4\", temperat...\n",
      "> Adding chunk: define LLM\n",
      "llm = OpenAI(model=\"gpt-4\", temperat...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure service context\n",
      "service_context = Ser...\n",
      "> Adding chunk: configure service context\n",
      "service_context = Ser...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Global ServiceContext\n",
      "\n",
      "If you wanted the servic...\n",
      "> Adding chunk: Global ServiceContext\n",
      "\n",
      "If you wanted the servic...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing Prompts\n",
      "\n",
      "Depending on the index use...\n",
      "> Adding chunk: Customizing Prompts\n",
      "\n",
      "Depending on the index use...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customizing embeddings\n",
      "\n",
      "For embedding-based ind...\n",
      "> Adding chunk: Customizing embeddings\n",
      "\n",
      "For embedding-based ind...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Cost Analysis\n",
      "\n",
      "Creating an index, inserting to ...\n",
      "> Adding chunk: Cost Analysis\n",
      "\n",
      "Creating an index, inserting to ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: [Optional] Save the index for future use\n",
      "\n",
      "By de...\n",
      "> Adding chunk: [Optional] Save the index for future use\n",
      "\n",
      "By de...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: rebuild storage context\n",
      "storage_context = Stora...\n",
      "> Adding chunk: rebuild storage context\n",
      "storage_context = Stora...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load index\n",
      "index = load_index_from_storage(stor...\n",
      "> Adding chunk: load index\n",
      "index = load_index_from_storage(stor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: when first building the index\n",
      "index = VectorSto...\n",
      "> Adding chunk: when first building the index\n",
      "index = VectorSto...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: when loading the index from disk\n",
      "index = load_i...\n",
      "> Adding chunk: when loading the index from disk\n",
      "index = load_i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 4. [Optional, Advanced] Building indices on top...\n",
      "> Adding chunk: 4. [Optional, Advanced] Building indices on top...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 5. Query the index.\n",
      "\n",
      "After building the index, ...\n",
      "> Adding chunk: 5. Query the index.\n",
      "\n",
      "After building the index, ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High-level API\n",
      "\n",
      "To start, you can query an inde...\n",
      "> Adding chunk: High-level API\n",
      "\n",
      "To start, you can query an inde...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Low-level API\n",
      "\n",
      "We also support a low-level comp...\n",
      "> Adding chunk: Low-level API\n",
      "\n",
      "We also support a low-level comp...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "> Adding chunk: build index\n",
      "index = VectorStoreIndex.from_docum...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure retriever\n",
      "retriever = VectorIndexRetr...\n",
      "> Adding chunk: configure retriever\n",
      "retriever = VectorIndexRetr...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: configure response synthesizer\n",
      "response_synthes...\n",
      "> Adding chunk: configure response synthesizer\n",
      "response_synthes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: assemble query engine\n",
      "query_engine = RetrieverQ...\n",
      "> Adding chunk: assemble query engine\n",
      "query_engine = RetrieverQ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: query\n",
      "response = query_engine.query(\"What did t...\n",
      "> Adding chunk: query\n",
      "response = query_engine.query(\"What did t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring retriever\n",
      "\n",
      "An index can have a vari...\n",
      "> Adding chunk: Configuring retriever\n",
      "\n",
      "An index can have a vari...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring response synthesis\n",
      "After a retrieve...\n",
      "> Adding chunk: Configuring response synthesis\n",
      "After a retrieve...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: default\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "> Adding chunk: default\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: compact\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "> Adding chunk: compact\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: tree summarize\n",
      "query_engine = RetrieverQueryEng...\n",
      "> Adding chunk: tree summarize\n",
      "query_engine = RetrieverQueryEng...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: no text\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "> Adding chunk: no text\n",
      "query_engine = RetrieverQueryEngine.fro...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Configuring node postprocessors (i.e. filtering...\n",
      "> Adding chunk: Configuring node postprocessors (i.e. filtering...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 5. Parsing the response\n",
      "\n",
      "The object returned is...\n",
      "> Adding chunk: 5. Parsing the response\n",
      "\n",
      "The object returned is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: get response\n",
      "str(response)...\n",
      "> Adding chunk: get response\n",
      "str(response)...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: get sources\n",
      "response.source_nodes...\n",
      "> Adding chunk: get sources\n",
      "response.source_nodes...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: formatted sources\n",
      "response.get_formatted_source...\n",
      "> Adding chunk: formatted sources\n",
      "response.get_formatted_source...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Use Cases\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/en...\n",
      "> Adding chunk: Use Cases\n",
      "\n",
      "```{toctree}\n",
      "---\n",
      "maxdepth: 1\n",
      "---\n",
      "/en...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Data Connector Examples\n",
      "\n",
      "Each of these notebook...\n",
      "> Adding chunk: Data Connector Examples\n",
      "\n",
      "Each of these notebook...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: FAQ...\n",
      "> Adding chunk: FAQ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: High-Level Concepts\n",
      "\n",
      "```{tip}\n",
      "If you haven't, i...\n",
      "> Adding chunk: High-Level Concepts\n",
      "\n",
      "```{tip}\n",
      "If you haven't, i...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Retrieval Augmented Generation (RAG)\n",
      "Retrieval ...\n",
      "> Adding chunk: Retrieval Augmented Generation (RAG)\n",
      "Retrieval ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Indexing Stage\n",
      "LlamaIndex help you prepare the ...\n",
      "> Adding chunk: Indexing Stage\n",
      "LlamaIndex help you prepare the ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Querying Stage\n",
      "In the querying stage, the RAG p...\n",
      "> Adding chunk: Querying Stage\n",
      "In the querying stage, the RAG p...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Building Blocks\n",
      "**Retrievers**: \n",
      "A retriever de...\n",
      "> Adding chunk: Building Blocks\n",
      "**Retrievers**: \n",
      "A retriever de...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Pipelines\n",
      "\n",
      "**Query Engines**:\n",
      "A query engine is...\n",
      "> Adding chunk: Pipelines\n",
      "\n",
      "**Query Engines**:\n",
      "A query engine is...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Customization Tutorial\n",
      "======================\n",
      "....\n",
      "> Adding chunk: Customization Tutorial\n",
      "======================\n",
      "....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: -----------------\n",
      "\n",
      "**\"I want to use a different...\n",
      "> Adding chunk: -----------------\n",
      "\n",
      "**\"I want to use a different...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Installation and Setup...\n",
      "> Adding chunk: Installation and Setup...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Installation from Pip\n",
      "\n",
      "You can simply do:\n",
      "\n",
      "```\n",
      "...\n",
      "> Adding chunk: Installation from Pip\n",
      "\n",
      "You can simply do:\n",
      "\n",
      "```\n",
      "...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Installation from Source\n",
      "\n",
      "Git clone this reposi...\n",
      "> Adding chunk: Installation from Source\n",
      "\n",
      "Git clone this reposi...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: OpenAI Environment Setup\n",
      "\n",
      "By default, we use th...\n",
      "> Adding chunk: OpenAI Environment Setup\n",
      "\n",
      "By default, we use th...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Local Environment Setup\n",
      "\n",
      "If you don't wish to u...\n",
      "> Adding chunk: Local Environment Setup\n",
      "\n",
      "If you don't wish to u...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Starter Tutorial\n",
      "\n",
      "```{tip}\n",
      "Make sure you've fol...\n",
      "> Adding chunk: Starter Tutorial\n",
      "\n",
      "```{tip}\n",
      "Make sure you've fol...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Download\n",
      "\n",
      "LlamaIndex examples can be found in t...\n",
      "> Adding chunk: Download\n",
      "\n",
      "LlamaIndex examples can be found in t...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Build and Query Index\n",
      "\n",
      "Create a new `.py` file ...\n",
      "> Adding chunk: Build and Query Index\n",
      "\n",
      "Create a new `.py` file ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Viewing Queries and Events Using Logging\n",
      "\n",
      "In a ...\n",
      "> Adding chunk: Viewing Queries and Events Using Logging\n",
      "\n",
      "In a ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Saving and Loading\n",
      "\n",
      "By default, data is stored ...\n",
      "> Adding chunk: Saving and Loading\n",
      "\n",
      "By default, data is stored ...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: rebuild storage context\n",
      "storage_context = Stora...\n",
      "> Adding chunk: rebuild storage context\n",
      "storage_context = Stora...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: load index\n",
      "index = load_index_from_storage(stor...\n",
      "> Adding chunk: load index\n",
      "index = load_index_from_storage(stor...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: .. LlamaIndex documentation master file, create...\n",
      "> Adding chunk: .. LlamaIndex documentation master file, create...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: collection of custom data connectors\n",
      "- 🧪 LlamaL...\n",
      "> Adding chunk: collection of custom data connectors\n",
      "- 🧪 LlamaL...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Documentation Guide\", \"A guide for docs contributors  The `docs` directory contains the sphinx source text for LlamaIndex docs, visit https://gpt-index.readthedocs.io/ to read the full documentation.  This guide is made for anyone who\\'s interested in running LlamaIndex documentation locally, making changes to it and make contributions. LlamaIndex is made by the thriving community behind it, and you\\'re always welcome to make contributions to the project and the  documentation.\", \"Build Docs  If you haven\\'t already, clone the LlamaIndex Github repo to a local directory:  ```bash git clone https://github.com/jerryjliu/llama_index.git && cd llama_index ```  Install all dependencies required for building docs (mainly `sphinx` and its extension):  ```bash pip install -r docs/requirements.txt ```  Build the sphinx docs:  ```bash cd docs make html ```  The docs HTML files are now generated under `docs/_build/html` directory, you can preview it locally with the following command:  ```bash python -m http.server 8000 -d _build/html ```  And open your browser at http://0.0.0.0:8000/ to view the generated docs.\", \"Watch Docs  We recommend using sphinx-autobuild during development, which provides a live-reloading  server, that rebuilds the documentation and refreshes any open pages automatically when  changes are saved. This enables a much shorter feedback loop which can help boost  productivity when writing documentation.  Simply run the following command from LlamaIndex project\\'s root directory:  ```bash make watch-docs ```\", \".. _Ref-Node:  Callbacks =================  .. automodule:: llama_index.callbacks    :members:    :inherited-members:\", \".. _Ref-Composability:  Composability =============  Below we show the API reference for composable data structures. This contains both the `ComposableGraph` class as well as any builder classes that generate `ComposableGraph` objects.  .. automodule:: llama_index.composability    :members:    :inherited-members:\", \".. _Ref-Example-Notebooks:  Example Notebooks =================  We offer a wide variety of example notebooks. They are referenced throughout the documentation.  Example notebooks are found `here <https://github.com/jerryjliu/llama_index/tree/main/docs/examples>`_.\", \".. _Ref-Finetuning:  Finetuning =============  .. automodule:: llama_index.finetuning    :members:    :inherited-members:\", \".. _Ref-API_Reference:  API Reference =============  API Reference for the ``llama-index`` package.  .. toctree::    :maxdepth: 1     indices.rst    query.rst    node.rst    llm_predictor.rst    llms.rst    memory.rst    node_postprocessor.rst    storage.rst    composability.rst    readers.rst    prompts.rst    service_context.rst    callbacks.rst    struct_store.rst    response.rst    playground.rst    finetuning.rst    example_notebooks.rst    langchain_integrations/base.rst\", \".. _Ref-Indices-Empty:  Empty Index ===========  Building the Empty Index  .. automodule:: llama_index.indices.empty    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Documentation Guide\", \"A guide for docs contributors  The `docs` directory contains the sphinx source text for LlamaIndex docs, visit https://gpt-index.readthedocs.io/ to read the full documentation.  This guide is made for anyone who\\'s interested in running LlamaIndex documentation locally, making changes to it and make contributions. LlamaIndex is made by the thriving community behind it, and you\\'re always welcome to make contributions to the project and the  documentation.\", \"Build Docs  If you haven\\'t already, clone the LlamaIndex Github repo to a local directory:  ```bash git clone https://github.com/jerryjliu/llama_index.git && cd llama_index ```  Install all dependencies required for building docs (mainly `sphinx` and its extension):  ```bash pip install -r docs/requirements.txt ```  Build the sphinx docs:  ```bash cd docs make html ```  The docs HTML files are now generated under `docs/_build/html` directory, you can preview it locally with the following command:  ```bash python -m http.server 8000 -d _build/html ```  And open your browser at http://0.0.0.0:8000/ to view the generated docs.\", \"Watch Docs  We recommend using sphinx-autobuild during development, which provides a live-reloading  server, that rebuilds the documentation and refreshes any open pages automatically when  changes are saved. This enables a much shorter feedback loop which can help boost  productivity when writing documentation.  Simply run the following command from LlamaIndex project\\'s root directory:  ```bash make watch-docs ```\", \".. _Ref-Node:  Callbacks =================  .. automodule:: llama_index.callbacks    :members:    :inherited-members:\", \".. _Ref-Composability:  Composability =============  Below we show the API reference for composable data structures. This contains both the `ComposableGraph` class as well as any builder classes that generate `ComposableGraph` objects.  .. automodule:: llama_index.composability    :members:    :inherited-members:\", \".. _Ref-Example-Notebooks:  Example Notebooks =================  We offer a wide variety of example notebooks. They are referenced throughout the documentation.  Example notebooks are found `here <https://github.com/jerryjliu/llama_index/tree/main/docs/examples>`_.\", \".. _Ref-Finetuning:  Finetuning =============  .. automodule:: llama_index.finetuning    :members:    :inherited-members:\", \".. _Ref-API_Reference:  API Reference =============  API Reference for the ``llama-index`` package.  .. toctree::    :maxdepth: 1     indices.rst    query.rst    node.rst    llm_predictor.rst    llms.rst    memory.rst    node_postprocessor.rst    storage.rst    composability.rst    readers.rst    prompts.rst    service_context.rst    callbacks.rst    struct_store.rst    response.rst    playground.rst    finetuning.rst    example_notebooks.rst    langchain_integrations/base.rst\", \".. _Ref-Indices-Empty:  Empty Index ===========  Building the Empty Index  .. automodule:: llama_index.indices.empty    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=180 request_id=3bad8ecde8e4f85fbab9be50cae3316b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=180 request_id=3bad8ecde8e4f85fbab9be50cae3316b response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\".. _Ref-Indices-Knowledge-Graph:  Knowledge Graph Index =====================  Building the Knowledge Graph Index  .. automodule:: llama_index.indices.knowledge_graph    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-List:  List Index ==========  Building the List Index  .. automodule:: llama_index.indices.list    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-StructStore:  Structured Store Index ======================  .. automodule:: llama_index.indices.struct_store    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-Table:  Table Index ===========  Building the Keyword Table Index  .. automodule:: llama_index.indices.keyword_table    :members:    :inherited-members:\", \".. _Ref-Indices-Tree:  Tree Index ==========  Building the Tree Index  .. automodule:: llama_index.indices.tree    :members:    :inherited-members:\", \".. _Ref-Indices-VectorStore:  Vector Store Index ==================  Below we show the vector store index classes.  Each vector store index class is a combination of a base vector store index class and a vector store, shown below.  .. automodule:: llama_index.indices.vector_store.base    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices:  Indices =======  This doc shows both the overarching class used to represent an index. These classes allow for index creation, insertion, and also querying. We first show the different index subclasses. We then show the base class that all indices inherit from, which contains  parameters and methods common to all indices.  |  .. toctree::    :maxdepth: 1    :caption: Index Data Structures     indices/list.rst    indices/table.rst    indices/tree.rst    indices/vector_store.rst    indices/struct_store.rst    indices/kg.rst    indices/empty.rst   Base Index Class ^^^^^^^^^^^^^^^^  .. automodule:: llama_index.indices.base    :members:    :inherited-members:\", \".. _Ref-Langchain-Integrations:  Langchain Integrations ======================  Agent Tools + Functions  .. automodule:: llama_index.langchain_helpers.agents    :members:    :inherited-members:  Memory Module  .. automodule:: llama_index.langchain_helpers.memory_wrapper    :members:    :inherited-members:\", \".. _Ref-Node:  LLM Predictors =================  .. automodule:: llama_index.llm_predictor    :members:    :inherited-members:\", \"Anthropic =========  .. autopydantic_model:: llama_index.llms.anthropic.Anthropic\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\".. _Ref-Indices-Knowledge-Graph:  Knowledge Graph Index =====================  Building the Knowledge Graph Index  .. automodule:: llama_index.indices.knowledge_graph    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-List:  List Index ==========  Building the List Index  .. automodule:: llama_index.indices.list    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-StructStore:  Structured Store Index ======================  .. automodule:: llama_index.indices.struct_store    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices-Table:  Table Index ===========  Building the Keyword Table Index  .. automodule:: llama_index.indices.keyword_table    :members:    :inherited-members:\", \".. _Ref-Indices-Tree:  Tree Index ==========  Building the Tree Index  .. automodule:: llama_index.indices.tree    :members:    :inherited-members:\", \".. _Ref-Indices-VectorStore:  Vector Store Index ==================  Below we show the vector store index classes.  Each vector store index class is a combination of a base vector store index class and a vector store, shown below.  .. automodule:: llama_index.indices.vector_store.base    :members:    :inherited-members:    :exclude-members: delete, docstore, index_struct, index_struct_cls\", \".. _Ref-Indices:  Indices =======  This doc shows both the overarching class used to represent an index. These classes allow for index creation, insertion, and also querying. We first show the different index subclasses. We then show the base class that all indices inherit from, which contains  parameters and methods common to all indices.  |  .. toctree::    :maxdepth: 1    :caption: Index Data Structures     indices/list.rst    indices/table.rst    indices/tree.rst    indices/vector_store.rst    indices/struct_store.rst    indices/kg.rst    indices/empty.rst   Base Index Class ^^^^^^^^^^^^^^^^  .. automodule:: llama_index.indices.base    :members:    :inherited-members:\", \".. _Ref-Langchain-Integrations:  Langchain Integrations ======================  Agent Tools + Functions  .. automodule:: llama_index.langchain_helpers.agents    :members:    :inherited-members:  Memory Module  .. automodule:: llama_index.langchain_helpers.memory_wrapper    :members:    :inherited-members:\", \".. _Ref-Node:  LLM Predictors =================  .. automodule:: llama_index.llm_predictor    :members:    :inherited-members:\", \"Anthropic =========  .. autopydantic_model:: llama_index.llms.anthropic.Anthropic\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=108 request_id=81519976ddf91896a1d0e9f29ebdaae7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=108 request_id=81519976ddf91896a1d0e9f29ebdaae7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Azure OpenAI ============  .. autopydantic_model:: llama_index.llms.azure_openai.AzureOpenAI\", \"HuggingFaceLLM ==============  .. autopydantic_model:: llama_index.llms.huggingface.HuggingFaceLLM\", \"LangChainLLM ============  .. autopydantic_model:: llama_index.llms.langchain.LangChainLLM\", \"LlamaCPP ========  .. autopydantic_model:: llama_index.llms.llama_cpp.LlamaCPP\", \"OpenAI ======  .. autopydantic_model:: llama_index.llms.openai.OpenAI\", \"PaLM ====  .. autopydantic_model:: llama_index.llms.palm.PaLM\", \"Predibase =========  .. autopydantic_model:: llama_index.llms.predibase.Predibase\", \"Replicate =========  .. autopydantic_model:: llama_index.llms.replicate.Replicate\", \"XOrbits Xinference ==================  .. autopydantic_model:: llama_index.llms.xinference.Xinference\", \".. _Ref-LLMs:   LLMs ====  A large language model (LLM) is a reasoning engine that can complete text, chat with users, and follow instructions.   LLM Implementations ^^^^^^^^^^^^^^^^^^^ .. toctree::    :maxdepth: 1    :caption: LLM Implementations     llms/openai.rst    llms/azure_openai.rst    llms/huggingface.rst    llms/langchain.rst    llms/anthropic.rst    llms/llama_cpp.rst    llms/palm.rst    llms/predibase.rst    llms/replicate.rst    llms/xinference.rst  LLM Interface ^^^^^^^^^^^^^^^^^^^^^^^ .. autoclass:: llama_index.llms.base.LLM    :members:    :inherited-members:  Schemas ^^^^^^^  .. autoclass:: llama_index.llms.base.MessageRole    :members:    :inherited-members:  .. autopydantic_model:: llama_index.llms.base.ChatMessage  .. autopydantic_model:: llama_index.llms.base.ChatResponse  .. autopydantic_model:: llama_index.llms.base.CompletionResponse  .. autopydantic_model:: llama_index.llms.base.LLMMetadata\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Azure OpenAI ============  .. autopydantic_model:: llama_index.llms.azure_openai.AzureOpenAI\", \"HuggingFaceLLM ==============  .. autopydantic_model:: llama_index.llms.huggingface.HuggingFaceLLM\", \"LangChainLLM ============  .. autopydantic_model:: llama_index.llms.langchain.LangChainLLM\", \"LlamaCPP ========  .. autopydantic_model:: llama_index.llms.llama_cpp.LlamaCPP\", \"OpenAI ======  .. autopydantic_model:: llama_index.llms.openai.OpenAI\", \"PaLM ====  .. autopydantic_model:: llama_index.llms.palm.PaLM\", \"Predibase =========  .. autopydantic_model:: llama_index.llms.predibase.Predibase\", \"Replicate =========  .. autopydantic_model:: llama_index.llms.replicate.Replicate\", \"XOrbits Xinference ==================  .. autopydantic_model:: llama_index.llms.xinference.Xinference\", \".. _Ref-LLMs:   LLMs ====  A large language model (LLM) is a reasoning engine that can complete text, chat with users, and follow instructions.   LLM Implementations ^^^^^^^^^^^^^^^^^^^ .. toctree::    :maxdepth: 1    :caption: LLM Implementations     llms/openai.rst    llms/azure_openai.rst    llms/huggingface.rst    llms/langchain.rst    llms/anthropic.rst    llms/llama_cpp.rst    llms/palm.rst    llms/predibase.rst    llms/replicate.rst    llms/xinference.rst  LLM Interface ^^^^^^^^^^^^^^^^^^^^^^^ .. autoclass:: llama_index.llms.base.LLM    :members:    :inherited-members:  Schemas ^^^^^^^  .. autoclass:: llama_index.llms.base.MessageRole    :members:    :inherited-members:  .. autopydantic_model:: llama_index.llms.base.ChatMessage  .. autopydantic_model:: llama_index.llms.base.ChatResponse  .. autopydantic_model:: llama_index.llms.base.CompletionResponse  .. autopydantic_model:: llama_index.llms.base.LLMMetadata\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=196 request_id=7b1656852a6e69419ccd7658e04449f8 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=196 request_id=7b1656852a6e69419ccd7658e04449f8 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\".. _Ref-Memory  Memory ======  .. automodule:: llama_index.memory    :members:    :inherited-members:\", \".. _Ref-Node:  Node =================  .. automodule:: llama_index.schema    :members:    :inherited-members:    :exclude-members: NodeType, ImageNode, IndexNode, TextNode\", \".. _Ref-Node-Postprocessor:  Node Postprocessor ===================  .. automodule:: llama_index.indices.postprocessor    :members:    :inherited-members:\", \".. _Ref-Playground:  Playground =================  .. automodule:: llama_index.playground.base    :members:    :inherited-members:\", \".. _Prompt-Templates:  Prompt Templates =================  These are the reference prompt templates.   We first show links to default prompts.  We then show the base prompt template class and its subclasses.  Default Prompts ^^^^^^^^^^^^^^^^^   * `Completion prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py>`_. * `Chat prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py>`_. * `Selector prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompt_selectors.py>`_.    Prompt Classes ^^^^^^^^^^^^^^^^^  .. autopydantic_model:: llama_index.prompts.base.BasePromptTemplate  .. autopydantic_model:: llama_index.prompts.base.PromptTemplate  .. autopydantic_model:: llama_index.prompts.base.ChatPromptTemplate  .. autopydantic_model:: llama_index.prompts.base.SelectorPromptTemplate  .. autopydantic_model:: llama_index.prompts.base.LangchainPromptTemplate   Subclass Prompts (deprecated) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Deprecated, but still available for reference at `this link <https://github.com/jerryjliu/llama_index/blob/113109365b216428440b19eb23c9fae749d6880a/llama_index/prompts/prompts.py>`_.\", \"Condense Question Chat Engine =======================  .. automodule:: llama_index.chat_engine.condense_question    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Simple Chat Engine =======================  .. automodule:: llama_index.chat_engine.simple    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \".. _Ref-Chat-Engines:  Chat Engines ================= Chat engine is a high-level interface for having a conversation with your data (multiple back-and-forth instead of a single question & answer).     Chat Engine Implementations ^^^^^^^^^^^^^^^^^^^^^ Below we show specific chat engine implementations.  .. toctree::    :maxdepth: 1    :caption: Chat Engines     chat_engines/simple_chat_engine.rst    chat_engines/condense_question_chat_engine.rst  Chat Engine Types ^^^^^^^^^^^^^^^^^^^^^ .. automodule:: llama_index.chat_engine.types    :members:    :inherited-members:\", \"Query Bundle ============  .. automodule:: llama_index.indices.query.schema    :members: QueryBundle    :inherited-members:    :exclude-members:\", \"Citation Query Engine =======================  .. automodule:: llama_index.query_engine.citation_query_engine    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\".. _Ref-Memory  Memory ======  .. automodule:: llama_index.memory    :members:    :inherited-members:\", \".. _Ref-Node:  Node =================  .. automodule:: llama_index.schema    :members:    :inherited-members:    :exclude-members: NodeType, ImageNode, IndexNode, TextNode\", \".. _Ref-Node-Postprocessor:  Node Postprocessor ===================  .. automodule:: llama_index.indices.postprocessor    :members:    :inherited-members:\", \".. _Ref-Playground:  Playground =================  .. automodule:: llama_index.playground.base    :members:    :inherited-members:\", \".. _Prompt-Templates:  Prompt Templates =================  These are the reference prompt templates.   We first show links to default prompts.  We then show the base prompt template class and its subclasses.  Default Prompts ^^^^^^^^^^^^^^^^^   * `Completion prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py>`_. * `Chat prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py>`_. * `Selector prompt templates <https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompt_selectors.py>`_.    Prompt Classes ^^^^^^^^^^^^^^^^^  .. autopydantic_model:: llama_index.prompts.base.BasePromptTemplate  .. autopydantic_model:: llama_index.prompts.base.PromptTemplate  .. autopydantic_model:: llama_index.prompts.base.ChatPromptTemplate  .. autopydantic_model:: llama_index.prompts.base.SelectorPromptTemplate  .. autopydantic_model:: llama_index.prompts.base.LangchainPromptTemplate   Subclass Prompts (deprecated) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Deprecated, but still available for reference at `this link <https://github.com/jerryjliu/llama_index/blob/113109365b216428440b19eb23c9fae749d6880a/llama_index/prompts/prompts.py>`_.\", \"Condense Question Chat Engine =======================  .. automodule:: llama_index.chat_engine.condense_question    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Simple Chat Engine =======================  .. automodule:: llama_index.chat_engine.simple    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \".. _Ref-Chat-Engines:  Chat Engines ================= Chat engine is a high-level interface for having a conversation with your data (multiple back-and-forth instead of a single question & answer).     Chat Engine Implementations ^^^^^^^^^^^^^^^^^^^^^ Below we show specific chat engine implementations.  .. toctree::    :maxdepth: 1    :caption: Chat Engines     chat_engines/simple_chat_engine.rst    chat_engines/condense_question_chat_engine.rst  Chat Engine Types ^^^^^^^^^^^^^^^^^^^^^ .. automodule:: llama_index.chat_engine.types    :members:    :inherited-members:\", \"Query Bundle ============  .. automodule:: llama_index.indices.query.schema    :members: QueryBundle    :inherited-members:    :exclude-members:\", \"Citation Query Engine =======================  .. automodule:: llama_index.query_engine.citation_query_engine    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=142 request_id=102c55b026dcfa9077ada98f20ff33c7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=142 request_id=102c55b026dcfa9077ada98f20ff33c7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Flare Query Engine =======================  .. automodule:: llama_index.query_engine.flare.base    :members:    :inherited-members:\", \"Graph Query Engine =======================  .. automodule:: llama_index.query_engine.graph_query_engine    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Knowledge Graph Query Engine ============================  .. automodule:: llama_index.query_engine.knowledge_graph_query_engine    :members:    :inherited-members:\", \"Knowledge Graph RAG Query Engine ============================  .. automodule:: llama_index.query_engine.knowledge_graph_rag_query_engine    :members:    :inherited-members:\", \"Multistep Query Engine =======================  .. automodule:: llama_index.query_engine.multistep_query_engine    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Pandas Query Engine =======================  .. automodule:: llama_index.query_engine.pandas_query_engine    :members:    :inherited-members:\", \"Retriever Query Engine =======================  .. automodule:: llama_index.query_engine.retriever_query_engine    :members:    :inherited-members:\", \"Retriever Router Query Engine =============================  .. automodule:: llama_index.query_engine.retriever_query_engine    :members:    :inherited-members:\", \"Router Query Engine =======================  .. automodule:: llama_index.query_engine.router_query_engine    :members:    :inherited-members:    :exclude-members: acombine_responses, combine_responses, default_node_to_metadata_fn\", \"SQL Join Query Engine =======================  .. automodule:: llama_index.query_engine.sql_join_query_engine    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Flare Query Engine =======================  .. automodule:: llama_index.query_engine.flare.base    :members:    :inherited-members:\", \"Graph Query Engine =======================  .. automodule:: llama_index.query_engine.graph_query_engine    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Knowledge Graph Query Engine ============================  .. automodule:: llama_index.query_engine.knowledge_graph_query_engine    :members:    :inherited-members:\", \"Knowledge Graph RAG Query Engine ============================  .. automodule:: llama_index.query_engine.knowledge_graph_rag_query_engine    :members:    :inherited-members:\", \"Multistep Query Engine =======================  .. automodule:: llama_index.query_engine.multistep_query_engine    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Pandas Query Engine =======================  .. automodule:: llama_index.query_engine.pandas_query_engine    :members:    :inherited-members:\", \"Retriever Query Engine =======================  .. automodule:: llama_index.query_engine.retriever_query_engine    :members:    :inherited-members:\", \"Retriever Router Query Engine =============================  .. automodule:: llama_index.query_engine.retriever_query_engine    :members:    :inherited-members:\", \"Router Query Engine =======================  .. automodule:: llama_index.query_engine.router_query_engine    :members:    :inherited-members:    :exclude-members: acombine_responses, combine_responses, default_node_to_metadata_fn\", \"SQL Join Query Engine =======================  .. automodule:: llama_index.query_engine.sql_join_query_engine    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=69 request_id=80c089c99347bba9c79ea523ead8ce53 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=69 request_id=80c089c99347bba9c79ea523ead8ce53 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"SQL Query Engine =======================  .. automodule:: llama_index.indices.struct_store.sql_query    :members:    :inherited-members:\", \"Sub Question Query Engine ==========================  .. automodule:: llama_index.query_engine.sub_question_query_engine    :members:    :inherited-members:\", \"Transform Query Engine =======================  .. automodule:: llama_index.query_engine.transform_query_engine    :members:    :inherited-members:\", \".. _Ref-Query-Engines:  Query Engines =================  Below we show some general query engine classes.  .. toctree::    :maxdepth: 1    :caption: General Query Engines     query_engines/graph_query_engine.rst    query_engines/multistep_query_engine.rst    query_engines/retriever_query_engine.rst    query_engines/transform_query_engine.rst    query_engines/router_query_engine.rst    query_engines/retriever_router_query_engine.rst    query_engines/sub_question_query_engine.rst    query_engines/sql_join_query_engine.rst    query_engines/flare_query_engine.rst    query_engines/citation_query_engine.rst    query_engines/knowledge_graph_query_engine.rst   We also show query engine classes specific to our structured indices.  .. toctree::    :maxdepth: 1    :caption: Structured Indices Query Engines     query_engines/sql_query_engine.rst    query_engines/pandas_query_engine.rst\", \"Query Transform ===============  .. automodule:: llama_index.indices.query.query_transform    :members:    :inherited-members:    :exclude-members:\", \".. _Ref-Response-Synthesizer:  Response Synthesizer =====================  .. automodule:: llama_index.response_synthesizers    :members:    :inherited-members:\", \"Empty Index Retriever =======================  .. automodule:: llama_index.indices.empty.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Knowledge Graph Retriever ==========================  .. automodule:: llama_index.indices.knowledge_graph.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"List Retriever =======================  .. automodule:: llama_index.indices.list.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Keyword Table Retrievers =========================  .. automodule:: llama_index.indices.keyword_table.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"SQL Query Engine =======================  .. automodule:: llama_index.indices.struct_store.sql_query    :members:    :inherited-members:\", \"Sub Question Query Engine ==========================  .. automodule:: llama_index.query_engine.sub_question_query_engine    :members:    :inherited-members:\", \"Transform Query Engine =======================  .. automodule:: llama_index.query_engine.transform_query_engine    :members:    :inherited-members:\", \".. _Ref-Query-Engines:  Query Engines =================  Below we show some general query engine classes.  .. toctree::    :maxdepth: 1    :caption: General Query Engines     query_engines/graph_query_engine.rst    query_engines/multistep_query_engine.rst    query_engines/retriever_query_engine.rst    query_engines/transform_query_engine.rst    query_engines/router_query_engine.rst    query_engines/retriever_router_query_engine.rst    query_engines/sub_question_query_engine.rst    query_engines/sql_join_query_engine.rst    query_engines/flare_query_engine.rst    query_engines/citation_query_engine.rst    query_engines/knowledge_graph_query_engine.rst   We also show query engine classes specific to our structured indices.  .. toctree::    :maxdepth: 1    :caption: Structured Indices Query Engines     query_engines/sql_query_engine.rst    query_engines/pandas_query_engine.rst\", \"Query Transform ===============  .. automodule:: llama_index.indices.query.query_transform    :members:    :inherited-members:    :exclude-members:\", \".. _Ref-Response-Synthesizer:  Response Synthesizer =====================  .. automodule:: llama_index.response_synthesizers    :members:    :inherited-members:\", \"Empty Index Retriever =======================  .. automodule:: llama_index.indices.empty.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Knowledge Graph Retriever ==========================  .. automodule:: llama_index.indices.knowledge_graph.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"List Retriever =======================  .. automodule:: llama_index.indices.list.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Keyword Table Retrievers =========================  .. automodule:: llama_index.indices.keyword_table.retrievers    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=73 request_id=eb83d0884a235d8808436d06728d9699 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=73 request_id=eb83d0884a235d8808436d06728d9699 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Transform Retriever =======================  .. automodule:: llama_index.retrievers.transform_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Tree Retrievers =======================  .. automodule:: llama_index.indices.tree.all_leaf_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper  .. automodule:: llama_index.indices.tree.select_leaf_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper  .. automodule:: llama_index.indices.tree.select_leaf_embedding_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Vector Store Retrievers =======================  .. automodule:: llama_index.indices.vector_store.retrievers.retriever    :members:    :inherited-members:  .. automodule:: llama_index.indices.vector_store.retrievers.auto_retriever.auto_retriever    :members:    :inherited-members:  .. automodule:: llama_index.vector_stores.types    :members:     :inherited-members:     :exclude-members: VectorStore, VectorStoreQueryResult, NodeWithEmbedding, VectorStoreQuerySpec, VectorStoreQuery\", \".. _Ref-Retrievers:  Retrievers =================  Index Retrievers ^^^^^^^^^^^^^^^^ Below we show index-specific retriever classes.  .. toctree::    :maxdepth: 1    :caption: Index Retrievers     retrievers/empty.rst    retrievers/kg.rst    retrievers/list.rst    retrievers/table.rst    retrievers/tree.rst    retrievers/vector_store.rst  NOTE: our structured indices (e.g. PandasIndex) don\\'t have any retrievers, since they are not designed to be used with the retriever API. Please see the :ref:`Query Engine <Ref-Query-Engines>` page for more details.   Additional Retrievers ^^^^^^^^^^^^^^^^^^^^^  Here we show additional retriever classes; these classes can augment existing retrievers with new capabilities (e.g. query transforms).  .. toctree::    :maxdepth: 1    :caption: Additional Retrievers     retrievers/transform.rst   Base Retriever ^^^^^^^^^^^^^^^^^^^^^  Here we show the base retriever class, which contains the `retrieve` method which is shared amongst all retrievers.   .. automodule:: llama_index.indices.base_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \".. _Ref-Query:  Querying an Index =================  This doc shows the classes that are used to query indices.   Main Query Classes ^^^^^^^^^^^^^^^^^^  Querying an index involves a three main components:  - **Retrievers**: A retriever class retrieves a set of Nodes from an index given a query. - **Response Synthesizer**: This class takes in a set of Nodes and synthesizes an answer given a query. - **Query Engine**: This class takes in a query and returns a Response object. It can make use of Retrievers and Response Synthesizer modules under the hood. - **Chat Engines**: This class enables conversation over a knowledge base. It is the stateful version of a query engine that keeps track of conversation history.   .. toctree::    :maxdepth: 1    :caption: Main query classes     query/retrievers.rst    query/response_synthesizer.rst    query/query_engines.rst    query/chat_engines.rst   Additional Query Classes ^^^^^^^^^^^^^^^^^^^^^^^^  We also detail some additional query classes below.  - **Query Bundle**: This is the input to the query classes: retriever, response synthesizer,     and query engine. It enables the user to customize the string(s)     used for embedding-based query. - **Query Transform**: This class augments a raw query string with     associated transformations to improve index querying. Can be used     with a Retriever (see TransformRetriever) or QueryEngine.   .. toctree::    :maxdepth: 1    :caption: Additional query classes     query/query_bundle.rst    query/query_transform.rst\", \"Data Connectors ===============  NOTE: Our data connectors are now offered through `LlamaHub <https://llamahub.ai/>`_ \\\\ud83e\\\\udd99.  LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  The following data connectors are still available in the core repo.  .. automodule:: llama_index.readers    :members:    :inherited-members:\", \".. _Ref-Response:  Response =================  .. automodule:: llama_index.response.schema    :members:    :inherited-members:\", \".. _Ref-Embeddings:  Embeddings =================  Users have a few options to choose from when it comes to embeddings.  - :code:`OpenAIEmbedding`: the default embedding class. Defaults to \\\\\"text-embedding-ada-002\\\\\" - :code:`LangchainEmbedding`: a wrapper around Langchain\\'s embedding models.   .. automodule:: llama_index.embeddings.openai    :members:    :inherited-members:    :exclude-members: OAEM, OpenAIEmbeddingMode   We also introduce a :code:`LangchainEmbedding` class, which is a wrapper around Langchain\\'s embedding models. A full list of embeddings can be found `here <https://langchain.readthedocs.io/en/latest/reference/modules/embeddings.html>`_.  .. automodule:: llama_index.embeddings.langchain    :members:    :inherited-members:\", \"Node Parser ===========  .. automodule:: llama_index.node_parser    :members:    :inherited-members:  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.MetadataExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.SummaryExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.QuestionsAnsweredExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.TitleExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.KeywordExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.EntityExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.MetadataFeatureExtractor\", \".. _Ref-Prompt-Helper:  PromptHelper =================  .. automodule:: llama_index.indices.prompt_helper    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Transform Retriever =======================  .. automodule:: llama_index.retrievers.transform_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Tree Retrievers =======================  .. automodule:: llama_index.indices.tree.all_leaf_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper  .. automodule:: llama_index.indices.tree.select_leaf_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper  .. automodule:: llama_index.indices.tree.select_leaf_embedding_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \"Vector Store Retrievers =======================  .. automodule:: llama_index.indices.vector_store.retrievers.retriever    :members:    :inherited-members:  .. automodule:: llama_index.indices.vector_store.retrievers.auto_retriever.auto_retriever    :members:    :inherited-members:  .. automodule:: llama_index.vector_stores.types    :members:     :inherited-members:     :exclude-members: VectorStore, VectorStoreQueryResult, NodeWithEmbedding, VectorStoreQuerySpec, VectorStoreQuery\", \".. _Ref-Retrievers:  Retrievers =================  Index Retrievers ^^^^^^^^^^^^^^^^ Below we show index-specific retriever classes.  .. toctree::    :maxdepth: 1    :caption: Index Retrievers     retrievers/empty.rst    retrievers/kg.rst    retrievers/list.rst    retrievers/table.rst    retrievers/tree.rst    retrievers/vector_store.rst  NOTE: our structured indices (e.g. PandasIndex) don\\'t have any retrievers, since they are not designed to be used with the retriever API. Please see the :ref:`Query Engine <Ref-Query-Engines>` page for more details.   Additional Retrievers ^^^^^^^^^^^^^^^^^^^^^  Here we show additional retriever classes; these classes can augment existing retrievers with new capabilities (e.g. query transforms).  .. toctree::    :maxdepth: 1    :caption: Additional Retrievers     retrievers/transform.rst   Base Retriever ^^^^^^^^^^^^^^^^^^^^^  Here we show the base retriever class, which contains the `retrieve` method which is shared amongst all retrievers.   .. automodule:: llama_index.indices.base_retriever    :members:    :inherited-members: ..    :exclude-members: index_struct, query, set_llm_predictor, set_prompt_helper\", \".. _Ref-Query:  Querying an Index =================  This doc shows the classes that are used to query indices.   Main Query Classes ^^^^^^^^^^^^^^^^^^  Querying an index involves a three main components:  - **Retrievers**: A retriever class retrieves a set of Nodes from an index given a query. - **Response Synthesizer**: This class takes in a set of Nodes and synthesizes an answer given a query. - **Query Engine**: This class takes in a query and returns a Response object. It can make use of Retrievers and Response Synthesizer modules under the hood. - **Chat Engines**: This class enables conversation over a knowledge base. It is the stateful version of a query engine that keeps track of conversation history.   .. toctree::    :maxdepth: 1    :caption: Main query classes     query/retrievers.rst    query/response_synthesizer.rst    query/query_engines.rst    query/chat_engines.rst   Additional Query Classes ^^^^^^^^^^^^^^^^^^^^^^^^  We also detail some additional query classes below.  - **Query Bundle**: This is the input to the query classes: retriever, response synthesizer,     and query engine. It enables the user to customize the string(s)     used for embedding-based query. - **Query Transform**: This class augments a raw query string with     associated transformations to improve index querying. Can be used     with a Retriever (see TransformRetriever) or QueryEngine.   .. toctree::    :maxdepth: 1    :caption: Additional query classes     query/query_bundle.rst    query/query_transform.rst\", \"Data Connectors ===============  NOTE: Our data connectors are now offered through `LlamaHub <https://llamahub.ai/>`_ \\\\ud83e\\\\udd99.  LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  The following data connectors are still available in the core repo.  .. automodule:: llama_index.readers    :members:    :inherited-members:\", \".. _Ref-Response:  Response =================  .. automodule:: llama_index.response.schema    :members:    :inherited-members:\", \".. _Ref-Embeddings:  Embeddings =================  Users have a few options to choose from when it comes to embeddings.  - :code:`OpenAIEmbedding`: the default embedding class. Defaults to \\\\\"text-embedding-ada-002\\\\\" - :code:`LangchainEmbedding`: a wrapper around Langchain\\'s embedding models.   .. automodule:: llama_index.embeddings.openai    :members:    :inherited-members:    :exclude-members: OAEM, OpenAIEmbeddingMode   We also introduce a :code:`LangchainEmbedding` class, which is a wrapper around Langchain\\'s embedding models. A full list of embeddings can be found `here <https://langchain.readthedocs.io/en/latest/reference/modules/embeddings.html>`_.  .. automodule:: llama_index.embeddings.langchain    :members:    :inherited-members:\", \"Node Parser ===========  .. automodule:: llama_index.node_parser    :members:    :inherited-members:  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.MetadataExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.SummaryExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.QuestionsAnsweredExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.TitleExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.KeywordExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.EntityExtractor  ..  autopydantic_model:: llama_index.node_parser.extractors.metadata_extractors.MetadataFeatureExtractor\", \".. _Ref-Prompt-Helper:  PromptHelper =================  .. automodule:: llama_index.indices.prompt_helper    :members:    :inherited-members:\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=58 request_id=382e5c24ed602f1ab4ee409911406ae2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=58 request_id=382e5c24ed602f1ab4ee409911406ae2 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\".. _Ref-Service-Context:  Service Context =================  The service context container is a utility container for LlamaIndex index and query classes. The container contains the following  objects that are commonly used for configuring every index and query, such as the LLMPredictor (for configuring the LLM), the PromptHelper (for configuring input size/chunk size), the BaseEmbedding (for configuring the embedding model), and more.  |   .. toctree::    :maxdepth: 1    :caption: Service Context Classes     service_context/embeddings.rst    service_context/node_parser.rst    service_context/prompt_helper.rst    llms.rst  ------------  .. automodule:: llama_index.indices.service_context    :members:    :inherited-members:\", \".. _Ref-Storage-Docstore:  Document Store =====================  .. automodule:: llama_index.storage.docstore    :members:    :inherited-members:\", \".. _Ref-Storage-Index-Store:  Index Store =====================  .. automodule:: llama_index.storage.index_store    :members:    :inherited-members:\", \".. _Ref-Indices-SaveLoad:  Loading Indices =====================  .. automodule:: llama_index.indices.loading    :members:    :inherited-members:\", \".. _Ref-Storage-KVStore:   KV Storage =================  .. automodule:: llama_index.storage.kvstore    :members:    :inherited-members:\", \".. _Ref-Storage-Vector-Store:  Vector Store =====================  .. automodule:: llama_index.vector_stores    :members:    :inherited-members:\", \".. _Ref-Storage:   Storage Context =================  LlamaIndex offers core abstractions around storage of Nodes, indices, and vectors. A key abstraction is the `StorageContext` - this contains the underlying `BaseDocumentStore` (for nodes), `BaseIndexStore` (for indices), and `VectorStore` (for vectors).   The Document/Node and index stores rely on a common `KVStore` abstraction, which is also detailed below.   We show the API references for the Storage Classes, loading indices from the Storage Context, and the Storage Context class itself below.  |  .. toctree::    :maxdepth: 1    :caption: Storage Classes     storage/docstore.rst    storage/index_store.rst    storage/vector_store.rst    storage/kv_store.rst  |   .. toctree::    :maxdepth: 1    :caption: Loading Indices     storage/indices_save_load.rst  ------------  .. automodule:: llama_index.storage.storage_context    :members:    :inherited-members:\", \".. _Ref-Struct-Store:  Structured Index Configuration ==============================  Our structured indices are documented in :ref:`Ref-Indices-StructStore`. Below, we provide a reference of the classes that are used to configure our structured indices.  .. automodule:: llama_index.langchain_helpers.sql_wrapper    :members:    :inherited-members:  .. automodule:: llama_index.indices.struct_store.container_builder    :members:    :inherited-members:  .. automodule:: llama_index.indices.common.struct_store.base    :members:    :inherited-members:\", \"App Showcase  Here is a sample of some of the incredible applications and tools built on top of LlamaIndex!\", \"Meru - Dense Data Retrieval API  Hosted API service. Includes a \\\\\"Dense Data Retrieval\\\\\" API built on top of LlamaIndex where users can upload their documents and query them. [Website]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\".. _Ref-Service-Context:  Service Context =================  The service context container is a utility container for LlamaIndex index and query classes. The container contains the following  objects that are commonly used for configuring every index and query, such as the LLMPredictor (for configuring the LLM), the PromptHelper (for configuring input size/chunk size), the BaseEmbedding (for configuring the embedding model), and more.  |   .. toctree::    :maxdepth: 1    :caption: Service Context Classes     service_context/embeddings.rst    service_context/node_parser.rst    service_context/prompt_helper.rst    llms.rst  ------------  .. automodule:: llama_index.indices.service_context    :members:    :inherited-members:\", \".. _Ref-Storage-Docstore:  Document Store =====================  .. automodule:: llama_index.storage.docstore    :members:    :inherited-members:\", \".. _Ref-Storage-Index-Store:  Index Store =====================  .. automodule:: llama_index.storage.index_store    :members:    :inherited-members:\", \".. _Ref-Indices-SaveLoad:  Loading Indices =====================  .. automodule:: llama_index.indices.loading    :members:    :inherited-members:\", \".. _Ref-Storage-KVStore:   KV Storage =================  .. automodule:: llama_index.storage.kvstore    :members:    :inherited-members:\", \".. _Ref-Storage-Vector-Store:  Vector Store =====================  .. automodule:: llama_index.vector_stores    :members:    :inherited-members:\", \".. _Ref-Storage:   Storage Context =================  LlamaIndex offers core abstractions around storage of Nodes, indices, and vectors. A key abstraction is the `StorageContext` - this contains the underlying `BaseDocumentStore` (for nodes), `BaseIndexStore` (for indices), and `VectorStore` (for vectors).   The Document/Node and index stores rely on a common `KVStore` abstraction, which is also detailed below.   We show the API references for the Storage Classes, loading indices from the Storage Context, and the Storage Context class itself below.  |  .. toctree::    :maxdepth: 1    :caption: Storage Classes     storage/docstore.rst    storage/index_store.rst    storage/vector_store.rst    storage/kv_store.rst  |   .. toctree::    :maxdepth: 1    :caption: Loading Indices     storage/indices_save_load.rst  ------------  .. automodule:: llama_index.storage.storage_context    :members:    :inherited-members:\", \".. _Ref-Struct-Store:  Structured Index Configuration ==============================  Our structured indices are documented in :ref:`Ref-Indices-StructStore`. Below, we provide a reference of the classes that are used to configure our structured indices.  .. automodule:: llama_index.langchain_helpers.sql_wrapper    :members:    :inherited-members:  .. automodule:: llama_index.indices.struct_store.container_builder    :members:    :inherited-members:  .. automodule:: llama_index.indices.common.struct_store.base    :members:    :inherited-members:\", \"App Showcase  Here is a sample of some of the incredible applications and tools built on top of LlamaIndex!\", \"Meru - Dense Data Retrieval API  Hosted API service. Includes a \\\\\"Dense Data Retrieval\\\\\" API built on top of LlamaIndex where users can upload their documents and query them. [Website]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=85e60e41535b84fd5f781099ee54e89a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=85e60e41535b84fd5f781099ee54e89a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Algovera  Build AI workflows using building blocks. Many workflows built on top of LlamaIndex.  [Website].\", \"SlideSpeak  Summarize PowerPoint files and other documents with AI. SlideSpeak is an open source chatbot for presentations and other documents. Built on top of LlamaIndex, it utilizes Pinecone as a Vector Storage. We currently use ChatGPT Turbo 3.5 as a model for the chatbot. We\\'re currently working on adding support for other document formats, which will allow you to summarize presentations, Word documents, Google Slides, PDFs and much more.  [Website] [GitHub]\", \"ChatGPT LlamaIndex  Interface that allows users to upload long docs and chat with the bot. [Tweet thread]\", \"AgentHQ  A web tool to build agents, interacting with LlamaIndex data structures.[Website]\", \"SiteChatAI  SiteChatAi is ChatGPT powered that can be integrated into any website. It is a simple chatbot that can be used to answer simple questions and can be trained to answer more complex questions. It provides human like conversation experience to the users. It can be used to answer questions related to the website or the business. It uses Llamma Index and LangChain    Current version of SiteChatAI support following features:   - Multi-lingual support   - Real time chat   - Easy to integrate   - Customizable   - Human like conversation experience   - Can be trained to answer complex questions - and more.     [Website]\", \"PapersGPT  Feed any of the following content into GPT to give it deep customized knowledge: - Scientific Papers - Substack Articles - Podcasts - Github Repos and more.  [Tweet thread] [Website]\", \"VideoQues + DocsQues  **VideoQues**: A tool that answers your queries on YouTube videos.  [LinkedIn post here].  **DocsQues**: A tool that answers your questions on longer documents (including .pdfs!) [LinkedIn post here].\", \"PaperBrain  A platform to access/understand research papers.  [Tweet thread].\", \"CACTUS Contextual search on top of LinkedIn search results.  [LinkedIn post here].\", \"Personal Note Chatbot A chatbot that can answer questions over a directory of Obsidian notes.  [Tweet thread].\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Algovera  Build AI workflows using building blocks. Many workflows built on top of LlamaIndex.  [Website].\", \"SlideSpeak  Summarize PowerPoint files and other documents with AI. SlideSpeak is an open source chatbot for presentations and other documents. Built on top of LlamaIndex, it utilizes Pinecone as a Vector Storage. We currently use ChatGPT Turbo 3.5 as a model for the chatbot. We\\'re currently working on adding support for other document formats, which will allow you to summarize presentations, Word documents, Google Slides, PDFs and much more.  [Website] [GitHub]\", \"ChatGPT LlamaIndex  Interface that allows users to upload long docs and chat with the bot. [Tweet thread]\", \"AgentHQ  A web tool to build agents, interacting with LlamaIndex data structures.[Website]\", \"SiteChatAI  SiteChatAi is ChatGPT powered that can be integrated into any website. It is a simple chatbot that can be used to answer simple questions and can be trained to answer more complex questions. It provides human like conversation experience to the users. It can be used to answer questions related to the website or the business. It uses Llamma Index and LangChain    Current version of SiteChatAI support following features:   - Multi-lingual support   - Real time chat   - Easy to integrate   - Customizable   - Human like conversation experience   - Can be trained to answer complex questions - and more.     [Website]\", \"PapersGPT  Feed any of the following content into GPT to give it deep customized knowledge: - Scientific Papers - Substack Articles - Podcasts - Github Repos and more.  [Tweet thread] [Website]\", \"VideoQues + DocsQues  **VideoQues**: A tool that answers your queries on YouTube videos.  [LinkedIn post here].  **DocsQues**: A tool that answers your questions on longer documents (including .pdfs!) [LinkedIn post here].\", \"PaperBrain  A platform to access/understand research papers.  [Tweet thread].\", \"CACTUS Contextual search on top of LinkedIn search results.  [LinkedIn post here].\", \"Personal Note Chatbot A chatbot that can answer questions over a directory of Obsidian notes.  [Tweet thread].\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=208 request_id=813a47b4cf750da9c7b09abf433cdeb1 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=208 request_id=813a47b4cf750da9c7b09abf433cdeb1 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"RHOBH AMA  Ask questions about the Real Housewives of Beverly Hills. [Tweet thread] [Website]\", \"Mynd  A journaling app that uses AI to uncover insights and patterns over time. [Website]\", \"CoFounder The First AI Co-Founder for Your Start-up \\\\ud83d\\\\ude4c  CoFounder is a platform to revolutionize the start-up ecosystem by providing founders with unparalleled tools, resources, and support. We are changing how founders build their companies from 0-1\\\\u2014productizing the accelerator/incubator programs using AI.  Current features:  * AI Investor Matching and Introduction and Tracking * AI Pitch Deck creation * Real-time Pitch Deck practice/feedback * Automatic Competitive Analysis / Watchlist * More coming soon...  [Website]\", \"Al-X by OpenExO  Your Digital Transformation Co-Pilot [Website]\", \"AnySummary  Summarize any document, audio or video with AI [Website]\", \"Blackmaria  Python package for webscraping in Natural language. [Tweet thread] [Github]\", \"ChatGPT Plugin Integrations  **NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!\", \"ChatGPT Retrieval Plugin Integrations  The OpenAI ChatGPT Retrieval Plugin offers a centralized API specification for any document storage system to interact  with ChatGPT. Since this can be deployed on any service, this means that more and more document retrieval services will implement this spec; this allows them to not only interact with ChatGPT, but also interact with any LLM toolkit that may use  a retrieval service.  LlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.\", \"Loading Data from LlamaHub into the ChatGPT Retrieval Plugin  The ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load documents. This offers a natural integration point with LlamaHub, which offers over 65 data loaders from various API\\'s and document formats.  Here is a sample code snippet of showing how to load a document from LlamaHub into the JSON format that `/upsert` expects:  ```python from llama_index import download_loader, Document from typing import Dict, List import json\", \"download loader, load documents SimpleWebPageReader = download_loader(\\\\\"SimpleWebPageReader\\\\\") loader = SimpleWebPageReader(html_to_text=True) url = \\\\\"http://www.paulgraham.com/worked.html\\\\\" documents = loader.load_data(urls=[url])\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"RHOBH AMA  Ask questions about the Real Housewives of Beverly Hills. [Tweet thread] [Website]\", \"Mynd  A journaling app that uses AI to uncover insights and patterns over time. [Website]\", \"CoFounder The First AI Co-Founder for Your Start-up \\\\ud83d\\\\ude4c  CoFounder is a platform to revolutionize the start-up ecosystem by providing founders with unparalleled tools, resources, and support. We are changing how founders build their companies from 0-1\\\\u2014productizing the accelerator/incubator programs using AI.  Current features:  * AI Investor Matching and Introduction and Tracking * AI Pitch Deck creation * Real-time Pitch Deck practice/feedback * Automatic Competitive Analysis / Watchlist * More coming soon...  [Website]\", \"Al-X by OpenExO  Your Digital Transformation Co-Pilot [Website]\", \"AnySummary  Summarize any document, audio or video with AI [Website]\", \"Blackmaria  Python package for webscraping in Natural language. [Tweet thread] [Github]\", \"ChatGPT Plugin Integrations  **NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!\", \"ChatGPT Retrieval Plugin Integrations  The OpenAI ChatGPT Retrieval Plugin offers a centralized API specification for any document storage system to interact  with ChatGPT. Since this can be deployed on any service, this means that more and more document retrieval services will implement this spec; this allows them to not only interact with ChatGPT, but also interact with any LLM toolkit that may use  a retrieval service.  LlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.\", \"Loading Data from LlamaHub into the ChatGPT Retrieval Plugin  The ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load documents. This offers a natural integration point with LlamaHub, which offers over 65 data loaders from various API\\'s and document formats.  Here is a sample code snippet of showing how to load a document from LlamaHub into the JSON format that `/upsert` expects:  ```python from llama_index import download_loader, Document from typing import Dict, List import json\", \"download loader, load documents SimpleWebPageReader = download_loader(\\\\\"SimpleWebPageReader\\\\\") loader = SimpleWebPageReader(html_to_text=True) url = \\\\\"http://www.paulgraham.com/worked.html\\\\\" documents = loader.load_data(urls=[url])\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=66 request_id=cec1a8859ccefd6384266f6ee2644af7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=66 request_id=cec1a8859ccefd6384266f6ee2644af7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Convert LlamaIndex Documents to JSON format def dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:     \\\\\"\\\\\"\\\\\"Convert LlamaIndex Documents to JSON format and save it.\\\\\"\\\\\"\\\\\"     result_json = []     for doc in documents:         cur_dict = {             \\\\\"text\\\\\": doc.get_text(),             \\\\\"id\\\\\": doc.get_doc_id(),             # NOTE: feel free to customize the other fields as you wish             # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage             # \\\\\"source\\\\\": ...,             # \\\\\"source_id\\\\\": ...,             # \\\\\"url\\\\\": url,             # \\\\\"created_at\\\\\": ...,             # \\\\\"author\\\\\": \\\\\"Paul Graham\\\\\",         }         result_json.append(cur_dict)          json.dump(result_json, open(out_path, \\'w\\'))  ```  For more details, check out the full example notebook.\", \"ChatGPT Retrieval Plugin Data Loader  The ChatGPT Retrieval Plugin data loader can be accessed on LlamaHub.  It allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.  Example code:  ```python from llama_index.readers import ChatGPTRetrievalPluginReader import os\", \"load documents bearer_token = os.getenv(\\\\\"BEARER_TOKEN\\\\\") reader = ChatGPTRetrievalPluginReader(     endpoint_url=\\\\\"http://localhost:8000\\\\\",     bearer_token=bearer_token ) documents = reader.load_data(\\\\\"What did the author do growing up?\\\\\")\", \"build and query index from llama_index import ListIndex index = ListIndex(documents)\", \"set Logging to DEBUG for more detailed outputs query_engine = vector_index.as_query_engine(     response_mode=\\\\\"compact\\\\\" ) response = query_engine.query(     \\\\\"Summarize the retrieved content and describe what the author did growing up\\\\\", )   ``` For more details, check out the full example notebook.\", \"ChatGPT Retrieval Plugin Index  The ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the  ChatGPT endpoint.  Note: this index is a vector index, allowing top-k retrieval.  Example code:  ```python from llama_index.indices.vector_store import ChatGPTRetrievalPluginIndex from llama_index import SimpleDirectoryReader import os\", \"load documents documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data()\", \"build index bearer_token = os.getenv(\\\\\"BEARER_TOKEN\\\\\")\", \"initialize without metadata filter index = ChatGPTRetrievalPluginIndex(     documents,      endpoint_url=\\\\\"http://localhost:8000\\\\\",     bearer_token=bearer_token, )\", \"query index query_engine = vector_index.as_query_engine(     similarity_top_k=3,     response_mode=\\\\\"compact\\\\\", ) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ```  For more details, check out the full example notebook.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Convert LlamaIndex Documents to JSON format def dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:     \\\\\"\\\\\"\\\\\"Convert LlamaIndex Documents to JSON format and save it.\\\\\"\\\\\"\\\\\"     result_json = []     for doc in documents:         cur_dict = {             \\\\\"text\\\\\": doc.get_text(),             \\\\\"id\\\\\": doc.get_doc_id(),             # NOTE: feel free to customize the other fields as you wish             # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage             # \\\\\"source\\\\\": ...,             # \\\\\"source_id\\\\\": ...,             # \\\\\"url\\\\\": url,             # \\\\\"created_at\\\\\": ...,             # \\\\\"author\\\\\": \\\\\"Paul Graham\\\\\",         }         result_json.append(cur_dict)          json.dump(result_json, open(out_path, \\'w\\'))  ```  For more details, check out the full example notebook.\", \"ChatGPT Retrieval Plugin Data Loader  The ChatGPT Retrieval Plugin data loader can be accessed on LlamaHub.  It allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.  Example code:  ```python from llama_index.readers import ChatGPTRetrievalPluginReader import os\", \"load documents bearer_token = os.getenv(\\\\\"BEARER_TOKEN\\\\\") reader = ChatGPTRetrievalPluginReader(     endpoint_url=\\\\\"http://localhost:8000\\\\\",     bearer_token=bearer_token ) documents = reader.load_data(\\\\\"What did the author do growing up?\\\\\")\", \"build and query index from llama_index import ListIndex index = ListIndex(documents)\", \"set Logging to DEBUG for more detailed outputs query_engine = vector_index.as_query_engine(     response_mode=\\\\\"compact\\\\\" ) response = query_engine.query(     \\\\\"Summarize the retrieved content and describe what the author did growing up\\\\\", )   ``` For more details, check out the full example notebook.\", \"ChatGPT Retrieval Plugin Index  The ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the  ChatGPT endpoint.  Note: this index is a vector index, allowing top-k retrieval.  Example code:  ```python from llama_index.indices.vector_store import ChatGPTRetrievalPluginIndex from llama_index import SimpleDirectoryReader import os\", \"load documents documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data()\", \"build index bearer_token = os.getenv(\\\\\"BEARER_TOKEN\\\\\")\", \"initialize without metadata filter index = ChatGPTRetrievalPluginIndex(     documents,      endpoint_url=\\\\\"http://localhost:8000\\\\\",     bearer_token=bearer_token, )\", \"query index query_engine = vector_index.as_query_engine(     similarity_top_k=3,     response_mode=\\\\\"compact\\\\\", ) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ```  For more details, check out the full example notebook.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=162 request_id=c373c6af2bc26a2543fc71ae10ee259e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=162 request_id=c373c6af2bc26a2543fc71ae10ee259e response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Using Graph Stores\", \"`Neo4jGraphStore`  `Neo4j` is supported as a graph store integration. You can persist, visualze, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.  If you\\'ve never used Neo4j before, you can download the desktop client here.  Once you open the client, create a new project and install the `apoc` integration. Full instructions here. Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server.  ```{toctree} --- maxdepth: 1 --- Neo4j Graph Store  ```\", \"`NebulaGraphStore`  We support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- Nebula Graph Store  Knowledge Graph Query Engine  ```\", \"`KuzuGraphStore`  We support a `KuzuGraphStore` integration, for persisting graphs directly in Kuzu.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- Kuzu Graph Store  ```\", \"`FalkorDBGraphStore`  We support a `FalkorDBGraphStore` integration, for persisting graphs directly in FalkorDB! Furthermore, you can generate cypher queries and return natural language responses for your FalkorDB graphs using the `KnowledgeGraphQueryEngine`.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- FalkorDB Graph Store  ```\", \"Tracing with Graphsignal  Graphsignal provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.  Graphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.  When OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.\", \"Installation and Setup  Adding Graphsignal tracer is simple, just install and configure it:  ```sh pip install graphsignal ```  ```python import graphsignal\", \"Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable graphsignal.configure(api_key=\\'my-api-key\\', deployment=\\'my-llama-index-app-prod\\') ```  You can get an API key here.  See the Quick Start guide, Integration guide, and an example app for more information.\", \"Tracing Other Functions  To additionally trace any function or code, you can use a decorator or a context manager:  ```python with graphsignal.start_trace(\\'load-external-data\\'):     reader.load_data() ```  See Python API Reference for complete instructions.\", \"Useful Links  * Tracing and Monitoring LlamaIndex Applications * Monitor OpenAI API Latency, Tokens, Rate Limits, and More * OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Using Graph Stores\", \"`Neo4jGraphStore`  `Neo4j` is supported as a graph store integration. You can persist, visualze, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.  If you\\'ve never used Neo4j before, you can download the desktop client here.  Once you open the client, create a new project and install the `apoc` integration. Full instructions here. Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server.  ```{toctree} --- maxdepth: 1 --- Neo4j Graph Store  ```\", \"`NebulaGraphStore`  We support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- Nebula Graph Store  Knowledge Graph Query Engine  ```\", \"`KuzuGraphStore`  We support a `KuzuGraphStore` integration, for persisting graphs directly in Kuzu.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- Kuzu Graph Store  ```\", \"`FalkorDBGraphStore`  We support a `FalkorDBGraphStore` integration, for persisting graphs directly in FalkorDB! Furthermore, you can generate cypher queries and return natural language responses for your FalkorDB graphs using the `KnowledgeGraphQueryEngine`.  See the associated guides below:  ```{toctree} --- maxdepth: 1 --- FalkorDB Graph Store  ```\", \"Tracing with Graphsignal  Graphsignal provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.  Graphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.  When OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.\", \"Installation and Setup  Adding Graphsignal tracer is simple, just install and configure it:  ```sh pip install graphsignal ```  ```python import graphsignal\", \"Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable graphsignal.configure(api_key=\\'my-api-key\\', deployment=\\'my-llama-index-app-prod\\') ```  You can get an API key here.  See the Quick Start guide, Integration guide, and an example app for more information.\", \"Tracing Other Functions  To additionally trace any function or code, you can use a decorator or a context manager:  ```python with graphsignal.start_trace(\\'load-external-data\\'):     reader.load_data() ```  See Python API Reference for complete instructions.\", \"Useful Links  * Tracing and Monitoring LlamaIndex Applications * Monitor OpenAI API Latency, Tokens, Rate Limits, and More * OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=93 request_id=ba17d790e99bbe15beed7a084f19790b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=93 request_id=ba17d790e99bbe15beed7a084f19790b response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Guidance  Guidance is a guidance language for controlling large language models developed by Microsoft.  Guidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\", \"Structured Output One particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \\\\\"suggesting\\\\\" the desired output structure to the LLM, guidance can actually \\\\\"force\\\\\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.  This is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.\", \"Creating a guidance program to generate pydantic objects In LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).  For example, if we want to generate an album of songs, with the following schema:  ```python class Song(BaseModel):     title: str     length_seconds: int      class Album(BaseModel):     name: str     artist: str     songs: List[Song] ```  It\\'s as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`,  and supplying a suitable prompt template.  > Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings.   > Note: We provide an utility function `from llama_index.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.   ```python program = GuidancePydanticProgram(     output_cls=Album,      prompt_template_str=\\\\\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\\\\\",     guidance_llm=OpenAI(\\'text-davinci-003\\'),     verbose=True, )  ```  Now we can run the program by calling it with additional user input.  Here let\\'s go for something spooky and create an album inspired by the Shining. ```python output = program(movie_name=\\'The Shining\\') ```  We have our pydantic object: ```python Album(name=\\'The Shining\\', artist=\\'Jack Torrance\\', songs=[Song(title=\\'All Work and No Play\\', length_seconds=180), Song(title=\\'The Overlook Hotel\\', length_seconds=240), Song(title=\\'The Shining\\', length_seconds=210)]) ```  You can play with this notebook for more details.\", \"Using guidance to improve the robustness of our sub-question query engine. LlamaIndex provides a toolkit of advanced query engines for tackling different use-cases. Several relies on structured output in intermediate steps. We can use guidance to improve the robustness of these query engines, by making sure the intermediate response has the expected structure (so that they can be parsed correctly to a structured object).  As an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting. ```python from llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator from guidance.llms import OpenAI as GuidanceOpenAI\", \"define guidance based question generator question_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=GuidanceOpenAI(\\'text-davinci-003\\'), verbose=False)\", \"define query engine tools query_engine_tools = ...\", \"construct sub-question query engine s_engine = SubQuestionQueryEngine.from_defaults(     question_gen=question_gen  # use guidance based question_gen defined above     query_engine_tools=query_engine_tools,  ) ```  See this notebook for more details.\", \"Using Managed Indices  LlamaIndex offers multiple integration points with Managed Indices. A managed index is a special type of index that is not managed locally as part of LlamaIndex but instead is managed via an API, such as Vectara.\", \"Using a Managed Index  Similar to any other index within LlamaIndex (tree, keyword table, list), any `ManagedIndex` can be constructed with a collection of documents. Once constructed, the index can be used for querying.  If the Index has been previously populated with documents - it can also be used directly for querying.  `VectaraIndex` is currently the only supported managed index, although we expect more to be available soon. Below we show how to use it.  **Vectara Index Construction/Querying**  Use the Vectara Console to create a corpus (aka Index), and add an API key for access.  Then put the customer id, corpus id, and API key in your environment as shown below.  Then construct the Vectara Index and query it as follows:  ```python from llama_index import ManagedIndex, SimpleDirectoryReade from llama_index.managed import VectaraIndex\", \"Load documents and build index vectara_customer_id = os.environ.get(\\\\\"VECTARA_CUSTOMER_ID\\\\\") vectara_corpus_id = os.environ.get(\\\\\"VECTARA_CORPUS_ID\\\\\") vectara_api_key = os.environ.get(\\\\\"VECTARA_API_KEY\\\\\") documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectaraIndex.from_documents(documents, vectara_customer_id=vectara_customer_id, vectara_corpus_id=vectara_corpus_id, vectara_api_key=vectara_api_key)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Guidance  Guidance is a guidance language for controlling large language models developed by Microsoft.  Guidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\", \"Structured Output One particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \\\\\"suggesting\\\\\" the desired output structure to the LLM, guidance can actually \\\\\"force\\\\\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.  This is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.\", \"Creating a guidance program to generate pydantic objects In LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).  For example, if we want to generate an album of songs, with the following schema:  ```python class Song(BaseModel):     title: str     length_seconds: int      class Album(BaseModel):     name: str     artist: str     songs: List[Song] ```  It\\'s as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`,  and supplying a suitable prompt template.  > Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings.   > Note: We provide an utility function `from llama_index.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.   ```python program = GuidancePydanticProgram(     output_cls=Album,      prompt_template_str=\\\\\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\\\\\",     guidance_llm=OpenAI(\\'text-davinci-003\\'),     verbose=True, )  ```  Now we can run the program by calling it with additional user input.  Here let\\'s go for something spooky and create an album inspired by the Shining. ```python output = program(movie_name=\\'The Shining\\') ```  We have our pydantic object: ```python Album(name=\\'The Shining\\', artist=\\'Jack Torrance\\', songs=[Song(title=\\'All Work and No Play\\', length_seconds=180), Song(title=\\'The Overlook Hotel\\', length_seconds=240), Song(title=\\'The Shining\\', length_seconds=210)]) ```  You can play with this notebook for more details.\", \"Using guidance to improve the robustness of our sub-question query engine. LlamaIndex provides a toolkit of advanced query engines for tackling different use-cases. Several relies on structured output in intermediate steps. We can use guidance to improve the robustness of these query engines, by making sure the intermediate response has the expected structure (so that they can be parsed correctly to a structured object).  As an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting. ```python from llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator from guidance.llms import OpenAI as GuidanceOpenAI\", \"define guidance based question generator question_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=GuidanceOpenAI(\\'text-davinci-003\\'), verbose=False)\", \"define query engine tools query_engine_tools = ...\", \"construct sub-question query engine s_engine = SubQuestionQueryEngine.from_defaults(     question_gen=question_gen  # use guidance based question_gen defined above     query_engine_tools=query_engine_tools,  ) ```  See this notebook for more details.\", \"Using Managed Indices  LlamaIndex offers multiple integration points with Managed Indices. A managed index is a special type of index that is not managed locally as part of LlamaIndex but instead is managed via an API, such as Vectara.\", \"Using a Managed Index  Similar to any other index within LlamaIndex (tree, keyword table, list), any `ManagedIndex` can be constructed with a collection of documents. Once constructed, the index can be used for querying.  If the Index has been previously populated with documents - it can also be used directly for querying.  `VectaraIndex` is currently the only supported managed index, although we expect more to be available soon. Below we show how to use it.  **Vectara Index Construction/Querying**  Use the Vectara Console to create a corpus (aka Index), and add an API key for access.  Then put the customer id, corpus id, and API key in your environment as shown below.  Then construct the Vectara Index and query it as follows:  ```python from llama_index import ManagedIndex, SimpleDirectoryReade from llama_index.managed import VectaraIndex\", \"Load documents and build index vectara_customer_id = os.environ.get(\\\\\"VECTARA_CUSTOMER_ID\\\\\") vectara_corpus_id = os.environ.get(\\\\\"VECTARA_CORPUS_ID\\\\\") vectara_api_key = os.environ.get(\\\\\"VECTARA_API_KEY\\\\\") documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectaraIndex.from_documents(documents, vectara_customer_id=vectara_customer_id, vectara_corpus_id=vectara_corpus_id, vectara_api_key=vectara_api_key)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=98 request_id=934cf5ddd206f4f3bac9e9ba3a8f652b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=98 request_id=934cf5ddd206f4f3bac9e9ba3a8f652b response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  Note that if the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY` are in the environment already, you do not have to explicitly specifying them in your call and the VectaraIndex class will read them from the enviornment. For example this should be equivalent to the above, if these variables are in the environment already:  ```python from llama_index import ManagedIndex, SimpleDirectoryReade from llama_index.managed import VectaraIndex\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectaraIndex.from_documents(documents)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```     ```{toctree} --- caption: Examples maxdepth: 1 --- ../../examples/vector_stores/VectaraDemo.ipynb ```\", \"Evaluating and Tracking with TruLens  This page covers how to use TruLens to evaluate and track LLM apps built on Llama-Index.\", \"What is TruLens?  TruLens is an opensource package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.  !TruLens Architecture  As you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you\\'ve set up. You\\'ll also be able to view evaluations at a record level, and explore the app metadata for each record.\", \"Installation and Setup  Adding TruLens is simple, just install it from pypi!  ```sh pip install trulens-eval ```  ```python from trulens_eval import TruLlama  ```\", \"Try it out!  llama_index_quickstart.ipynb  ![Open In Colab](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb)\", \"Read more  * Build and Evaluate LLM Apps with LlamaIndex and TruLens * More examples * trulens.org\", \"Using with Langchain \\\\ud83e\\\\udd9c\\\\ud83d\\\\udd17  LlamaIndex provides both Tool abstractions for a Langchain agent as well as a memory module.  The API reference of the Tool abstractions + memory modules are here.\", \"Use any data loader as a Langchain Tool  LlamaIndex allows you to use any data loader within the LlamaIndex core repo or in LlamaHub as an \\\\\"on-demand\\\\\" data query Tool within a LangChain agent.  The Tool will 1) load data using the data loader, 2) index the data, and 3) query the data and return the response in an ad-hoc manner.  **Resources** - OnDemandLoaderTool Tutorial\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  Note that if the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY` are in the environment already, you do not have to explicitly specifying them in your call and the VectaraIndex class will read them from the enviornment. For example this should be equivalent to the above, if these variables are in the environment already:  ```python from llama_index import ManagedIndex, SimpleDirectoryReade from llama_index.managed import VectaraIndex\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectaraIndex.from_documents(documents)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```     ```{toctree} --- caption: Examples maxdepth: 1 --- ../../examples/vector_stores/VectaraDemo.ipynb ```\", \"Evaluating and Tracking with TruLens  This page covers how to use TruLens to evaluate and track LLM apps built on Llama-Index.\", \"What is TruLens?  TruLens is an opensource package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.  !TruLens Architecture  As you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you\\'ve set up. You\\'ll also be able to view evaluations at a record level, and explore the app metadata for each record.\", \"Installation and Setup  Adding TruLens is simple, just install it from pypi!  ```sh pip install trulens-eval ```  ```python from trulens_eval import TruLlama  ```\", \"Try it out!  llama_index_quickstart.ipynb  ![Open In Colab](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb)\", \"Read more  * Build and Evaluate LLM Apps with LlamaIndex and TruLens * More examples * trulens.org\", \"Using with Langchain \\\\ud83e\\\\udd9c\\\\ud83d\\\\udd17  LlamaIndex provides both Tool abstractions for a Langchain agent as well as a memory module.  The API reference of the Tool abstractions + memory modules are here.\", \"Use any data loader as a Langchain Tool  LlamaIndex allows you to use any data loader within the LlamaIndex core repo or in LlamaHub as an \\\\\"on-demand\\\\\" data query Tool within a LangChain agent.  The Tool will 1) load data using the data loader, 2) index the data, and 3) query the data and return the response in an ad-hoc manner.  **Resources** - OnDemandLoaderTool Tutorial\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=212 request_id=50f0635396bc928248fe7d5a841e9166 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=212 request_id=50f0635396bc928248fe7d5a841e9166 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Use a query engine as a Langchain Tool LlamaIndex provides Tool abstractions so that you can use a LlamaIndex query engine along with a Langchain agent.   For instance, you can choose to create a \\\\\"Tool\\\\\" from an `QueryEngine` directly as follows:  ```python from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool  tool_config = IndexToolConfig(     query_engine=query_engine,      name=f\\\\\"Vector Index\\\\\",     description=f\\\\\"useful for when you want to answer queries about X\\\\\",     tool_kwargs={\\\\\"return_direct\\\\\": True} )  tool = LlamaIndexTool.from_tool_config(tool_config)  ```  You can also choose to provide a `LlamaToolkit`:  ```python toolkit = LlamaToolkit(     index_configs=index_configs, ) ```  Such a toolkit can be used to create a downstream Langchain-based chat agent through our `create_llama_agent` and `create_llama_chat_agent` commands:  ```python from llama_index.langchain_helpers.agents import create_llama_chat_agent  agent_chain = create_llama_chat_agent(     toolkit,     llm,     memory=memory,     verbose=True )  agent_chain.run(input=\\\\\"Query about X\\\\\") ```  You can take a look at the full tutorial notebook here.\", \"Llama Demo Notebook: Tool + Memory module  We provide another demo notebook showing how you can build a chat agent with the following components. - Using LlamaIndex as a generic callable tool with a Langchain agent - Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!  Please see the notebook here.\", \"Using Vector Stores  LlamaIndex offers multiple integration points with vector stores / vector databases:  1. LlamaIndex can use a vector store itself as an index. Like any other index, this index can store documents and be used to answer queries. 2. LlamaIndex can load data from vector stores, similar to any other data connector. This data can then be used within LlamaIndex data structures.  (vector-store-index)=\", \"Using a Vector Store as an Index  LlamaIndex also supports different vector stores as the storage backend for `VectorStoreIndex`.  - Apache Cassandra\\\\u00ae and compatible databases such as Astra DB (`CassandraVectorStore`) - Chroma (`ChromaVectorStore`) Installation - DeepLake (`DeepLakeVectorStore`) Installation - Qdrant (`QdrantVectorStore`) Installation Python Client - Weaviate (`WeaviateVectorStore`). Installation. Python Client. - Zep (`ZepVectorStore`). Installation. Python Client. - Pinecone (`PineconeVectorStore`). Installation/Quickstart. - Faiss (`FaissVectorStore`). Installation. - Milvus (`MilvusVectorStore`). Installation - Zilliz (`MilvusVectorStore`). Quickstart - MyScale (`MyScaleVectorStore`). Quickstart. Installation/Python Client. - Supabase (`SupabaseVectorStore`). Quickstart. - DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). Installation/Python Client. - MongoDB Atlas (`MongoDBAtlasVectorSearch`). Installation/Quickstart. - Redis (`RedisVectorStore`). Installation.  A detailed API reference is found here.  Similar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection of documents. We use the vector store within the index to store embeddings for the input text chunks.  Once constructed, the index can be used for querying.  **Default Vector Store Index Construction/Querying**  By default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore` that\\'s initialized as part of the default storage context.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ```  **Custom Vector Store Index Construction/Querying**  We can query over a custom vector store as follows:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext from llama_index.vector_stores import DeepLakeVectorStore\", \"construct vector store and customize storage context storage_context = StorageContext.from_defaults(     vector_store = DeepLakeVectorStore(dataset_path=\\\\\"\\\\\") )\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  Below we show more examples of how to construct various vector stores we support.  **Redis**  First, start Redis-Stack (or get url from Redis provider)  ```bash docker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ```  Then connect and use Redis as a vector database with LlamaIndex  ```python from llama_index.vector_stores import RedisVectorStore vector_store = RedisVectorStore(     index_name=\\\\\"llm-project\\\\\",     redis_url=\\\\\"redis://localhost:6379\\\\\",     overwrite=True ) ```  This can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.  **DeepLake**  ```python import os import getpath from llama_index.vector_stores import DeepLakeVectorStore  os.environ[\\\\\"OPENAI_API_KEY\\\\\"] = getpath.getpath(\\\\\"OPENAI_API_KEY: \\\\\") os.environ[\\\\\"ACTIVELOOP_TOKEN\\\\\"] = getpath.getpath(\\\\\"ACTIVELOOP_TOKEN: \\\\\") dataset_path = \\\\\"hub://adilkhan/paul_graham_essay\\\\\"\", \"construct vector store vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True) ```  **Faiss**  ```python import faiss from llama_index.vector_stores import FaissVectorStore\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Use a query engine as a Langchain Tool LlamaIndex provides Tool abstractions so that you can use a LlamaIndex query engine along with a Langchain agent.   For instance, you can choose to create a \\\\\"Tool\\\\\" from an `QueryEngine` directly as follows:  ```python from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool  tool_config = IndexToolConfig(     query_engine=query_engine,      name=f\\\\\"Vector Index\\\\\",     description=f\\\\\"useful for when you want to answer queries about X\\\\\",     tool_kwargs={\\\\\"return_direct\\\\\": True} )  tool = LlamaIndexTool.from_tool_config(tool_config)  ```  You can also choose to provide a `LlamaToolkit`:  ```python toolkit = LlamaToolkit(     index_configs=index_configs, ) ```  Such a toolkit can be used to create a downstream Langchain-based chat agent through our `create_llama_agent` and `create_llama_chat_agent` commands:  ```python from llama_index.langchain_helpers.agents import create_llama_chat_agent  agent_chain = create_llama_chat_agent(     toolkit,     llm,     memory=memory,     verbose=True )  agent_chain.run(input=\\\\\"Query about X\\\\\") ```  You can take a look at the full tutorial notebook here.\", \"Llama Demo Notebook: Tool + Memory module  We provide another demo notebook showing how you can build a chat agent with the following components. - Using LlamaIndex as a generic callable tool with a Langchain agent - Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!  Please see the notebook here.\", \"Using Vector Stores  LlamaIndex offers multiple integration points with vector stores / vector databases:  1. LlamaIndex can use a vector store itself as an index. Like any other index, this index can store documents and be used to answer queries. 2. LlamaIndex can load data from vector stores, similar to any other data connector. This data can then be used within LlamaIndex data structures.  (vector-store-index)=\", \"Using a Vector Store as an Index  LlamaIndex also supports different vector stores as the storage backend for `VectorStoreIndex`.  - Apache Cassandra\\\\u00ae and compatible databases such as Astra DB (`CassandraVectorStore`) - Chroma (`ChromaVectorStore`) Installation - DeepLake (`DeepLakeVectorStore`) Installation - Qdrant (`QdrantVectorStore`) Installation Python Client - Weaviate (`WeaviateVectorStore`). Installation. Python Client. - Zep (`ZepVectorStore`). Installation. Python Client. - Pinecone (`PineconeVectorStore`). Installation/Quickstart. - Faiss (`FaissVectorStore`). Installation. - Milvus (`MilvusVectorStore`). Installation - Zilliz (`MilvusVectorStore`). Quickstart - MyScale (`MyScaleVectorStore`). Quickstart. Installation/Python Client. - Supabase (`SupabaseVectorStore`). Quickstart. - DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). Installation/Python Client. - MongoDB Atlas (`MongoDBAtlasVectorSearch`). Installation/Quickstart. - Redis (`RedisVectorStore`). Installation.  A detailed API reference is found here.  Similar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection of documents. We use the vector store within the index to store embeddings for the input text chunks.  Once constructed, the index can be used for querying.  **Default Vector Store Index Construction/Querying**  By default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore` that\\'s initialized as part of the default storage context.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ```  **Custom Vector Store Index Construction/Querying**  We can query over a custom vector store as follows:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext from llama_index.vector_stores import DeepLakeVectorStore\", \"construct vector store and customize storage context storage_context = StorageContext.from_defaults(     vector_store = DeepLakeVectorStore(dataset_path=\\\\\"\\\\\") )\", \"Load documents and build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"Query index query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  Below we show more examples of how to construct various vector stores we support.  **Redis**  First, start Redis-Stack (or get url from Redis provider)  ```bash docker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ```  Then connect and use Redis as a vector database with LlamaIndex  ```python from llama_index.vector_stores import RedisVectorStore vector_store = RedisVectorStore(     index_name=\\\\\"llm-project\\\\\",     redis_url=\\\\\"redis://localhost:6379\\\\\",     overwrite=True ) ```  This can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.  **DeepLake**  ```python import os import getpath from llama_index.vector_stores import DeepLakeVectorStore  os.environ[\\\\\"OPENAI_API_KEY\\\\\"] = getpath.getpath(\\\\\"OPENAI_API_KEY: \\\\\") os.environ[\\\\\"ACTIVELOOP_TOKEN\\\\\"] = getpath.getpath(\\\\\"ACTIVELOOP_TOKEN: \\\\\") dataset_path = \\\\\"hub://adilkhan/paul_graham_essay\\\\\"\", \"construct vector store vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True) ```  **Faiss**  ```python import faiss from llama_index.vector_stores import FaissVectorStore\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=06fc8254ee53c007d3d95505e8c815cc response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=06fc8254ee53c007d3d95505e8c815cc response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"create faiss index d = 1536 faiss_index = faiss.IndexFlatL2(d)\", \"construct vector store vector_store = FaissVectorStore(faiss_index)  ...\", \"NOTE: since faiss index is in-memory, we need to explicitly call storage_context.persist() ```  **Weaviate**  ```python import weaviate from llama_index.vector_stores import WeaviateVectorStore\", \"creating a Weaviate client resource_owner_config = weaviate.AuthClientPassword(     username=\\\\\"\\\\\",     password=\\\\\"\\\\\", ) client = weaviate.Client(     \\\\\"https://.semi.network/\\\\\", auth_client_secret=resource_owner_config )\", \"construct vector store vector_store = WeaviateVectorStore(weaviate_client=client) ```  **Zep**  Zep stores texts, metadata, and embeddings. All are returned in search results.  ```python  from llama_index.vector_stores import ZepVectorStore  vector_store = ZepVectorStore(     api_url=\\\\\"\\\\\",      api_key=\\\\\"\\\\\",      collection_name=\\\\\"\\\\\",  # Can either be an existing collection or a new one     embedding_dimensions=1536 # Optional, required if creating a new collection )  storage_context = StorageContext.from_defaults(vector_store=vector_store)  index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"Query index using both a text query and metadata filters filters = MetadataFilters(filters=[ExactMatchFilter(key=\\\\\"theme\\\\\", value=\\\\\"Mafia\\\\\")]) retriever = index.as_retriever(filters=filters) result = retriever.retrieve(\\\\\"What is inception about?\\\\\") ```   **Pinecone**  ```python import pinecone from llama_index.vector_stores import PineconeVectorStore\", \"Creating a Pinecone index api_key = \\\\\"api_key\\\\\" pinecone.init(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\") pinecone.create_index(     \\\\\"quickstart\\\\\",     dimension=1536,     metric=\\\\\"euclidean\\\\\",     pod_type=\\\\\"p1\\\\\" ) index = pinecone.Index(\\\\\"quickstart\\\\\")\", \"can define filters specific to this vector index (so you can metadata_filters = {\\\\\"title\\\\\": \\\\\"paul_graham_essay\\\\\"}\", \"construct vector store vector_store = PineconeVectorStore(     pinecone_index=index,     metadata_filters=metadata_filters ) ```  **Qdrant**  ```python import qdrant_client from llama_index.vector_stores import QdrantVectorStore\", \"Creating a Qdrant vector store client = qdrant_client.QdrantClient(     host=\\\\\"\\\\\",     api_key=\\\\\"\\\\\",     https=True ) collection_name = \\\\\"paul_graham\\\\\"\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"create faiss index d = 1536 faiss_index = faiss.IndexFlatL2(d)\", \"construct vector store vector_store = FaissVectorStore(faiss_index)  ...\", \"NOTE: since faiss index is in-memory, we need to explicitly call storage_context.persist() ```  **Weaviate**  ```python import weaviate from llama_index.vector_stores import WeaviateVectorStore\", \"creating a Weaviate client resource_owner_config = weaviate.AuthClientPassword(     username=\\\\\"\\\\\",     password=\\\\\"\\\\\", ) client = weaviate.Client(     \\\\\"https://.semi.network/\\\\\", auth_client_secret=resource_owner_config )\", \"construct vector store vector_store = WeaviateVectorStore(weaviate_client=client) ```  **Zep**  Zep stores texts, metadata, and embeddings. All are returned in search results.  ```python  from llama_index.vector_stores import ZepVectorStore  vector_store = ZepVectorStore(     api_url=\\\\\"\\\\\",      api_key=\\\\\"\\\\\",      collection_name=\\\\\"\\\\\",  # Can either be an existing collection or a new one     embedding_dimensions=1536 # Optional, required if creating a new collection )  storage_context = StorageContext.from_defaults(vector_store=vector_store)  index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"Query index using both a text query and metadata filters filters = MetadataFilters(filters=[ExactMatchFilter(key=\\\\\"theme\\\\\", value=\\\\\"Mafia\\\\\")]) retriever = index.as_retriever(filters=filters) result = retriever.retrieve(\\\\\"What is inception about?\\\\\") ```   **Pinecone**  ```python import pinecone from llama_index.vector_stores import PineconeVectorStore\", \"Creating a Pinecone index api_key = \\\\\"api_key\\\\\" pinecone.init(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\") pinecone.create_index(     \\\\\"quickstart\\\\\",     dimension=1536,     metric=\\\\\"euclidean\\\\\",     pod_type=\\\\\"p1\\\\\" ) index = pinecone.Index(\\\\\"quickstart\\\\\")\", \"can define filters specific to this vector index (so you can metadata_filters = {\\\\\"title\\\\\": \\\\\"paul_graham_essay\\\\\"}\", \"construct vector store vector_store = PineconeVectorStore(     pinecone_index=index,     metadata_filters=metadata_filters ) ```  **Qdrant**  ```python import qdrant_client from llama_index.vector_stores import QdrantVectorStore\", \"Creating a Qdrant vector store client = qdrant_client.QdrantClient(     host=\\\\\"\\\\\",     api_key=\\\\\"\\\\\",     https=True ) collection_name = \\\\\"paul_graham\\\\\"\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=229a2231d03d948f51dbf2803251396b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=229a2231d03d948f51dbf2803251396b response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"construct vector store vector_store = QdrantVectorStore(     client=client,     collection_name=collection_name, ) ```  **Cassandra** (covering DataStax Astra DB as well, which is built on Cassandra)  ```python from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider from llama_index.vector_stores import CassandraVectorStore\", \"for a Cassandra cluster: cluster = Cluster([\\\\\"127.0.0.1\\\\\"])\", \"for an Astra DB cloud instance: cluster = Cluster(   cloud={\\\\\"secure_connect_bundle\\\\\": \\\\\"/home/USER/secure-bundle.zip\\\\\"},   auth_provider=PlainTextAuthProvider(\\\\\"token\\\\\", \\\\\"AstraCS:...\\\\\") ) # session = cluster.connect() keyspace = \\\\\"my_cassandra_keyspace\\\\\"  vector_store = CassandraVectorStore(     session=session,     keyspace=keyspace,     table=\\\\\"llamaindex_vector_test_1\\\\\",     embedding_dimension=1536,     #insertion_batch_size=50,  # optional ) ```  **Chroma**  ```python import chromadb from llama_index.vector_stores import ChromaVectorStore\", \"Creating a Chroma client chroma_client = chromadb.EphemeralClient() chroma_collection = chroma_client.create_collection(\\\\\"quickstart\\\\\")\", \"construct vector store vector_store = ChromaVectorStore(     chroma_collection=chroma_collection, ) ```  **Milvus**  - Milvus Index offers the ability to store both Documents and their embeddings. Documents are limited to the predefined Document attributes and does not include metadata.  ```python import pymilvus from llama_index.vector_stores import MilvusVectorStore\", \"construct vector store vector_store = MilvusVectorStore(     host=\\'localhost\\',     port=19530,     overwrite=\\'True\\' )  ```  **Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\\'s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.  **Zilliz**  - Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.  ```python import pymilvus from llama_index.vector_stores import MilvusVectorStore\", \"construct vector store vector_store = MilvusVectorStore(     host=\\'foo.vectordb.zillizcloud.com\\',     port=403,     user=\\\\\"db_admin\\\\\",     password=\\\\\"foo\\\\\",     use_secure=True,     overwrite=\\'True\\' ) ```  **Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\\'s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.  **MyScale**  ```python import clickhouse_connect from llama_index.vector_stores import MyScaleVectorStore\", \"Creating a MyScale client client = clickhouse_connect.get_client(     host=\\'YOUR_CLUSTER_HOST\\',     port=8443,     username=\\'YOUR_USERNAME\\',     password=\\'YOUR_CLUSTER_PASSWORD\\' )\", \"construct vector store vector_store = MyScaleVectorStore(     myscale_client=client ) ```  **DocArray**  ```python from llama_index.vector_stores import (     DocArrayHnswVectorStore,      DocArrayInMemoryVectorStore, )\", \"construct vector store vector_store = DocArrayHnswVectorStore(work_dir=\\'hnsw_index\\')\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"construct vector store vector_store = QdrantVectorStore(     client=client,     collection_name=collection_name, ) ```  **Cassandra** (covering DataStax Astra DB as well, which is built on Cassandra)  ```python from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider from llama_index.vector_stores import CassandraVectorStore\", \"for a Cassandra cluster: cluster = Cluster([\\\\\"127.0.0.1\\\\\"])\", \"for an Astra DB cloud instance: cluster = Cluster(   cloud={\\\\\"secure_connect_bundle\\\\\": \\\\\"/home/USER/secure-bundle.zip\\\\\"},   auth_provider=PlainTextAuthProvider(\\\\\"token\\\\\", \\\\\"AstraCS:...\\\\\") ) # session = cluster.connect() keyspace = \\\\\"my_cassandra_keyspace\\\\\"  vector_store = CassandraVectorStore(     session=session,     keyspace=keyspace,     table=\\\\\"llamaindex_vector_test_1\\\\\",     embedding_dimension=1536,     #insertion_batch_size=50,  # optional ) ```  **Chroma**  ```python import chromadb from llama_index.vector_stores import ChromaVectorStore\", \"Creating a Chroma client chroma_client = chromadb.EphemeralClient() chroma_collection = chroma_client.create_collection(\\\\\"quickstart\\\\\")\", \"construct vector store vector_store = ChromaVectorStore(     chroma_collection=chroma_collection, ) ```  **Milvus**  - Milvus Index offers the ability to store both Documents and their embeddings. Documents are limited to the predefined Document attributes and does not include metadata.  ```python import pymilvus from llama_index.vector_stores import MilvusVectorStore\", \"construct vector store vector_store = MilvusVectorStore(     host=\\'localhost\\',     port=19530,     overwrite=\\'True\\' )  ```  **Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\\'s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.  **Zilliz**  - Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.  ```python import pymilvus from llama_index.vector_stores import MilvusVectorStore\", \"construct vector store vector_store = MilvusVectorStore(     host=\\'foo.vectordb.zillizcloud.com\\',     port=403,     user=\\\\\"db_admin\\\\\",     password=\\\\\"foo\\\\\",     use_secure=True,     overwrite=\\'True\\' ) ```  **Note**: `MilvusVectorStore` depends on the `pymilvus` library. Use `pip install pymilvus` if not already installed. If you get stuck at building wheel for `grpcio`, check if you are using python 3.11 (there\\'s a known issue: https://github.com/milvus-io/pymilvus/issues/1308) and try downgrading.  **MyScale**  ```python import clickhouse_connect from llama_index.vector_stores import MyScaleVectorStore\", \"Creating a MyScale client client = clickhouse_connect.get_client(     host=\\'YOUR_CLUSTER_HOST\\',     port=8443,     username=\\'YOUR_USERNAME\\',     password=\\'YOUR_CLUSTER_PASSWORD\\' )\", \"construct vector store vector_store = MyScaleVectorStore(     myscale_client=client ) ```  **DocArray**  ```python from llama_index.vector_stores import (     DocArrayHnswVectorStore,      DocArrayInMemoryVectorStore, )\", \"construct vector store vector_store = DocArrayHnswVectorStore(work_dir=\\'hnsw_index\\')\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=157 request_id=1f1d51496508a18cf9eb02625a4af0f2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=157 request_id=1f1d51496508a18cf9eb02625a4af0f2 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"alternatively, construct the in-memory vector store vector_store = DocArrayInMemoryVectorStore() ```  **MongoDBAtlas** ```python\", \"Provide URI to constructor, or use environment variable import pymongo from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch from llama_index.indices.vector_store.base import VectorStoreIndex from llama_index.storage.storage_context import StorageContext from llama_index.readers.file.base import SimpleDirectoryReader\", \"mongo_uri = os.environ[\\\\\"MONGO_URI\\\\\"] mongo_uri = \\\\\"mongodb+srv://:@?retryWrites=true&w=majority\\\\\" mongodb_client = pymongo.MongoClient(mongo_uri)\", \"construct store store = MongoDBAtlasVectorSearch(mongodb_client) storage_context = StorageContext.from_defaults(vector_store=store) uber_docs = SimpleDirectoryReader(input_files=[\\\\\"../data/10k/uber_2021.pdf\\\\\"]).load_data()\", \"construct index index = VectorStoreIndex.from_documents(uber_docs, storage_context=storage_context) ```  Example notebooks can be found here.\", \"Loading Data from Vector Stores using Data Connector  LlamaIndex supports loading data from the following sources. See Data Connectors for more details and API documentation.  Chroma stores both documents and vectors. This is an example of how to use Chroma:  ```python  from llama_index.readers.chroma import ChromaReader from llama_index.indices import ListIndex\", \"The chroma reader loads data from a persisted Chroma collection. reader = ChromaReader(     collection_name=\\\\\"chroma_collection\\\\\",     persist_directory=\\\\\"examples/data_connectors/chroma_collection\\\\\" )  query_vector=[n1, n2, n3, ...]  documents = reader.load_data(collection_name=\\\\\"demo\\\\\", query_vector=query_vector, limit=5) index = ListIndex.from_documents(documents)  query_engine = index.as_query_engine() response = query_engine.query(\\\\\"\\\\\") display(Markdown(f\\\\\"{response}\\\\\")) ```  Qdrant also stores both documents and vectors. This is an example of how to use Qdrant:  ```python  from llama_index.readers.qdrant import QdrantReader  reader = QdrantReader(host=\\\\\"localhost\\\\\")\", \"the query_vector is an embedding representation of your query_vector  query_vector = [n1, n2, n3, ...]\", \"NOTE: Required args are collection_name, query_vector.  documents = reader.load_data(collection_name=\\\\\"demo\\\\\", query_vector=query_vector, limit=5)  ```  NOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.  ```python\", \"option 1: specify class_name and properties\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"alternatively, construct the in-memory vector store vector_store = DocArrayInMemoryVectorStore() ```  **MongoDBAtlas** ```python\", \"Provide URI to constructor, or use environment variable import pymongo from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch from llama_index.indices.vector_store.base import VectorStoreIndex from llama_index.storage.storage_context import StorageContext from llama_index.readers.file.base import SimpleDirectoryReader\", \"mongo_uri = os.environ[\\\\\"MONGO_URI\\\\\"] mongo_uri = \\\\\"mongodb+srv://:@?retryWrites=true&w=majority\\\\\" mongodb_client = pymongo.MongoClient(mongo_uri)\", \"construct store store = MongoDBAtlasVectorSearch(mongodb_client) storage_context = StorageContext.from_defaults(vector_store=store) uber_docs = SimpleDirectoryReader(input_files=[\\\\\"../data/10k/uber_2021.pdf\\\\\"]).load_data()\", \"construct index index = VectorStoreIndex.from_documents(uber_docs, storage_context=storage_context) ```  Example notebooks can be found here.\", \"Loading Data from Vector Stores using Data Connector  LlamaIndex supports loading data from the following sources. See Data Connectors for more details and API documentation.  Chroma stores both documents and vectors. This is an example of how to use Chroma:  ```python  from llama_index.readers.chroma import ChromaReader from llama_index.indices import ListIndex\", \"The chroma reader loads data from a persisted Chroma collection. reader = ChromaReader(     collection_name=\\\\\"chroma_collection\\\\\",     persist_directory=\\\\\"examples/data_connectors/chroma_collection\\\\\" )  query_vector=[n1, n2, n3, ...]  documents = reader.load_data(collection_name=\\\\\"demo\\\\\", query_vector=query_vector, limit=5) index = ListIndex.from_documents(documents)  query_engine = index.as_query_engine() response = query_engine.query(\\\\\"\\\\\") display(Markdown(f\\\\\"{response}\\\\\")) ```  Qdrant also stores both documents and vectors. This is an example of how to use Qdrant:  ```python  from llama_index.readers.qdrant import QdrantReader  reader = QdrantReader(host=\\\\\"localhost\\\\\")\", \"the query_vector is an embedding representation of your query_vector  query_vector = [n1, n2, n3, ...]\", \"NOTE: Required args are collection_name, query_vector.  documents = reader.load_data(collection_name=\\\\\"demo\\\\\", query_vector=query_vector, limit=5)  ```  NOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.  ```python\", \"option 1: specify class_name and properties\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=53 request_id=51fde9a1a96a24de98d8baaf79edd447 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=53 request_id=51fde9a1a96a24de98d8baaf79edd447 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"1) load data using class_name and properties documents = reader.load_data(     class_name=\\\\\"\\\\\",     properties=[\\\\\"property1\\\\\", \\\\\"property2\\\\\", \\\\\"...\\\\\"],     separate_documents=True )\", \"2) example GraphQL query query = \\\\\"\\\\\"\\\\\" {     Get {          {                                   }     } } \\\\\"\\\\\"\\\\\"  documents = reader.load_data(graphql_query=query, separate_documents=True) ```  NOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.  For instance, this is an example usage of the Pinecone data loader `PineconeReader`:  ```python  from llama_index.readers.pinecone import PineconeReader  reader = PineconeReader(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\")  id_to_text_map = {     \\\\\"id1\\\\\": \\\\\"text blob 1\\\\\",     \\\\\"id2\\\\\": \\\\\"text blob 2\\\\\", }  query_vector=[n1, n2, n3, ..]  documents = reader.load_data(     index_name=\\\\\"quickstart\\\\\", id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True )  ```  Example notebooks can be found here.   ```{toctree} --- caption: Examples maxdepth: 1 --- ../../examples/vector_stores/SimpleIndexDemo.ipynb ../../examples/vector_stores/SimpleIndexDemoMMR.ipynb ../../examples/vector_stores/RedisIndexDemo.ipynb ../../examples/vector_stores/QdrantIndexDemo.ipynb ../../examples/vector_stores/FaissIndexDemo.ipynb ../../examples/vector_stores/DeepLakeIndexDemo.ipynb ../../examples/vector_stores/MyScaleIndexDemo.ipynb ../../examples/vector_stores/MetalIndexDemo.ipynb ../../examples/vector_stores/WeaviateIndexDemo.ipynb ../../examples/vector_stores/ZepIndexDemo.ipynb ../../examples/vector_stores/OpensearchDemo.ipynb ../../examples/vector_stores/PineconeIndexDemo.ipynb ../../examples/vector_stores/CassandraIndexDemo.ipynb ../../examples/vector_stores/ChromaIndexDemo.ipynb ../../examples/vector_stores/LanceDBIndexDemo.ipynb ../../examples/vector_stores/MilvusIndexDemo.ipynb ../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb ../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb ../../examples/vector_stores/AsyncIndexCreationDemo.ipynb ../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb ../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb ../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb ../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb ../../examples/vector_stores/postgres.ipynb ../../examples/vector_stores/AwadbDemo.ipynb ```\", \"Integrations  LlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\", \"Data Loaders  The full set of data loaders are found on LlamaHub\", \"Agent Tools The full set of agent tools are found on LlamaHub\", \"LLMs The full set of supported LLMs are found here.\", \"Observability/Tracing  Check out our one-click observability page for full tracing integrations.  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/one_click_observability.md integrations/graphsignal.md integrations/trulens.md  ```\", \"Structured Outputs ```{toctree} --- maxdepth: 1 --- integrations/guidance.md Guardrails  OpenAI Function Calling  ```\", \"Storage ```{toctree} --- maxdepth: 1 --- integrations/vector_stores.md integrations/graph_stores.md ```\", \"Application Frameworks ```{toctree} --- maxdepth: 1 --- integrations/using_with_langchain.md Streamlit  Chainlit  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"1) load data using class_name and properties documents = reader.load_data(     class_name=\\\\\"\\\\\",     properties=[\\\\\"property1\\\\\", \\\\\"property2\\\\\", \\\\\"...\\\\\"],     separate_documents=True )\", \"2) example GraphQL query query = \\\\\"\\\\\"\\\\\" {     Get {          {                                   }     } } \\\\\"\\\\\"\\\\\"  documents = reader.load_data(graphql_query=query, separate_documents=True) ```  NOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.  For instance, this is an example usage of the Pinecone data loader `PineconeReader`:  ```python  from llama_index.readers.pinecone import PineconeReader  reader = PineconeReader(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\")  id_to_text_map = {     \\\\\"id1\\\\\": \\\\\"text blob 1\\\\\",     \\\\\"id2\\\\\": \\\\\"text blob 2\\\\\", }  query_vector=[n1, n2, n3, ..]  documents = reader.load_data(     index_name=\\\\\"quickstart\\\\\", id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True )  ```  Example notebooks can be found here.   ```{toctree} --- caption: Examples maxdepth: 1 --- ../../examples/vector_stores/SimpleIndexDemo.ipynb ../../examples/vector_stores/SimpleIndexDemoMMR.ipynb ../../examples/vector_stores/RedisIndexDemo.ipynb ../../examples/vector_stores/QdrantIndexDemo.ipynb ../../examples/vector_stores/FaissIndexDemo.ipynb ../../examples/vector_stores/DeepLakeIndexDemo.ipynb ../../examples/vector_stores/MyScaleIndexDemo.ipynb ../../examples/vector_stores/MetalIndexDemo.ipynb ../../examples/vector_stores/WeaviateIndexDemo.ipynb ../../examples/vector_stores/ZepIndexDemo.ipynb ../../examples/vector_stores/OpensearchDemo.ipynb ../../examples/vector_stores/PineconeIndexDemo.ipynb ../../examples/vector_stores/CassandraIndexDemo.ipynb ../../examples/vector_stores/ChromaIndexDemo.ipynb ../../examples/vector_stores/LanceDBIndexDemo.ipynb ../../examples/vector_stores/MilvusIndexDemo.ipynb ../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb ../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb ../../examples/vector_stores/AsyncIndexCreationDemo.ipynb ../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb ../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb ../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb ../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb ../../examples/vector_stores/postgres.ipynb ../../examples/vector_stores/AwadbDemo.ipynb ```\", \"Integrations  LlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\", \"Data Loaders  The full set of data loaders are found on LlamaHub\", \"Agent Tools The full set of agent tools are found on LlamaHub\", \"LLMs The full set of supported LLMs are found here.\", \"Observability/Tracing  Check out our one-click observability page for full tracing integrations.  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/one_click_observability.md integrations/graphsignal.md integrations/trulens.md  ```\", \"Structured Outputs ```{toctree} --- maxdepth: 1 --- integrations/guidance.md Guardrails  OpenAI Function Calling  ```\", \"Storage ```{toctree} --- maxdepth: 1 --- integrations/vector_stores.md integrations/graph_stores.md ```\", \"Application Frameworks ```{toctree} --- maxdepth: 1 --- integrations/using_with_langchain.md Streamlit  Chainlit  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=52 request_id=f33169fcba3ca262e24a8e4d3f76ce0e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=52 request_id=f33169fcba3ca262e24a8e4d3f76ce0e response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Distributed Compute ```{toctree} --- maxdepth: 1 --- LlamaIndex + Ray   ```\", \"Other ```{toctree} --- maxdepth: 1 --- integrations/chatgpt_plugins.md Poe  Airbyte   ```\", \"Module Guides  These guide provide an overview of how to use our agent classes.  For more detailed guides on how to use specific tools, check out our tools module guides.\", \"OpenAI Agent ```{toctree} --- maxdepth: 1 --- /examples/agent/openai_agent.ipynb /examples/agent/openai_agent_with_query_engine.ipynb /examples/agent/openai_agent_retrieval.ipynb /examples/agent/openai_agent_query_cookbook.ipynb /examples/agent/openai_agent_query_plan.ipynb /examples/agent/openai_agent_context_retrieval.ipynb /examples/query_engine/recursive_retriever_agents.ipynb ```\", \"ReAct Agent ```{toctree} --- maxdepth: 1 --- /examples/agent/react_agent_with_query_engine.ipynb ```\", \"Data Agents\", \"Concept Data Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \\\\u201cread\\\\u201d and \\\\u201cwrite\\\\u201d function. They are capable of the following:  - Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured. - Calling any external service API in a structured fashion, and processing the response + storing it for later.  In that sense, agents are a step beyond our query engines in that they can not only \\\\\"read\\\\\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.  Building a data agent requires the following core components:  - A reasoning loop - Tool abstractions  A data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\", \"Reasoning Loop The reasoning loop depends on the type of agent. We have support for the following agents:  - OpenAI Function agent (built on top of the OpenAI Function API) - a ReAct agent (which works across any chat/text completion endpoint).\", \"Tool Abstractions  You can learn more about our Tool abstractions in our Tools section.\", \"Blog Post  For full details, please check out our detailed blog post.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Distributed Compute ```{toctree} --- maxdepth: 1 --- LlamaIndex + Ray   ```\", \"Other ```{toctree} --- maxdepth: 1 --- integrations/chatgpt_plugins.md Poe  Airbyte   ```\", \"Module Guides  These guide provide an overview of how to use our agent classes.  For more detailed guides on how to use specific tools, check out our tools module guides.\", \"OpenAI Agent ```{toctree} --- maxdepth: 1 --- /examples/agent/openai_agent.ipynb /examples/agent/openai_agent_with_query_engine.ipynb /examples/agent/openai_agent_retrieval.ipynb /examples/agent/openai_agent_query_cookbook.ipynb /examples/agent/openai_agent_query_plan.ipynb /examples/agent/openai_agent_context_retrieval.ipynb /examples/query_engine/recursive_retriever_agents.ipynb ```\", \"ReAct Agent ```{toctree} --- maxdepth: 1 --- /examples/agent/react_agent_with_query_engine.ipynb ```\", \"Data Agents\", \"Concept Data Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \\\\u201cread\\\\u201d and \\\\u201cwrite\\\\u201d function. They are capable of the following:  - Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured. - Calling any external service API in a structured fashion, and processing the response + storing it for later.  In that sense, agents are a step beyond our query engines in that they can not only \\\\\"read\\\\\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.  Building a data agent requires the following core components:  - A reasoning loop - Tool abstractions  A data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\", \"Reasoning Loop The reasoning loop depends on the type of agent. We have support for the following agents:  - OpenAI Function agent (built on top of the OpenAI Function API) - a ReAct agent (which works across any chat/text completion endpoint).\", \"Tool Abstractions  You can learn more about our Tool abstractions in our Tools section.\", \"Blog Post  For full details, please check out our detailed blog post.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=202 request_id=fc8ccfcf72828e3c88c55ee0fff2ed9c response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=202 request_id=fc8ccfcf72828e3c88c55ee0fff2ed9c response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern  Data agents can be used in the following manner (the example uses the OpenAI Function API) ```python from llama_index.agent import OpenAIAgent from llama_index.llms import OpenAI\", \"import and define tools ...\", \"initialize llm llm = OpenAI(model=\\\\\"gpt-3.5-turbo-0613\\\\\")\", \"initialize openai agent agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True) ```  See our usage pattern guide for more details. ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  Learn more about our different agent types in our module guides below.  Also take a look at our tools section!  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started  An agent is initialized from a set of Tools. Here\\'s an example of instantiating a ReAct agent from a set of Tools.  ```python from llama_index.tools import FunctionTool from llama_index.llms import OpenAI from llama_index.agent import ReActAgent\", \"define sample Tool def multiply(a: int, b: int) -> int:     \\\\\"\\\\\"\\\\\"Multiple two integers and returns the result integer\\\\\"\\\\\"\\\\\"     return a * b  multiply_tool = FunctionTool.from_defaults(fn=multiply)\", \"initialize llm llm = OpenAI(model=\\\\\"gpt-3.5-turbo-0613\\\\\")\", \"initialize ReAct agent agent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True) ```  An agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.  Example usage: ```python agent.chat(\\\\\"What is 2123 * 215123\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern  Data agents can be used in the following manner (the example uses the OpenAI Function API) ```python from llama_index.agent import OpenAIAgent from llama_index.llms import OpenAI\", \"import and define tools ...\", \"initialize llm llm = OpenAI(model=\\\\\"gpt-3.5-turbo-0613\\\\\")\", \"initialize openai agent agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True) ```  See our usage pattern guide for more details. ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  Learn more about our different agent types in our module guides below.  Also take a look at our tools section!  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started  An agent is initialized from a set of Tools. Here\\'s an example of instantiating a ReAct agent from a set of Tools.  ```python from llama_index.tools import FunctionTool from llama_index.llms import OpenAI from llama_index.agent import ReActAgent\", \"define sample Tool def multiply(a: int, b: int) -> int:     \\\\\"\\\\\"\\\\\"Multiple two integers and returns the result integer\\\\\"\\\\\"\\\\\"     return a * b  multiply_tool = FunctionTool.from_defaults(fn=multiply)\", \"initialize llm llm = OpenAI(model=\\\\\"gpt-3.5-turbo-0613\\\\\")\", \"initialize ReAct agent agent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True) ```  An agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.  Example usage: ```python agent.chat(\\\\\"What is 2123 * 215123\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=160 request_id=7f4a585ecd2d4ad8e58d2f5f4cacf111 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=160 request_id=7f4a585ecd2d4ad8e58d2f5f4cacf111 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Query Engine Tools  It is easy to wrap query engines as tools for an agent as well. Simply do the following:  ```python  from llama_index.agent import ReActAgent from llama_index.tools import QueryEngineTool\", \"NOTE: lyft_index and uber_index are both SimpleVectorIndex instances lyft_engine = lyft_index.as_query_engine(similarity_top_k=3) uber_engine = uber_index.as_query_engine(similarity_top_k=3)  query_engine_tools = [     QueryEngineTool(         query_engine=lyft_engine,         metadata=ToolMetadata(             name=\\\\\"lyft_10k\\\\\",             description=\\\\\"Provides information about Lyft financials for year 2021. \\\\\"             \\\\\"Use a detailed plain text question as input to the tool.\\\\\",         ),     ),     QueryEngineTool(         query_engine=uber_engine,         metadata=ToolMetadata(             name=\\\\\"uber_10k\\\\\",             description=\\\\\"Provides information about Uber financials for year 2021. \\\\\"             \\\\\"Use a detailed plain text question as input to the tool.\\\\\",         ),     ), ]\", \"initialize ReAct agent agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)  ```\", \"Use other agents as Tools  A nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools through our `QueryEngineTool`.   ```python from llama_index.tools import QueryEngineTool  query_engine_tools = [     QueryEngineTool(         query_engine=sql_agent,         metadata=ToolMetadata(             name=\\\\\"sql_agent\\\\\",             description=\\\\\"Agent that can execute SQL queries.\\\\\"         ),     ),     QueryEngineTool(         query_engine=gmail_agent,         metadata=ToolMetadata(             name=\\\\\"gmail_agent\\\\\",             description=\\\\\"Tool that can send emails on Gmail.\\\\\"         ),     ), ]  outer_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True) ```\", \"Advanced Concepts (for `OpenAIAgent`, in beta)  You can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and being able to perform query planning over an existing set of Tools.  These are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support for our more general `ReActAgent` is something we\\'re actively investigating.  NOTE: these are largely still in beta. The abstractions may change and become more general over time.\", \"Function Retrieval Agents  If the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.  We first build an `ObjectIndex` over an existing set of Tools.  ```python\", \"define an \\\\\"object\\\\\" index over these tools from llama_index import VectorStoreIndex from llama_index.objects import ObjectIndex, SimpleToolNodeMapping  tool_mapping = SimpleToolNodeMapping.from_objects(all_tools) obj_index = ObjectIndex.from_objects(     all_tools,     tool_mapping,     VectorStoreIndex, ) ```  We then define our `FnRetrieverOpenAIAgent`:  ```python from llama_index.agent import FnRetrieverOpenAIAgent  agent = FnRetrieverOpenAIAgent.from_retriever(obj_index.as_retriever(), verbose=True) ```\", \"Context Retrieval Agents  Our context-augmented OpenAI Agent will always perform retrieval before calling any tools.  This helps to provide additional context that can help the agent better pick Tools, versus just trying to make a decision without any context.  ```python from llama_index.schema import Document from llama_index.agent import ContextRetrieverOpenAIAgent\", \"toy index - stores a list of abbreviations texts = [     \\\\\"Abbrevation: X = Revenue\\\\\",     \\\\\"Abbrevation: YZ = Risk Factors\\\\\",     \\\\\"Abbreviation: Z = Costs\\\\\", ] docs = [Document(text=t) for t in texts] context_index = VectorStoreIndex.from_documents(docs)\", \"add context agent context_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(     query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True ) response = context_agent.chat(\\\\\"What is the YZ of March 2022?\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Query Engine Tools  It is easy to wrap query engines as tools for an agent as well. Simply do the following:  ```python  from llama_index.agent import ReActAgent from llama_index.tools import QueryEngineTool\", \"NOTE: lyft_index and uber_index are both SimpleVectorIndex instances lyft_engine = lyft_index.as_query_engine(similarity_top_k=3) uber_engine = uber_index.as_query_engine(similarity_top_k=3)  query_engine_tools = [     QueryEngineTool(         query_engine=lyft_engine,         metadata=ToolMetadata(             name=\\\\\"lyft_10k\\\\\",             description=\\\\\"Provides information about Lyft financials for year 2021. \\\\\"             \\\\\"Use a detailed plain text question as input to the tool.\\\\\",         ),     ),     QueryEngineTool(         query_engine=uber_engine,         metadata=ToolMetadata(             name=\\\\\"uber_10k\\\\\",             description=\\\\\"Provides information about Uber financials for year 2021. \\\\\"             \\\\\"Use a detailed plain text question as input to the tool.\\\\\",         ),     ), ]\", \"initialize ReAct agent agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)  ```\", \"Use other agents as Tools  A nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools through our `QueryEngineTool`.   ```python from llama_index.tools import QueryEngineTool  query_engine_tools = [     QueryEngineTool(         query_engine=sql_agent,         metadata=ToolMetadata(             name=\\\\\"sql_agent\\\\\",             description=\\\\\"Agent that can execute SQL queries.\\\\\"         ),     ),     QueryEngineTool(         query_engine=gmail_agent,         metadata=ToolMetadata(             name=\\\\\"gmail_agent\\\\\",             description=\\\\\"Tool that can send emails on Gmail.\\\\\"         ),     ), ]  outer_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True) ```\", \"Advanced Concepts (for `OpenAIAgent`, in beta)  You can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and being able to perform query planning over an existing set of Tools.  These are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support for our more general `ReActAgent` is something we\\'re actively investigating.  NOTE: these are largely still in beta. The abstractions may change and become more general over time.\", \"Function Retrieval Agents  If the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.  We first build an `ObjectIndex` over an existing set of Tools.  ```python\", \"define an \\\\\"object\\\\\" index over these tools from llama_index import VectorStoreIndex from llama_index.objects import ObjectIndex, SimpleToolNodeMapping  tool_mapping = SimpleToolNodeMapping.from_objects(all_tools) obj_index = ObjectIndex.from_objects(     all_tools,     tool_mapping,     VectorStoreIndex, ) ```  We then define our `FnRetrieverOpenAIAgent`:  ```python from llama_index.agent import FnRetrieverOpenAIAgent  agent = FnRetrieverOpenAIAgent.from_retriever(obj_index.as_retriever(), verbose=True) ```\", \"Context Retrieval Agents  Our context-augmented OpenAI Agent will always perform retrieval before calling any tools.  This helps to provide additional context that can help the agent better pick Tools, versus just trying to make a decision without any context.  ```python from llama_index.schema import Document from llama_index.agent import ContextRetrieverOpenAIAgent\", \"toy index - stores a list of abbreviations texts = [     \\\\\"Abbrevation: X = Revenue\\\\\",     \\\\\"Abbrevation: YZ = Risk Factors\\\\\",     \\\\\"Abbreviation: Z = Costs\\\\\", ] docs = [Document(text=t) for t in texts] context_index = VectorStoreIndex.from_documents(docs)\", \"add context agent context_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(     query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True ) response = context_agent.chat(\\\\\"What is the YZ of March 2022?\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=129 request_id=8ce6f61cbf6e416042510f5e598a3516 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=129 request_id=8ce6f61cbf6e416042510f5e598a3516 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Query Planning  OpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent with a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query plan over a set of subtools.  ```python\", \"define query plan tool from llama_index.tools import QueryPlanTool from llama_index import get_response_synthesizer  response_synthesizer = get_response_synthesizer(service_context=service_context) query_plan_tool = QueryPlanTool.from_defaults(     query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],     response_synthesizer=response_synthesizer, )\", \"initialize agent agent = OpenAIAgent.from_tools(     [query_plan_tool],     max_function_calls=10,     llm=OpenAI(temperature=0, model=\\\\\"gpt-4-0613\\\\\"),     verbose=True, )\", \"should output a query plan to call march, june, and september tools response = agent.query(\\\\\"Analyze Uber revenue growth in March, June, and September\\\\\")  ```\", \"LlamaHub Tools Guide  We offer a rich set of Tool Specs that are offered through LlamaHub \\\\ud83e\\\\udd99.  !  These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.   We also provide a list of **utility tools** that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\", \"Tool Specs  Coming soon!\", \"Utility Tools  Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).   To tackle this, we\\\\u2019ve provided an initial set of \\\\u201cutility tools\\\\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\\\\u2019s returned from any API request.  Let\\\\u2019s walk through our two main utility tools below.\", \"OnDemandLoaderTool  This tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \\\\u201con-demand\\\\u201d. All three of these steps happen in a single tool call.  Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.   A usage example is given below:  ```python from llama_hub.wikipedia.base import WikipediaReader from llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool  tool = OnDemandLoaderTool.from_defaults( \\\\treader, \\\\tname=\\\\\"Wikipedia Tool\\\\\", \\\\tdescription=\\\\\"A tool for loading data and querying articles from Wikipedia\\\\\" ) ```\", \"LoadAndSearchToolSpec  The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.  The `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.  This is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.  Example usage is shown below:  ```python from llama_hub.tools.wikipedia.base import WikipediaToolSpec from llama_index.tools.tool_spec.load_and_search import LoadAndSearchToolSpec  wiki_spec = WikipediaToolSpec()\", \"Get the search wikipedia tool tool = wiki_spec.to_tool_list()[1]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Query Planning  OpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent with a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query plan over a set of subtools.  ```python\", \"define query plan tool from llama_index.tools import QueryPlanTool from llama_index import get_response_synthesizer  response_synthesizer = get_response_synthesizer(service_context=service_context) query_plan_tool = QueryPlanTool.from_defaults(     query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],     response_synthesizer=response_synthesizer, )\", \"initialize agent agent = OpenAIAgent.from_tools(     [query_plan_tool],     max_function_calls=10,     llm=OpenAI(temperature=0, model=\\\\\"gpt-4-0613\\\\\"),     verbose=True, )\", \"should output a query plan to call march, june, and september tools response = agent.query(\\\\\"Analyze Uber revenue growth in March, June, and September\\\\\")  ```\", \"LlamaHub Tools Guide  We offer a rich set of Tool Specs that are offered through LlamaHub \\\\ud83e\\\\udd99.  !  These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.   We also provide a list of **utility tools** that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\", \"Tool Specs  Coming soon!\", \"Utility Tools  Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).   To tackle this, we\\\\u2019ve provided an initial set of \\\\u201cutility tools\\\\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\\\\u2019s returned from any API request.  Let\\\\u2019s walk through our two main utility tools below.\", \"OnDemandLoaderTool  This tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \\\\u201con-demand\\\\u201d. All three of these steps happen in a single tool call.  Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.   A usage example is given below:  ```python from llama_hub.wikipedia.base import WikipediaReader from llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool  tool = OnDemandLoaderTool.from_defaults( \\\\treader, \\\\tname=\\\\\"Wikipedia Tool\\\\\", \\\\tdescription=\\\\\"A tool for loading data and querying articles from Wikipedia\\\\\" ) ```\", \"LoadAndSearchToolSpec  The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.  The `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.  This is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.  Example usage is shown below:  ```python from llama_hub.tools.wikipedia.base import WikipediaToolSpec from llama_index.tools.tool_spec.load_and_search import LoadAndSearchToolSpec  wiki_spec = WikipediaToolSpec()\", \"Get the search wikipedia tool tool = wiki_spec.to_tool_list()[1]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=105 request_id=23c9122ff2143444ab577c5f459a2d53 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=105 request_id=23c9122ff2143444ab577c5f459a2d53 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Create the Agent with load/search tools agent = OpenAIAgent.from_tools(  LoadAndSearchToolSpec.from_defaults(     tool  ).to_tool_list(), verbose=True ) ```\", \"Tools\", \"Concept  Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood.   A Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).  A Tool Spec defines a full API specification of any service that can be converted into a list of Tools.  We offer a few different types of Tools: - `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema. - `QueryEngineTool`: A tool that wraps an existing query engine. Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.  We offer a rich set of Tools and Tool Specs through LlamaHub \\\\ud83e\\\\udd99.\", \"Blog Post  For full details, please check out our detailed blog post.\", \"Usage Pattern  Our Tool Specs and Tools can be imported from the `llama-hub` package.  To use with our agent, ```python from llama_index.agent import OpenAIAgent from llama_hub.tools.gmail.base import GmailToolSpec  tool_spec = GmailToolSpec() agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)  ```  See our Usage Pattern Guide for more details. ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"LlamaHub Tools Guide \\\\ud83d\\\\udee0\\\\ufe0f  Check out our guide for a full overview of the Tools/Tool Specs in LlamaHub!  ```{toctree} --- maxdepth: 1 --- llamahub_tools_guide.md ```   <!-- We offer a rich set of Tool Specs that are offered through LlamaHub \\\\ud83e\\\\udd99.  These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.   ! -->   <!-- ## Module Guides ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Tool Example Notebooks  Coming soon!  -->\", \"Usage Pattern  You can create custom LlamaHub Tool Specs and Tools or they can be imported from the `llama-hub` package. They can be plugged into our native agents, or LangChain agents.\", \"Using with our Agents  To use with our OpenAIAgent, ```python from llama_index.agent import OpenAIAgent from llama_hub.tools.gmail.base import GmailToolSpec from llama_index.tools.function_tool import FunctionTool\", \"Use a tool spec from Llama-Hub tool_spec = GmailToolSpec()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Create the Agent with load/search tools agent = OpenAIAgent.from_tools(  LoadAndSearchToolSpec.from_defaults(     tool  ).to_tool_list(), verbose=True ) ```\", \"Tools\", \"Concept  Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood.   A Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).  A Tool Spec defines a full API specification of any service that can be converted into a list of Tools.  We offer a few different types of Tools: - `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema. - `QueryEngineTool`: A tool that wraps an existing query engine. Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.  We offer a rich set of Tools and Tool Specs through LlamaHub \\\\ud83e\\\\udd99.\", \"Blog Post  For full details, please check out our detailed blog post.\", \"Usage Pattern  Our Tool Specs and Tools can be imported from the `llama-hub` package.  To use with our agent, ```python from llama_index.agent import OpenAIAgent from llama_hub.tools.gmail.base import GmailToolSpec  tool_spec = GmailToolSpec() agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)  ```  See our Usage Pattern Guide for more details. ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"LlamaHub Tools Guide \\\\ud83d\\\\udee0\\\\ufe0f  Check out our guide for a full overview of the Tools/Tool Specs in LlamaHub!  ```{toctree} --- maxdepth: 1 --- llamahub_tools_guide.md ```   <!-- We offer a rich set of Tool Specs that are offered through LlamaHub \\\\ud83e\\\\udd99.  These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.   ! -->   <!-- ## Module Guides ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Tool Example Notebooks  Coming soon!  -->\", \"Usage Pattern  You can create custom LlamaHub Tool Specs and Tools or they can be imported from the `llama-hub` package. They can be plugged into our native agents, or LangChain agents.\", \"Using with our Agents  To use with our OpenAIAgent, ```python from llama_index.agent import OpenAIAgent from llama_hub.tools.gmail.base import GmailToolSpec from llama_index.tools.function_tool import FunctionTool\", \"Use a tool spec from Llama-Hub tool_spec = GmailToolSpec()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=60 request_id=e802b5cb359315ce84f259d18aa9230d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=60 request_id=e802b5cb359315ce84f259d18aa9230d response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Create a custom tool. Type annotations and docstring are used for the def add_numbers(x: int, y: int) -> int:     \\\\\"\\\\\"\\\\\"     Adds the two numbers together and returns the result.     \\\\\"\\\\\"\\\\\"     return x + y  function_tool = FunctionTool.from_defaults(fn=add_numbers)  tools = tool_spec.to_tool_list() + [function_tool] agent = OpenAIAgent.from_tools(tools, verbose=True)\", \"use agent agent.chat(\\\\\"Can you create a new email to helpdesk and support @example.com about a service outage\\\\\") ```  Full Tool details can be found on our LlamaHub page. Each tool contains a \\\\\"Usage\\\\\" section showing how that tool can be used.\", \"Using with LangChain To use with a LangChain agent, simply convert tools to LangChain tools with `to_langchain_tool()`.  ```python tools = tool_spec.to_tool_list() langchain_tools = [t.to_langchain_tool() for t in tools]\", \"plug into LangChain agent from langchain.agents import initialize_agent  agent_executor = initialize_agent(     langchain_tools, llm, agent=\\\\\"conversational-react-description\\\\\", memory=memory )  ```\", \"Module Guides   ```{toctree} --- maxdepth: 1 --- ../../../examples/data_connectors/simple_directory_reader.ipynb ../../../examples/data_connectors/PsychicDemo.ipynb ../../../examples/data_connectors/DeepLakeReader.ipynb ../../../examples/data_connectors/QdrantDemo.ipynb ../../../examples/data_connectors/DiscordDemo.ipynb ../../../examples/data_connectors/MongoDemo.ipynb ../../../examples/data_connectors/ChromaDemo.ipynb ../../../examples/data_connectors/MyScaleReaderDemo.ipynb ../../../examples/data_connectors/FaissDemo.ipynb ../../../examples/data_connectors/ObsidianReaderDemo.ipynb ../../../examples/data_connectors/SlackDemo.ipynb ../../../examples/data_connectors/WebPageDemo.ipynb ../../../examples/data_connectors/PineconeDemo.ipynb ../../../examples/data_connectors/MboxReaderDemo.ipynb ../../../examples/data_connectors/MilvusReaderDemo.ipynb ../../../examples/data_connectors/NotionDemo.ipynb ../../../examples/data_connectors/GithubRepositoryReaderDemo.ipynb ../../../examples/data_connectors/GoogleDocsDemo.ipynb ../../../examples/data_connectors/DatabaseReaderDemo.ipynb ../../../examples/data_connectors/TwitterDemo.ipynb ../../../examples/data_connectors/WeaviateDemo.ipynb ../../../examples/data_connectors/MakeDemo.ipynb ../../../examples/data_connectors/deplot/DeplotReader.ipynb ```\", \"Data Connectors (LlamaHub)\", \"Concept A data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).  ```{tip} Once you\\'ve ingested your data, you can build an Index on top, ask questions using a Query Engine, and have a conversation using a Chat Engine. ```\", \"LlamaHub Our data connectors are offered through LlamaHub \\\\ud83e\\\\udd99.  LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  !\", \"Usage Pattern Get started with: ```python from llama_index import download_loader  GoogleDocsReader = download_loader(\\'GoogleDocsReader\\') loader = GoogleDocsReader() documents = loader.load_data(document_ids=[...]) ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules  Some sample data connectors: - local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc. - Notion (`NotionPageReader`) - Google Docs (`GoogleDocsReader`) - Slack (`SlackReader`) - Discord (`DiscordReader`) - Apify Actors (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.  See below for detailed guides.  ```{toctree} --- maxdepth: 2 --- modules.rst ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Create a custom tool. Type annotations and docstring are used for the def add_numbers(x: int, y: int) -> int:     \\\\\"\\\\\"\\\\\"     Adds the two numbers together and returns the result.     \\\\\"\\\\\"\\\\\"     return x + y  function_tool = FunctionTool.from_defaults(fn=add_numbers)  tools = tool_spec.to_tool_list() + [function_tool] agent = OpenAIAgent.from_tools(tools, verbose=True)\", \"use agent agent.chat(\\\\\"Can you create a new email to helpdesk and support @example.com about a service outage\\\\\") ```  Full Tool details can be found on our LlamaHub page. Each tool contains a \\\\\"Usage\\\\\" section showing how that tool can be used.\", \"Using with LangChain To use with a LangChain agent, simply convert tools to LangChain tools with `to_langchain_tool()`.  ```python tools = tool_spec.to_tool_list() langchain_tools = [t.to_langchain_tool() for t in tools]\", \"plug into LangChain agent from langchain.agents import initialize_agent  agent_executor = initialize_agent(     langchain_tools, llm, agent=\\\\\"conversational-react-description\\\\\", memory=memory )  ```\", \"Module Guides   ```{toctree} --- maxdepth: 1 --- ../../../examples/data_connectors/simple_directory_reader.ipynb ../../../examples/data_connectors/PsychicDemo.ipynb ../../../examples/data_connectors/DeepLakeReader.ipynb ../../../examples/data_connectors/QdrantDemo.ipynb ../../../examples/data_connectors/DiscordDemo.ipynb ../../../examples/data_connectors/MongoDemo.ipynb ../../../examples/data_connectors/ChromaDemo.ipynb ../../../examples/data_connectors/MyScaleReaderDemo.ipynb ../../../examples/data_connectors/FaissDemo.ipynb ../../../examples/data_connectors/ObsidianReaderDemo.ipynb ../../../examples/data_connectors/SlackDemo.ipynb ../../../examples/data_connectors/WebPageDemo.ipynb ../../../examples/data_connectors/PineconeDemo.ipynb ../../../examples/data_connectors/MboxReaderDemo.ipynb ../../../examples/data_connectors/MilvusReaderDemo.ipynb ../../../examples/data_connectors/NotionDemo.ipynb ../../../examples/data_connectors/GithubRepositoryReaderDemo.ipynb ../../../examples/data_connectors/GoogleDocsDemo.ipynb ../../../examples/data_connectors/DatabaseReaderDemo.ipynb ../../../examples/data_connectors/TwitterDemo.ipynb ../../../examples/data_connectors/WeaviateDemo.ipynb ../../../examples/data_connectors/MakeDemo.ipynb ../../../examples/data_connectors/deplot/DeplotReader.ipynb ```\", \"Data Connectors (LlamaHub)\", \"Concept A data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).  ```{tip} Once you\\'ve ingested your data, you can build an Index on top, ask questions using a Query Engine, and have a conversation using a Chat Engine. ```\", \"LlamaHub Our data connectors are offered through LlamaHub \\\\ud83e\\\\udd99.  LlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.  !\", \"Usage Pattern Get started with: ```python from llama_index import download_loader  GoogleDocsReader = download_loader(\\'GoogleDocsReader\\') loader = GoogleDocsReader() documents = loader.load_data(document_ids=[...]) ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules  Some sample data connectors: - local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc. - Notion (`NotionPageReader`) - Google Docs (`GoogleDocsReader`) - Slack (`SlackReader`) - Discord (`DiscordReader`) - Apify Actors (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.  See below for detailed guides.  ```{toctree} --- maxdepth: 2 --- modules.rst ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=37 request_id=8bab276d981d999863aa33650b50dc28 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=37 request_id=8bab276d981d999863aa33650b50dc28 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern\", \"Get Started Each data loader contains a \\\\\"Usage\\\\\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.  Example usage:  ```python from llama_index import VectorStoreIndex, download_loader  GoogleDocsReader = download_loader(\\'GoogleDocsReader\\')  gdoc_ids = [\\'1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\\'] loader = GoogleDocsReader() documents = loader.load_data(document_ids=gdoc_ids) index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() query_engine.query(\\'Where did the author go to school?\\') ```\", \"Documents / Nodes\", \"Concept  Document and Node objects are core abstractions within LlamaIndex.  A **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below. - `metadata` - a dictionary of annotations that can be appended to the text. - `relationships` - a dictionary containing relationships to other Documents/Nodes.  *Note*: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.  A **Node** represents a \\\\\"chunk\\\\\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \\\\\"file_name\\\\\" filed in the Document is propagated to every Node).\", \"Usage Pattern  Here are some simple snippets to get started with Documents and Nodes.\", \"Documents  ```python from llama_index import Document, VectorStoreIndex  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list]\", \"build index index = VectorStoreIndex.from_documents(documents)  ```\", \"Nodes ```python  from llama_index.node_parser import SimpleNodeParser\", \"load documents ...\", \"parse nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern\", \"Get Started Each data loader contains a \\\\\"Usage\\\\\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which downloads the loader file into a module that you can use within your application.  Example usage:  ```python from llama_index import VectorStoreIndex, download_loader  GoogleDocsReader = download_loader(\\'GoogleDocsReader\\')  gdoc_ids = [\\'1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec\\'] loader = GoogleDocsReader() documents = loader.load_data(document_ids=gdoc_ids) index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() query_engine.query(\\'Where did the author go to school?\\') ```\", \"Documents / Nodes\", \"Concept  Document and Node objects are core abstractions within LlamaIndex.  A **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below. - `metadata` - a dictionary of annotations that can be appended to the text. - `relationships` - a dictionary containing relationships to other Documents/Nodes.  *Note*: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.  A **Node** represents a \\\\\"chunk\\\\\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \\\\\"file_name\\\\\" filed in the Document is propagated to every Node).\", \"Usage Pattern  Here are some simple snippets to get started with Documents and Nodes.\", \"Documents  ```python from llama_index import Document, VectorStoreIndex  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list]\", \"build index index = VectorStoreIndex.from_documents(documents)  ```\", \"Nodes ```python  from llama_index.node_parser import SimpleNodeParser\", \"load documents ...\", \"parse nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=df58a94c08250d847605879aea09784d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=df58a94c08250d847605879aea09784d response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"build index index = VectorStoreIndex(nodes)  ```\", \"Document/Node Usage  Take a look at our in-depth guides for more details on how to use Documents/Nodes.  ```{toctree} --- maxdepth: 1 --- usage_documents.md usage_nodes.md usage_metadata_extractor.md ```\", \"Defining and Customizing Documents\", \"Defining Documents  Documents can either be created automatically via data loaders, or constructed manually.  By default, all of our data loaders (including those offered on LlamaHub) return `Document` objects through the `load_data` function.  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'./data\\').load_data() ```  You can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.  ```python from llama_index import Document  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list] ```  To speed up prototyping and development, you can also quickly create a document using some default text:  ```python document = Document.example() ```\", \"Customizing Documents  This section covers various ways to customize `Document` objects. Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.\", \"Metadata  Documents also offer the chance to include useful metadata. Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. This information can be anything, such as filenames or categories. If you are intergrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).  Any information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document. Additionaly, this information is included in the nodes, enabling the index to utilize it on queries and responses. By default, the metadata is injected into the text for both embedding and LLM model calls.  There are a few ways to set up this dictionary:  1. In the document constructor:  ```python document = Document(     text=\\'text\\',      metadata={         \\'filename\\': \\'\\',          \\'category\\': \\'\\'     } ) ```  2. After the document is created:  ```python document.metadata = {\\'filename\\': \\'\\'} ```  3. Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook. This will automatically run the hook on each document to set the `metadata` field:  ```python from llama_index import SimpleDirectoryReader filename_fn = lambda filename: {\\'file_name\\': filename}\", \"automatically sets the metadata of each document according to filename_fn documents = SimpleDirectoryReader(\\'./data\\', file_metadata=filename_fn).load_data() ```\", \"Customizing the id  As detailed in the section Document Management, the doc `id_` is used to enable effecient refreshing of documents in the index. When using the `SimpleDirectoryReader`, you can automatically set the doc `id_` to be the full path to each document:  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\\\\"./data\\\\\", filename_as_id=True).load_data() print([x.doc_id for x in documents]) ```  You can also set the `id_` of any `Document` or `TextNode` directly!  ```python document.id_ = \\\\\"My new document id!\\\\\" ```\", \"Advanced - Metadata Customization  A key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.\", \"Customizing LLM Metadata Text  Typically, a document might have many metadata keys, but you might not want all of them visibile to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.   We can exclude it like so:  ```python document.excluded_llm_metadata_keys = [\\'file_name\\'] ```  Then, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:  ```python from llama_index.schema import MetadataMode print(document.get_content(metadata_mode=MetadataMode.LLM)) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"build index index = VectorStoreIndex(nodes)  ```\", \"Document/Node Usage  Take a look at our in-depth guides for more details on how to use Documents/Nodes.  ```{toctree} --- maxdepth: 1 --- usage_documents.md usage_nodes.md usage_metadata_extractor.md ```\", \"Defining and Customizing Documents\", \"Defining Documents  Documents can either be created automatically via data loaders, or constructed manually.  By default, all of our data loaders (including those offered on LlamaHub) return `Document` objects through the `load_data` function.  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'./data\\').load_data() ```  You can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.  ```python from llama_index import Document  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list] ```  To speed up prototyping and development, you can also quickly create a document using some default text:  ```python document = Document.example() ```\", \"Customizing Documents  This section covers various ways to customize `Document` objects. Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.\", \"Metadata  Documents also offer the chance to include useful metadata. Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. This information can be anything, such as filenames or categories. If you are intergrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).  Any information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document. Additionaly, this information is included in the nodes, enabling the index to utilize it on queries and responses. By default, the metadata is injected into the text for both embedding and LLM model calls.  There are a few ways to set up this dictionary:  1. In the document constructor:  ```python document = Document(     text=\\'text\\',      metadata={         \\'filename\\': \\'\\',          \\'category\\': \\'\\'     } ) ```  2. After the document is created:  ```python document.metadata = {\\'filename\\': \\'\\'} ```  3. Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook. This will automatically run the hook on each document to set the `metadata` field:  ```python from llama_index import SimpleDirectoryReader filename_fn = lambda filename: {\\'file_name\\': filename}\", \"automatically sets the metadata of each document according to filename_fn documents = SimpleDirectoryReader(\\'./data\\', file_metadata=filename_fn).load_data() ```\", \"Customizing the id  As detailed in the section Document Management, the doc `id_` is used to enable effecient refreshing of documents in the index. When using the `SimpleDirectoryReader`, you can automatically set the doc `id_` to be the full path to each document:  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\\\\"./data\\\\\", filename_as_id=True).load_data() print([x.doc_id for x in documents]) ```  You can also set the `id_` of any `Document` or `TextNode` directly!  ```python document.id_ = \\\\\"My new document id!\\\\\" ```\", \"Advanced - Metadata Customization  A key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.\", \"Customizing LLM Metadata Text  Typically, a document might have many metadata keys, but you might not want all of them visibile to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.   We can exclude it like so:  ```python document.excluded_llm_metadata_keys = [\\'file_name\\'] ```  Then, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:  ```python from llama_index.schema import MetadataMode print(document.get_content(metadata_mode=MetadataMode.LLM)) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=100 request_id=8ed456240d641c505b026de6075492fb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=100 request_id=8ed456240d641c505b026de6075492fb response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Customizing Embedding Metadata Text  Similar to customing the metadata visibile to the LLM, we can also customize the metadata visible to emebddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON\\'T want particular text to bias the embeddings.  ```python document.excluded_embed_metadata_keys = [\\'file_name\\'] ```  Then, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:  ```python from llama_index.schema import MetadataMode print(document.get_content(metadata_mode=MetadataMode.EMBED)) ```\", \"Customizing Metadata Format  As you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model. By default, the format of this metadata is controlled by three attributes:  1. `Document.metadata_seperator` -> default = `\\\\\"\\\\\\\\n\\\\\"`  When concatenating all key/value fields of your metadata, this field controls the seperator bewtween each key/value pair.  2. `Document.metadata_template` -> default = `\\\\\"{key}: {value}\\\\\"`  This attribute controls how each key/value pair in your metadata is formatted. The two variables `key` and `value` string keys are required.  3. `Document.text_template` -> default = `{metadata_str}\\\\\\\\n\\\\\\\\n{content}`  Once your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node. The `metadata` and `content` string keys are required.\", \"Summary  Knowing all this, let\\'s create a short example using all this power:  ```python from llama_index import Document from llama_index.schema import MetadataMode  document = Document(     text=\\\\\"This is a super-customized document\\\\\",     metadata={         \\\\\"file_name\\\\\": \\\\\"super_secret_document.txt\\\\\",         \\\\\"category\\\\\": \\\\\"finance\\\\\",         \\\\\"author\\\\\": \\\\\"LlamaIndex\\\\\"         },     excluded_llm_metadata_keys=[\\'file_name\\'],     metadata_seperator=\\\\\"::\\\\\",     metadata_template=\\\\\"{key}=>{value}\\\\\",     text_template=\\\\\"Metadata: {metadata_str}\\\\\\\\n-----\\\\\\\\nContent: {content}\\\\\", )  print(\\\\\"The LLM sees this: \\\\\\\\n\\\\\", document.get_content(metadata_mode=MetadataMode.LLM)) print(\\\\\"The Embedding model sees this: \\\\\\\\n\\\\\", document.get_content(metadata_mode=MetadataMode.EMBED)) ```\", \"Advanced - Automatic Metadata Extraction  We have initial examples of using LLMs themselves to perform metadata extraction.  Take a look here!   ```{toctree} --- maxdepth: 1 --- /examples/metadata_extraction/MetadataExtractionSEC.ipynb ```\", \"Automated Metadata Extraction for Nodes  You can use LLMs to automate metadata extraction with our `MetadataExtractor` modules.  Our metadata extractor modules include the following \\\\\"feature extractors\\\\\": - `SummaryExtractor` - automatically extracts a summary over a set of Nodes - `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer - `TitleExtractor` - extracts a title over the context of each Node - `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node  You can use these feature extractors within our overall `MetadataExtractor` class. Then you can plug in the `MetadataExtractor` into our node parser:  ```python from llama_index.node_parser.extractors import (     MetadataExtractor,     TitleExtractor,     QuestionsAnsweredExtractor ) from llama_index.text_splitter import TokenTextSplitter  text_splitter = TokenTextSplitter(separator=\\\\\" \\\\\", chunk_size=512, chunk_overlap=128) metadata_extractor = MetadataExtractor(     extractors=[         TitleExtractor(nodes=5),         QuestionsAnsweredExtractor(questions=3),     ], )  node_parser = SimpleNodeParser.from_defaults(     text_splitter=text_splitter,     metadata_extractor=metadata_extractor, )\", \"assume documents are defined -> extract nodes nodes = node_parser.get_nodes_from_documents(documents) ```   ```{toctree} --- caption: Metadata Extraction Guides maxdepth: 1 --- /examples/metadata_extraction/MetadataExtractionSEC.ipynb /examples/metadata_extraction/MetadataExtraction_LLMSurvey.ipynb /examples/metadata_extraction/EntityExtractionClimate.ipynb ```\", \"Defining and Customizing Nodes  Nodes represent \\\\\"chunks\\\\\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes.  For instance, you can do  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults()  nodes = parser.get_nodes_from_documents(documents) ```  You can also choose to construct Node objects manually and skip the first section. For instance,  ```python from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo  node1 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\") node2 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\")\", \"set relationships node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id) node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id) nodes = [node1, node2] ```  The `RelatedNodeInfo` class can also store additional `metadata` if needed:  ```python node2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\\\\\"key\\\\\": \\\\\"val\\\\\"}) ```\", \"Composability   LlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.  Composability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\", \"Defining Subindices To see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.  ```python from llama_index import SimpleDirectoryReader  doc1 = SimpleDirectoryReader(\\'data1\\').load_data() doc2 = SimpleDirectoryReader(\\'data2\\').load_data() doc3 = SimpleDirectoryReader(\\'data3\\').load_data() ```  !  Now let\\'s define a tree index for each document. In order to persist the graph later, each index should share the same storage context.  In Python, we have:  ```python from llama_index import TreeIndex  storage_context = storage_context.from_defaults()  index1 = TreeIndex.from_documents(doc1, storage_context=storage_context) index2 = TreeIndex.from_documents(doc2, storage_context=storage_context) index3 = TreeIndex.from_documents(doc3, storage_context=storage_context) ```  !\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Customizing Embedding Metadata Text  Similar to customing the metadata visibile to the LLM, we can also customize the metadata visible to emebddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON\\'T want particular text to bias the embeddings.  ```python document.excluded_embed_metadata_keys = [\\'file_name\\'] ```  Then, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:  ```python from llama_index.schema import MetadataMode print(document.get_content(metadata_mode=MetadataMode.EMBED)) ```\", \"Customizing Metadata Format  As you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model. By default, the format of this metadata is controlled by three attributes:  1. `Document.metadata_seperator` -> default = `\\\\\"\\\\\\\\n\\\\\"`  When concatenating all key/value fields of your metadata, this field controls the seperator bewtween each key/value pair.  2. `Document.metadata_template` -> default = `\\\\\"{key}: {value}\\\\\"`  This attribute controls how each key/value pair in your metadata is formatted. The two variables `key` and `value` string keys are required.  3. `Document.text_template` -> default = `{metadata_str}\\\\\\\\n\\\\\\\\n{content}`  Once your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node. The `metadata` and `content` string keys are required.\", \"Summary  Knowing all this, let\\'s create a short example using all this power:  ```python from llama_index import Document from llama_index.schema import MetadataMode  document = Document(     text=\\\\\"This is a super-customized document\\\\\",     metadata={         \\\\\"file_name\\\\\": \\\\\"super_secret_document.txt\\\\\",         \\\\\"category\\\\\": \\\\\"finance\\\\\",         \\\\\"author\\\\\": \\\\\"LlamaIndex\\\\\"         },     excluded_llm_metadata_keys=[\\'file_name\\'],     metadata_seperator=\\\\\"::\\\\\",     metadata_template=\\\\\"{key}=>{value}\\\\\",     text_template=\\\\\"Metadata: {metadata_str}\\\\\\\\n-----\\\\\\\\nContent: {content}\\\\\", )  print(\\\\\"The LLM sees this: \\\\\\\\n\\\\\", document.get_content(metadata_mode=MetadataMode.LLM)) print(\\\\\"The Embedding model sees this: \\\\\\\\n\\\\\", document.get_content(metadata_mode=MetadataMode.EMBED)) ```\", \"Advanced - Automatic Metadata Extraction  We have initial examples of using LLMs themselves to perform metadata extraction.  Take a look here!   ```{toctree} --- maxdepth: 1 --- /examples/metadata_extraction/MetadataExtractionSEC.ipynb ```\", \"Automated Metadata Extraction for Nodes  You can use LLMs to automate metadata extraction with our `MetadataExtractor` modules.  Our metadata extractor modules include the following \\\\\"feature extractors\\\\\": - `SummaryExtractor` - automatically extracts a summary over a set of Nodes - `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer - `TitleExtractor` - extracts a title over the context of each Node - `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node  You can use these feature extractors within our overall `MetadataExtractor` class. Then you can plug in the `MetadataExtractor` into our node parser:  ```python from llama_index.node_parser.extractors import (     MetadataExtractor,     TitleExtractor,     QuestionsAnsweredExtractor ) from llama_index.text_splitter import TokenTextSplitter  text_splitter = TokenTextSplitter(separator=\\\\\" \\\\\", chunk_size=512, chunk_overlap=128) metadata_extractor = MetadataExtractor(     extractors=[         TitleExtractor(nodes=5),         QuestionsAnsweredExtractor(questions=3),     ], )  node_parser = SimpleNodeParser.from_defaults(     text_splitter=text_splitter,     metadata_extractor=metadata_extractor, )\", \"assume documents are defined -> extract nodes nodes = node_parser.get_nodes_from_documents(documents) ```   ```{toctree} --- caption: Metadata Extraction Guides maxdepth: 1 --- /examples/metadata_extraction/MetadataExtractionSEC.ipynb /examples/metadata_extraction/MetadataExtraction_LLMSurvey.ipynb /examples/metadata_extraction/EntityExtractionClimate.ipynb ```\", \"Defining and Customizing Nodes  Nodes represent \\\\\"chunks\\\\\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes.  For instance, you can do  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults()  nodes = parser.get_nodes_from_documents(documents) ```  You can also choose to construct Node objects manually and skip the first section. For instance,  ```python from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo  node1 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\") node2 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\")\", \"set relationships node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id) node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id) nodes = [node1, node2] ```  The `RelatedNodeInfo` class can also store additional `metadata` if needed:  ```python node2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\\\\\"key\\\\\": \\\\\"val\\\\\"}) ```\", \"Composability   LlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.  Composability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\", \"Defining Subindices To see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.  ```python from llama_index import SimpleDirectoryReader  doc1 = SimpleDirectoryReader(\\'data1\\').load_data() doc2 = SimpleDirectoryReader(\\'data2\\').load_data() doc3 = SimpleDirectoryReader(\\'data3\\').load_data() ```  !  Now let\\'s define a tree index for each document. In order to persist the graph later, each index should share the same storage context.  In Python, we have:  ```python from llama_index import TreeIndex  storage_context = storage_context.from_defaults()  index1 = TreeIndex.from_documents(doc1, storage_context=storage_context) index2 = TreeIndex.from_documents(doc2, storage_context=storage_context) index3 = TreeIndex.from_documents(doc3, storage_context=storage_context) ```  !\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=109 request_id=4cf4b07084e7c4328f2c6dcd7de6a3c9 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=109 request_id=4cf4b07084e7c4328f2c6dcd7de6a3c9 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Defining Summary Text  You then need to explicitly define *summary text* for each subindex. This allows   the subindices to be used as Documents for higher-level indices.  ```python index1_summary = \\\\\"\\\\\" index2_summary = \\\\\"\\\\\" index3_summary = \\\\\"\\\\\" ```  You may choose to manually specify the summary text, or use LlamaIndex itself to generate a summary, for instance with the following:  ```python summary = index1.query(     \\\\\"What is a summary of this document?\\\\\", retriever_mode=\\\\\"all_leaf\\\\\" ) index1_summary = str(summary) ```  **If specified**, this summary text for each subindex can be used to refine the answer during query-time.\", \"Creating a Graph with a Top-Level Index  We can then create a graph with a list index on top of these 3 tree indices: We can query, save, and load the graph to/from disk as any other index.  ```python from llama_index.indices.composability import ComposableGraph  graph = ComposableGraph.from_indices(     ListIndex,     [index1, index2, index3],     index_summaries=[index1_summary, index2_summary, index3_summary],     storage_context=storage_context, )  ```  !\", \"Querying the Graph  During a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index.  The query will be executed recursively, starting from the root index, then the sub-indices. The default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`. Below we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).   More detail on how to configure `ComposableGraphQueryEngine` can be found here.   ```python\", \"set custom retrievers. An example is provided below custom_query_engines = {     index.index_id: index.as_query_engine(         child_branch_factor=2     )      for index in [index1, index2, index3] } query_engine = graph.as_query_engine(     custom_query_engines=custom_query_engines ) response = query_engine.query(\\\\\"Where did the author grow up?\\\\\") ```  > Note that specifying custom retriever for index by id > might require you to inspect e.g., `index1.index_id`. > Alternatively, you can explicitly set it as follows: ```python index1.set_index_id(\\\\\"\\\\\") index2.set_index_id(\\\\\"\\\\\") index3.set_index_id(\\\\\"\\\\\") ```  !  So within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.  !  NOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base!\", \"[Optional] Persisting the Graph  The graph can also be persisted to storage, and then loaded again when needed. Note that you\\'ll need to set the  ID of the root index, or keep track of the default.  ```python\", \"set the ID graph.root_index.set_index_id(\\\\\"my_id\\\\\")\", \"persist to storage graph.root_index.storage_context.persist(persist_dir=\\\\\"./storage\\\\\")\", \"load from llama_index import StorageContext, load_graph_from_storage  storage_context = StorageContext.from_defaults(persist_dir=\\\\\"./storage\\\\\") graph = load_graph_from_storage(storage_context, root_id=\\\\\"my_id\\\\\") ```   We can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham\\'s essay. We then define a keyword extractor index over the two tree indices.  Here is an example notebook.   ```{toctree} --- caption: Examples maxdepth: 1 --- ../../../../examples/composable_indices/ComposableIndices-Prior.ipynb ../../../../examples/composable_indices/ComposableIndices-Weaviate.ipynb ../../../../examples/composable_indices/ComposableIndices.ipynb ```\", \"Document Management  Most LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.\", \"Insertion  You can \\\\\"insert\\\\\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.  The underlying mechanism behind insertion depends on the index structure. For instance, for the list index, a new Document is inserted as additional node(s) in the list. For the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.  An example notebook showcasing our insert capabilities is given here. In this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures.  An example code snippet is given below:  ```python from llama_index import ListIndex, Document  index = ListIndex([]) text_chunks = [\\'text_chunk_1\\', \\'text_chunk_2\\', \\'text_chunk_3\\']  doc_chunks = [] for i, text in enumerate(text_chunks):     doc = Document(text=text, id_=f\\\\\"doc_id_{i}\\\\\")     doc_chunks.append(doc)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Defining Summary Text  You then need to explicitly define *summary text* for each subindex. This allows   the subindices to be used as Documents for higher-level indices.  ```python index1_summary = \\\\\"\\\\\" index2_summary = \\\\\"\\\\\" index3_summary = \\\\\"\\\\\" ```  You may choose to manually specify the summary text, or use LlamaIndex itself to generate a summary, for instance with the following:  ```python summary = index1.query(     \\\\\"What is a summary of this document?\\\\\", retriever_mode=\\\\\"all_leaf\\\\\" ) index1_summary = str(summary) ```  **If specified**, this summary text for each subindex can be used to refine the answer during query-time.\", \"Creating a Graph with a Top-Level Index  We can then create a graph with a list index on top of these 3 tree indices: We can query, save, and load the graph to/from disk as any other index.  ```python from llama_index.indices.composability import ComposableGraph  graph = ComposableGraph.from_indices(     ListIndex,     [index1, index2, index3],     index_summaries=[index1_summary, index2_summary, index3_summary],     storage_context=storage_context, )  ```  !\", \"Querying the Graph  During a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index.  The query will be executed recursively, starting from the root index, then the sub-indices. The default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`. Below we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).   More detail on how to configure `ComposableGraphQueryEngine` can be found here.   ```python\", \"set custom retrievers. An example is provided below custom_query_engines = {     index.index_id: index.as_query_engine(         child_branch_factor=2     )      for index in [index1, index2, index3] } query_engine = graph.as_query_engine(     custom_query_engines=custom_query_engines ) response = query_engine.query(\\\\\"Where did the author grow up?\\\\\") ```  > Note that specifying custom retriever for index by id > might require you to inspect e.g., `index1.index_id`. > Alternatively, you can explicitly set it as follows: ```python index1.set_index_id(\\\\\"\\\\\") index2.set_index_id(\\\\\"\\\\\") index3.set_index_id(\\\\\"\\\\\") ```  !  So within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.  !  NOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base!\", \"[Optional] Persisting the Graph  The graph can also be persisted to storage, and then loaded again when needed. Note that you\\'ll need to set the  ID of the root index, or keep track of the default.  ```python\", \"set the ID graph.root_index.set_index_id(\\\\\"my_id\\\\\")\", \"persist to storage graph.root_index.storage_context.persist(persist_dir=\\\\\"./storage\\\\\")\", \"load from llama_index import StorageContext, load_graph_from_storage  storage_context = StorageContext.from_defaults(persist_dir=\\\\\"./storage\\\\\") graph = load_graph_from_storage(storage_context, root_id=\\\\\"my_id\\\\\") ```   We can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham\\'s essay. We then define a keyword extractor index over the two tree indices.  Here is an example notebook.   ```{toctree} --- caption: Examples maxdepth: 1 --- ../../../../examples/composable_indices/ComposableIndices-Prior.ipynb ../../../../examples/composable_indices/ComposableIndices-Weaviate.ipynb ../../../../examples/composable_indices/ComposableIndices.ipynb ```\", \"Document Management  Most LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.\", \"Insertion  You can \\\\\"insert\\\\\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.  The underlying mechanism behind insertion depends on the index structure. For instance, for the list index, a new Document is inserted as additional node(s) in the list. For the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.  An example notebook showcasing our insert capabilities is given here. In this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures.  An example code snippet is given below:  ```python from llama_index import ListIndex, Document  index = ListIndex([]) text_chunks = [\\'text_chunk_1\\', \\'text_chunk_2\\', \\'text_chunk_3\\']  doc_chunks = [] for i, text in enumerate(text_chunks):     doc = Document(text=text, id_=f\\\\\"doc_id_{i}\\\\\")     doc_chunks.append(doc)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=cdadd177f82e6be6529573a064b4bcf7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=cdadd177f82e6be6529573a064b4bcf7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"insert for doc_chunk in doc_chunks:     index.insert(doc_chunk) ```\", \"Deletion  You can \\\\\"delete\\\\\" a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.  ```python index.delete_ref_doc(\\\\\"doc_id_0\\\\\", delete_from_docstore=True) ```  `delete_from_docstore` will default to `False` in case you are sharing nodes betweeen indexes using the same docstore. However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.\", \"Update  If a Document is already present within an index, you can \\\\\"update\\\\\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).  ```python\", \"NOTE: the document has a `doc_id` specified doc_chunks[0].text = \\\\\"Brand new document text\\\\\" index.update_ref_doc(     doc_chunks[0],      update_kwargs={\\\\\"delete_kwargs\\\\\": {\\'delete_from_docstore\\': True}} ) ```  Here, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\", \"Refresh  If you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.  The `refresh()` function will only update documents who have the same doc `id_`, but different text contents. Any documents not present in the index at all will also be inserted.  `refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.  ```python\", \"modify first document, with the same doc_id doc_chunks[0] = Document(text=\\'Super new document text\\', id_=\\\\\"doc_id_0\\\\\")\", \"add a new document doc_chunks.append(Document(text=\\\\\"This isn\\'t in the index yet, but it will be soon!\\\\\", id_=\\\\\"doc_id_3\\\\\"))\", \"refresh the index refreshed_docs = index.refresh_ref_docs(     doc_chunks,     update_kwargs={\\\\\"delete_kwargs\\\\\": {\\'delete_from_docstore\\': True}} )\", \"refreshed_docs[0] and refreshed_docs[-1] should be true ```  Again, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.  If you `print()` the output of `refresh()`, you would see which input documents were refreshed:  ```python print(refreshed_docs) > [True, False, False, True] ```  This is most useful when you are reading from a directory that is constantly updating with new information.  To autmatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag. More details can be found here.\", \"Document Tracking  Any index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.   ```python print(index.ref_doc_info) > {\\'doc_id_1\\': RefDocInfo(node_ids=[\\'071a66a8-3c47-49ad-84fa-7010c6277479\\'], metadata={}),     \\'doc_id_2\\': RefDocInfo(node_ids=[\\'9563e84b-f934-41c3-acfd-22e88492c869\\'], metadata={}),     \\'doc_id_0\\': RefDocInfo(node_ids=[\\'b53e6c2f-16f7-4024-af4c-42890e945f36\\'], metadata={}),     \\'doc_id_3\\': RefDocInfo(node_ids=[\\'6bedb29f-15db-4c7c-9885-7490e10aa33f\\'], metadata={})} ```  Each entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into.   Lastly, the original `metadata` dictionary of each input document is also tracked. You can read more about the `metadata` attribute in Customizing Documents.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"insert for doc_chunk in doc_chunks:     index.insert(doc_chunk) ```\", \"Deletion  You can \\\\\"delete\\\\\" a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.  ```python index.delete_ref_doc(\\\\\"doc_id_0\\\\\", delete_from_docstore=True) ```  `delete_from_docstore` will default to `False` in case you are sharing nodes betweeen indexes using the same docstore. However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.\", \"Update  If a Document is already present within an index, you can \\\\\"update\\\\\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).  ```python\", \"NOTE: the document has a `doc_id` specified doc_chunks[0].text = \\\\\"Brand new document text\\\\\" index.update_ref_doc(     doc_chunks[0],      update_kwargs={\\\\\"delete_kwargs\\\\\": {\\'delete_from_docstore\\': True}} ) ```  Here, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\", \"Refresh  If you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.  The `refresh()` function will only update documents who have the same doc `id_`, but different text contents. Any documents not present in the index at all will also be inserted.  `refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.  ```python\", \"modify first document, with the same doc_id doc_chunks[0] = Document(text=\\'Super new document text\\', id_=\\\\\"doc_id_0\\\\\")\", \"add a new document doc_chunks.append(Document(text=\\\\\"This isn\\'t in the index yet, but it will be soon!\\\\\", id_=\\\\\"doc_id_3\\\\\"))\", \"refresh the index refreshed_docs = index.refresh_ref_docs(     doc_chunks,     update_kwargs={\\\\\"delete_kwargs\\\\\": {\\'delete_from_docstore\\': True}} )\", \"refreshed_docs[0] and refreshed_docs[-1] should be true ```  Again, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.  If you `print()` the output of `refresh()`, you would see which input documents were refreshed:  ```python print(refreshed_docs) > [True, False, False, True] ```  This is most useful when you are reading from a directory that is constantly updating with new information.  To autmatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag. More details can be found here.\", \"Document Tracking  Any index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.   ```python print(index.ref_doc_info) > {\\'doc_id_1\\': RefDocInfo(node_ids=[\\'071a66a8-3c47-49ad-84fa-7010c6277479\\'], metadata={}),     \\'doc_id_2\\': RefDocInfo(node_ids=[\\'9563e84b-f934-41c3-acfd-22e88492c869\\'], metadata={}),     \\'doc_id_0\\': RefDocInfo(node_ids=[\\'b53e6c2f-16f7-4024-af4c-42890e945f36\\'], metadata={}),     \\'doc_id_3\\': RefDocInfo(node_ids=[\\'6bedb29f-15db-4c7c-9885-7490e10aa33f\\'], metadata={})} ```  Each entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into.   Lastly, the original `metadata` dictionary of each input document is also tracked. You can read more about the `metadata` attribute in Customizing Documents.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=385 request_id=29412d9f6f4be94e0fe0f096e33d0588 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=385 request_id=29412d9f6f4be94e0fe0f096e33d0588 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"How Each Index Works  This guide describes how each index works with diagrams.   Some terminology: - **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects. - **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to      specify different response modes here.\", \"List Index  The list index simply stores Nodes as a sequential chain.  !\", \"Querying  During query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into our Response Synthesis module.  !  The list index does offer numerous ways of querying a list index, from an embedding-based query which  will fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:  !\", \"Vector Store Index  The vector store index stores each Node and a corresponding embedding in a Vector Store.  !\", \"Querying  Querying a vector store index involves fetching the top-k most similar Nodes, and passing those into our Response Synthesis module.  !\", \"Tree Index  The tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).  !\", \"Querying  Querying a tree index involves traversing from root nodes down  to leaf nodes. By default, (`child_branch_factor=1`), a query chooses one child node given a parent node. If `child_branch_factor=2`, a query chooses two child nodes per level.  !\", \"Keyword Table Index  The keyword table index extracts keywords from each Node and builds a mapping from  each keyword to the corresponding Nodes of that keyword.  !\", \"Querying  During query time, we extract relevant keywords from the query, and match those with pre-extracted Node keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our  Response Synthesis module.  !\", \"Metadata Extraction\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"How Each Index Works  This guide describes how each index works with diagrams.   Some terminology: - **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects. - **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to      specify different response modes here.\", \"List Index  The list index simply stores Nodes as a sequential chain.  !\", \"Querying  During query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into our Response Synthesis module.  !  The list index does offer numerous ways of querying a list index, from an embedding-based query which  will fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:  !\", \"Vector Store Index  The vector store index stores each Node and a corresponding embedding in a Vector Store.  !\", \"Querying  Querying a vector store index involves fetching the top-k most similar Nodes, and passing those into our Response Synthesis module.  !\", \"Tree Index  The tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).  !\", \"Querying  Querying a tree index involves traversing from root nodes down  to leaf nodes. By default, (`child_branch_factor=1`), a query chooses one child node given a parent node. If `child_branch_factor=2`, a query chooses two child nodes per level.  !\", \"Keyword Table Index  The keyword table index extracts keywords from each Node and builds a mapping from  each keyword to the corresponding Nodes of that keyword.  !\", \"Querying  During query time, we extract relevant keywords from the query, and match those with pre-extracted Node keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our  Response Synthesis module.  !\", \"Metadata Extraction\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=181 request_id=ff1b0afcf7dfa5bb55ab131f2c852fdc response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=181 request_id=ff1b0afcf7dfa5bb55ab131f2c852fdc response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Introduction In many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.   To combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.  We show this in an example notebook and demonstrate its effectiveness in processing long documents.\", \"Usage  First, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.  We then feed this to the node parser, which will add the additional metadata to each node. ```python from llama_index.node_parser import SimpleNodeParser from llama_index.node_parser.extractors import (     MetadataExtractor,     SummaryExtractor,     QuestionsAnsweredExtractor,     TitleExtractor,     KeywordExtractor,     EntityExtractor, )  metadata_extractor = MetadataExtractor(     extractors=[         TitleExtractor(nodes=5),         QuestionsAnsweredExtractor(questions=3),         SummaryExtractor(summaries=[\\\\\"prev\\\\\", \\\\\"self\\\\\"]),         KeywordExtractor(keywords=10),         EntityExtractor(prediction_threshold=0.5),     ], )  node_parser = SimpleNodeParser.from_defaults(     metadata_extractor=metadata_extractor, ) ```  Here is an sample of extracted metadata:  ``` {\\'page_label\\': \\'2\\',  \\'file_name\\': \\'10k-132.pdf\\',  \\'document_title\\': \\'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings\\',  \\'questions_this_excerpt_can_answer\\': \\'\\\\\\\\n\\\\\\\\n1. How many countries does Uber Technologies, Inc. operate in?\\\\\\\\n2. What is the total number of MAPCs served by Uber Technologies, Inc.?\\\\\\\\n3. How much gross bookings did Uber Technologies, Inc. generate in 2019?\\',  \\'prev_section_summary\\': \\\\\"\\\\\\\\n\\\\\\\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year. These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives. It also provides an overview of the organization\\'s strategic objectives and goals for the upcoming year.\\\\\",  \\'section_summary\\': \\'\\\\\\\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure. It enables consumers and drivers to tap a button and get a ride or work. The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries. The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.\\',  \\'excerpt_keywords\\': \\'\\\\\\\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B\\'} ```\", \"Custom Extractors  If the provided extractors do not fit your needs, you can also define a custom extractor like so: ```python from llama_index.node_parser.extractors import MetadataFeatureExtractor  class CustomExtractor(MetadataFeatureExtractor):     def extract(self, nodes) -> List[Dict]:         metadata_list = [             {                 \\\\\"custom\\\\\": node.metadata[\\\\\"document_title\\\\\"]                 + \\\\\"\\\\\\\\n\\\\\"                 + node.metadata[\\\\\"excerpt_keywords\\\\\"]             }             for node in nodes         ]         return metadata_list ```  In a more advanced example, it can also make use of an `llm` to extract features from the node content and the existing metadata. Refer to the source code of the provided metadata extractors for more details.\", \"Module Guides  ```{toctree} --- maxdepth: 1 --- vector_store_guide.ipynb List Index  Tree Index  Keyword Table Index  /examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb /examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb /examples/query_engine/knowledge_graph_query_engine.ipynb /examples/query_engine/knowledge_graph_rag_query_engine.ipynb REBEL + Knowledge Graph Index  SQL Index  /examples/index_structs/struct_indices/duckdb_sql_query.ipynb /examples/index_structs/doc_summary/DocSummary.ipynb ```\", \"Indexes\", \"Concept An `Index` is a data structure that allows us to quickly retrieve relevant context for a user query. For LlamaIndex, it\\'s the core foundation for retrieval-augmented generation (RAG) use-cases.   At a high-level, `Indices` are built from Documents. They are used to build Query Engines and Chat Engines which enables question & answer and chat over your data.    Under the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a Retriever interface that supports additional configuration and automation.  For a more in-depth explanation, check out our guide below: ```{toctree} --- maxdepth: 1 --- index_guide.md ```\", \"Usage Pattern Get started with: ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(docs) ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Advanced Concepts  ```{toctree} --- maxdepth: 1 --- composability.md ```\", \"Usage Pattern\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Introduction In many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.   To combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.  We show this in an example notebook and demonstrate its effectiveness in processing long documents.\", \"Usage  First, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.  We then feed this to the node parser, which will add the additional metadata to each node. ```python from llama_index.node_parser import SimpleNodeParser from llama_index.node_parser.extractors import (     MetadataExtractor,     SummaryExtractor,     QuestionsAnsweredExtractor,     TitleExtractor,     KeywordExtractor,     EntityExtractor, )  metadata_extractor = MetadataExtractor(     extractors=[         TitleExtractor(nodes=5),         QuestionsAnsweredExtractor(questions=3),         SummaryExtractor(summaries=[\\\\\"prev\\\\\", \\\\\"self\\\\\"]),         KeywordExtractor(keywords=10),         EntityExtractor(prediction_threshold=0.5),     ], )  node_parser = SimpleNodeParser.from_defaults(     metadata_extractor=metadata_extractor, ) ```  Here is an sample of extracted metadata:  ``` {\\'page_label\\': \\'2\\',  \\'file_name\\': \\'10k-132.pdf\\',  \\'document_title\\': \\'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings\\',  \\'questions_this_excerpt_can_answer\\': \\'\\\\\\\\n\\\\\\\\n1. How many countries does Uber Technologies, Inc. operate in?\\\\\\\\n2. What is the total number of MAPCs served by Uber Technologies, Inc.?\\\\\\\\n3. How much gross bookings did Uber Technologies, Inc. generate in 2019?\\',  \\'prev_section_summary\\': \\\\\"\\\\\\\\n\\\\\\\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year. These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives. It also provides an overview of the organization\\'s strategic objectives and goals for the upcoming year.\\\\\",  \\'section_summary\\': \\'\\\\\\\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure. It enables consumers and drivers to tap a button and get a ride or work. The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries. The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.\\',  \\'excerpt_keywords\\': \\'\\\\\\\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B\\'} ```\", \"Custom Extractors  If the provided extractors do not fit your needs, you can also define a custom extractor like so: ```python from llama_index.node_parser.extractors import MetadataFeatureExtractor  class CustomExtractor(MetadataFeatureExtractor):     def extract(self, nodes) -> List[Dict]:         metadata_list = [             {                 \\\\\"custom\\\\\": node.metadata[\\\\\"document_title\\\\\"]                 + \\\\\"\\\\\\\\n\\\\\"                 + node.metadata[\\\\\"excerpt_keywords\\\\\"]             }             for node in nodes         ]         return metadata_list ```  In a more advanced example, it can also make use of an `llm` to extract features from the node content and the existing metadata. Refer to the source code of the provided metadata extractors for more details.\", \"Module Guides  ```{toctree} --- maxdepth: 1 --- vector_store_guide.ipynb List Index  Tree Index  Keyword Table Index  /examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb /examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb /examples/query_engine/knowledge_graph_query_engine.ipynb /examples/query_engine/knowledge_graph_rag_query_engine.ipynb REBEL + Knowledge Graph Index  SQL Index  /examples/index_structs/struct_indices/duckdb_sql_query.ipynb /examples/index_structs/doc_summary/DocSummary.ipynb ```\", \"Indexes\", \"Concept An `Index` is a data structure that allows us to quickly retrieve relevant context for a user query. For LlamaIndex, it\\'s the core foundation for retrieval-augmented generation (RAG) use-cases.   At a high-level, `Indices` are built from Documents. They are used to build Query Engines and Chat Engines which enables question & answer and chat over your data.    Under the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a Retriever interface that supports additional configuration and automation.  For a more in-depth explanation, check out our guide below: ```{toctree} --- maxdepth: 1 --- index_guide.md ```\", \"Usage Pattern Get started with: ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(docs) ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Advanced Concepts  ```{toctree} --- maxdepth: 1 --- composability.md ```\", \"Usage Pattern\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=86 request_id=5c65fae7ff3a4a1ee1cfae51763401ec response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=86 request_id=5c65fae7ff3a4a1ee1cfae51763401ec response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Get Started  Build an index from documents:  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(docs) ```  ```{tip} To learn how to load documents, see Data Connectors ```\", \"What is happening under the hood?  1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstractions over text str that additionally keep track of metadata and relationships). 2. Additional computation is performed to add `Node` into index data structure    > Note: the computation is index-specific.    >    > - For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects    > - For a document summary index, this means calling an LLM to generate a summary\", \"Configuring Document Parsing  The most common configuration you might want to change is how to parse document into `Node` objects.\", \"High-Level API  We can configure our service context to use the desired chunk size and set `show_progress` to display a progress bar during index construction.  ```python from llama_index import ServiceContext, VectorStoreIndex  service_context = ServiceContext.from_defaults(chunk_size=512) index = VectorStoreIndex.from_documents(     docs,     service_context=service_context,     show_progress=True ) ```  > Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\", \"Low-Level API  You can use the low-level composition API if you need more granular control.  Here we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.   The steps are:  1. Configure a node parser  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults(     chunk_size=512,     include_extra_info=False,     include_prev_next_rel=False, ) ```  2. Parse document into `Node` objects  ```python nodes = parser.get_nodes_from_documents(documents) ```  3. build index from `Node` objects  ```python index = VectorStoreIndex(nodes) ```\", \"Handling Document Update  Read more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.  ```{toctree} --- maxdepth: 1 --- metadata_extraction.md document_management.md ```\", \"Node Parser\", \"Concept  Node parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific size. When a document is broken into nodes, all of it\\'s attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properties here.  A node parser can configure the chunk size (in tokens) as well as any overlap between chunked nodes. The chunking is done by using a `TokenTextSplitter`, which default to a chunk size of 1024 and a default chunk overlap of 20 tokens.\", \"Usage Pattern  ```python from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) ```  You can find more usage details and availbale customization options below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Usage Pattern\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Get Started  Build an index from documents:  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(docs) ```  ```{tip} To learn how to load documents, see Data Connectors ```\", \"What is happening under the hood?  1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstractions over text str that additionally keep track of metadata and relationships). 2. Additional computation is performed to add `Node` into index data structure    > Note: the computation is index-specific.    >    > - For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects    > - For a document summary index, this means calling an LLM to generate a summary\", \"Configuring Document Parsing  The most common configuration you might want to change is how to parse document into `Node` objects.\", \"High-Level API  We can configure our service context to use the desired chunk size and set `show_progress` to display a progress bar during index construction.  ```python from llama_index import ServiceContext, VectorStoreIndex  service_context = ServiceContext.from_defaults(chunk_size=512) index = VectorStoreIndex.from_documents(     docs,     service_context=service_context,     show_progress=True ) ```  > Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\", \"Low-Level API  You can use the low-level composition API if you need more granular control.  Here we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.   The steps are:  1. Configure a node parser  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults(     chunk_size=512,     include_extra_info=False,     include_prev_next_rel=False, ) ```  2. Parse document into `Node` objects  ```python nodes = parser.get_nodes_from_documents(documents) ```  3. build index from `Node` objects  ```python index = VectorStoreIndex(nodes) ```\", \"Handling Document Update  Read more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.  ```{toctree} --- maxdepth: 1 --- metadata_extraction.md document_management.md ```\", \"Node Parser\", \"Concept  Node parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific size. When a document is broken into nodes, all of it\\'s attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properties here.  A node parser can configure the chunk size (in tokens) as well as any overlap between chunked nodes. The chunking is done by using a `TokenTextSplitter`, which default to a chunk size of 1024 and a default chunk overlap of 20 tokens.\", \"Usage Pattern  ```python from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) ```  You can find more usage details and availbale customization options below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Usage Pattern\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1056 request_id=4308628da993d3c681b6002f1d818bf4 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1056 request_id=4308628da993d3c681b6002f1d818bf4 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Getting Started  Node parsers can be used on their own:  ```python from llama_index import Document from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)  nodes = node_parser.get_nodes_from_documents([Document(text=\\\\\"long text\\\\\")], show_progress=False) ```  Or set inside a `ServiceContext` to be used automatically when an index is constructed using `.from_documents()`:  ```python from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext from llama_index.node_parser import SimpleNodeParser  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) service_context = ServiceContext.from_defaults(node_parser=node_parser)  index = VectorStoreIndex.from_documents(documents, service_context=service_context) ```\", \"Customization  There are several options available to customize:  - `text_splitter` (defaults to `TokenTextSplitter`) - the text splitter used to split text into chunks. - `include_metadata` (defaults to `True`) - whether or not `Node`s should inherit the document metadata. - `include_prev_next_rel` (defaults to `True`) - whether or not to include previous/next relationships between chunked `Node`s - `metadata_extractor` (defaults to `None`) - extra processing to extract helpful metadata. See here for details.  If you don\\'t want to change the `text_splitter`, you can use `SimpleNodeParser.from_defaults()` to easily change the chunk size and chunk overlap. The defaults are 1024 and 20 respectively.  ```python from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) ```\", \"Text Splitter Customization  If you do customize the `text_splitter` from the default `SentenceSplitter`, you can use any splitter from langchain, or optionally our `TokenTextSplitter` or `CodeSplitter`. Each text splitter has options for the default separator, as well as options for additional config. These are useful for languages that are sufficiently different from English.  `SentenceSplitter` default configuration:  ```python import tiktoken from llama_index.text_splitter import SentenceSplitter  text_splitter = SentenceSplitter(   separator=\\\\\" \\\\\",   chunk_size=1024,   chunk_overlap=20,   paragraph_separator=\\\\\"\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\",   secondary_chunking_regex=\\\\\"[^,.;\\\\u3002]+[,.;\\\\u3002]?\\\\\",   tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```  `TokenTextSplitter` default configuration:  ```python import tiktoken from llama_index.text_splitter import TokenTextSplitter  text_splitter = TokenTextSplitter(   separator=\\\\\" \\\\\",   chunk_size=1024,   chunk_overlap=20,   backup_separators=[\\\\\"\\\\\\\\n\\\\\"],   tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```  `CodeSplitter` configuration:  ```python from llama_index.text_splitter import CodeSplitter  text_splitter = CodeSplitter(   language=\\\\\"python\\\\\",   chunk_lines=40,   chunk_lines_overlap=15,   max_chars=1500, )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```\", \"SentenceWindowNodeParser  The `SentenceWindowNodeParser` is similar to the `SimpleNodeParser`, except that it splits all documents into individual sentences. The resulting nodes also contain the surrounding \\\\\"window\\\\\" of sentences around each node in the metadata. Note that this metadata will not be visible to the LLM or embedding model.  This is most useful for generating embeddings that have a very specific scope. Then, combined with a `MetadataReplacementNodePostProcessor`, you can replace the sentence with it\\'s surrounding context before sending the node to the LLM.   An example of setting up the parser with default settings is below. In practice, you would usually only want to adjust the window size of sentences.  ```python import nltk from llama_index.node_parser import SentenceWindowNodeParser  node_parser = SentenceWindowNodeParser.from_defaults(   # how many sentences on either side to capture   window_size=3,     # the metadata key that holds the window of surrounding sentences   window_metadata_key=\\\\\"window\\\\\",     # the metadata key that holds the original sentence   original_text_metadata_key=\\\\\"original_sentence\\\\\" ) ```  A full example can be found here in combination with the `MetadataReplacementNodePostProcessor`.\", \"Customizing Storage  By default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code: ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(\\\\\"Summarize the documents.\\\\\") ```  Under the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.   !\", \"Low-Level API To do this, instead of the high-level API, ```python index = VectorStoreIndex.from_documents(documents) ``` we use a lower-level API that gives more granular control: ```python from llama_index.storage.docstore import SimpleDocumentStore from llama_index.storage.index_store import SimpleIndexStore from llama_index.vector_stores import SimpleVectorStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create storage context using default stores storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore(),     vector_store=SimpleVectorStore(),     index_store=SimpleIndexStore(), )\", \"create (or load) docstore and add nodes storage_context.docstore.add_documents(nodes)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Getting Started  Node parsers can be used on their own:  ```python from llama_index import Document from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)  nodes = node_parser.get_nodes_from_documents([Document(text=\\\\\"long text\\\\\")], show_progress=False) ```  Or set inside a `ServiceContext` to be used automatically when an index is constructed using `.from_documents()`:  ```python from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext from llama_index.node_parser import SimpleNodeParser  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) service_context = ServiceContext.from_defaults(node_parser=node_parser)  index = VectorStoreIndex.from_documents(documents, service_context=service_context) ```\", \"Customization  There are several options available to customize:  - `text_splitter` (defaults to `TokenTextSplitter`) - the text splitter used to split text into chunks. - `include_metadata` (defaults to `True`) - whether or not `Node`s should inherit the document metadata. - `include_prev_next_rel` (defaults to `True`) - whether or not to include previous/next relationships between chunked `Node`s - `metadata_extractor` (defaults to `None`) - extra processing to extract helpful metadata. See here for details.  If you don\\'t want to change the `text_splitter`, you can use `SimpleNodeParser.from_defaults()` to easily change the chunk size and chunk overlap. The defaults are 1024 and 20 respectively.  ```python from llama_index.node_parser import SimpleNodeParser  node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20) ```\", \"Text Splitter Customization  If you do customize the `text_splitter` from the default `SentenceSplitter`, you can use any splitter from langchain, or optionally our `TokenTextSplitter` or `CodeSplitter`. Each text splitter has options for the default separator, as well as options for additional config. These are useful for languages that are sufficiently different from English.  `SentenceSplitter` default configuration:  ```python import tiktoken from llama_index.text_splitter import SentenceSplitter  text_splitter = SentenceSplitter(   separator=\\\\\" \\\\\",   chunk_size=1024,   chunk_overlap=20,   paragraph_separator=\\\\\"\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\",   secondary_chunking_regex=\\\\\"[^,.;\\\\u3002]+[,.;\\\\u3002]?\\\\\",   tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```  `TokenTextSplitter` default configuration:  ```python import tiktoken from llama_index.text_splitter import TokenTextSplitter  text_splitter = TokenTextSplitter(   separator=\\\\\" \\\\\",   chunk_size=1024,   chunk_overlap=20,   backup_separators=[\\\\\"\\\\\\\\n\\\\\"],   tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```  `CodeSplitter` configuration:  ```python from llama_index.text_splitter import CodeSplitter  text_splitter = CodeSplitter(   language=\\\\\"python\\\\\",   chunk_lines=40,   chunk_lines_overlap=15,   max_chars=1500, )  node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter) ```\", \"SentenceWindowNodeParser  The `SentenceWindowNodeParser` is similar to the `SimpleNodeParser`, except that it splits all documents into individual sentences. The resulting nodes also contain the surrounding \\\\\"window\\\\\" of sentences around each node in the metadata. Note that this metadata will not be visible to the LLM or embedding model.  This is most useful for generating embeddings that have a very specific scope. Then, combined with a `MetadataReplacementNodePostProcessor`, you can replace the sentence with it\\'s surrounding context before sending the node to the LLM.   An example of setting up the parser with default settings is below. In practice, you would usually only want to adjust the window size of sentences.  ```python import nltk from llama_index.node_parser import SentenceWindowNodeParser  node_parser = SentenceWindowNodeParser.from_defaults(   # how many sentences on either side to capture   window_size=3,     # the metadata key that holds the window of surrounding sentences   window_metadata_key=\\\\\"window\\\\\",     # the metadata key that holds the original sentence   original_text_metadata_key=\\\\\"original_sentence\\\\\" ) ```  A full example can be found here in combination with the `MetadataReplacementNodePostProcessor`.\", \"Customizing Storage  By default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code: ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(\\\\\"Summarize the documents.\\\\\") ```  Under the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.   !\", \"Low-Level API To do this, instead of the high-level API, ```python index = VectorStoreIndex.from_documents(documents) ``` we use a lower-level API that gives more granular control: ```python from llama_index.storage.docstore import SimpleDocumentStore from llama_index.storage.index_store import SimpleIndexStore from llama_index.vector_stores import SimpleVectorStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create storage context using default stores storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore(),     vector_store=SimpleVectorStore(),     index_store=SimpleIndexStore(), )\", \"create (or load) docstore and add nodes storage_context.docstore.add_documents(nodes)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=77 request_id=e12f7492ddd10a3372495aa4203dea56 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=77 request_id=e12f7492ddd10a3372495aa4203dea56 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"save index index.storage_context.persist(persist_dir=\\\\\"\\\\\")\", \"can also set index_id to save multiple indexes to the same folder index.set_index_id = \\\\\"\\\\\" index.storage_context.persist(persist_dir=\\\\\"\\\\\")\", \"to load index later, make sure you setup the storage context storage_context = StorageContext.from_defaults(     persist_dir=\\\\\"\\\\\" )\", \"then load the index object from llama_index import load_index_from_storage loaded_index = load_index_from_storage(storage_context)\", \"if loading an index from a persist_dir containing multiple indexes loaded_index = load_index_from_storage(storage_context, index_id=\\\\\"\\\\\")\", \"if loading multiple indexes from a persist dir loaded_indicies = load_index_from_storage(storage_context, index_ids=[\\\\\"\\\\\", ...]) ```  You can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores. See Document Stores, Vector Stores, Index Stores guides for more details.  For saving and loading a graph/composable index, see the full guide here.\", \"Vector Store Integrations and Storage  Most of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to exlicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.  The vector stores that support this practice are:  - ChatGPTRetrievalPluginClient - CassandraVectorStore - ChromaVectorStore - DocArrayHnswVectorStore - DocArrayInMemoryVectorStore - LanceDBVectorStore - MetalVectorStore - MilvusVectorStore - MyScaleVectorStore - OpensearchVectorStore - PineconeVectorStore - QdrantVectorStore - RedisVectorStore - WeaviateVectorStore  A small example using Pinecone is below:  ```python import pinecone from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.vector_stores import PineconeVectorStore\", \"Creating a Pinecone index api_key = \\\\\"api_key\\\\\" pinecone.init(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\") pinecone.create_index(     \\\\\"quickstart\\\\\",     dimension=1536,     metric=\\\\\"euclidean\\\\\",     pod_type=\\\\\"p1\\\\\" ) index = pinecone.Index(\\\\\"quickstart\\\\\")\", \"construct vector store vector_store = PineconeVectorStore(pinecone_index=index)\", \"create storage context storage_context = StorageContext.from_defaults(vector_store=vector_store)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"save index index.storage_context.persist(persist_dir=\\\\\"\\\\\")\", \"can also set index_id to save multiple indexes to the same folder index.set_index_id = \\\\\"\\\\\" index.storage_context.persist(persist_dir=\\\\\"\\\\\")\", \"to load index later, make sure you setup the storage context storage_context = StorageContext.from_defaults(     persist_dir=\\\\\"\\\\\" )\", \"then load the index object from llama_index import load_index_from_storage loaded_index = load_index_from_storage(storage_context)\", \"if loading an index from a persist_dir containing multiple indexes loaded_index = load_index_from_storage(storage_context, index_id=\\\\\"\\\\\")\", \"if loading multiple indexes from a persist dir loaded_indicies = load_index_from_storage(storage_context, index_ids=[\\\\\"\\\\\", ...]) ```  You can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores. See Document Stores, Vector Stores, Index Stores guides for more details.  For saving and loading a graph/composable index, see the full guide here.\", \"Vector Store Integrations and Storage  Most of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to exlicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.  The vector stores that support this practice are:  - ChatGPTRetrievalPluginClient - CassandraVectorStore - ChromaVectorStore - DocArrayHnswVectorStore - DocArrayInMemoryVectorStore - LanceDBVectorStore - MetalVectorStore - MilvusVectorStore - MyScaleVectorStore - OpensearchVectorStore - PineconeVectorStore - QdrantVectorStore - RedisVectorStore - WeaviateVectorStore  A small example using Pinecone is below:  ```python import pinecone from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.vector_stores import PineconeVectorStore\", \"Creating a Pinecone index api_key = \\\\\"api_key\\\\\" pinecone.init(api_key=api_key, environment=\\\\\"us-west1-gcp\\\\\") pinecone.create_index(     \\\\\"quickstart\\\\\",     dimension=1536,     metric=\\\\\"euclidean\\\\\",     pod_type=\\\\\"p1\\\\\" ) index = pinecone.Index(\\\\\"quickstart\\\\\")\", \"construct vector store vector_store = PineconeVectorStore(pinecone_index=index)\", \"create storage context storage_context = StorageContext.from_defaults(vector_store=vector_store)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=102 request_id=033bcc3ab10bdcfdcf3fedf42ca6bdd6 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=102 request_id=033bcc3ab10bdcfdcf3fedf42ca6bdd6 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"load documents documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\", \"create index, which will insert documents/vectors to pinecone index = VectorStoreIndex.from_documents(documents, storage_context=storage_context) ```  If you have an existing vector store with data already loaded in,  you can connect to it and directly create a `VectorStoreIndex` as follows:  ```python index = pinecone.Index(\\\\\"quickstart\\\\\") vector_store = PineconeVectorStore(pinecone_index=index) loaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store) ```\", \"Document Stores Document stores contain ingested document chunks, which we call `Node` objects.  See the API Reference for more details.\", \"Simple Document Store By default, the `SimpleDocumentStore` stores `Node` objects in-memory.  They can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).  A more complete example can be found here\", \"MongoDB Document Store We support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested. ```python from llama_index.storage.docstore import MongoDocumentStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = MongoDocumentStore.from_uri(uri=\\\\\"\\\\\") docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes. > Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\\\\\"db_docstore\\\\\"` and `namespace=\\\\\"docstore\\\\\"`.  Note that it\\'s not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore` since data is persisted by default.   You can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.  A more complete example can be found here\", \"Redis Document Store  We support Redis as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.docstore import RedisDocumentStore from llama_index.node_parser import SimpleNodeParser\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"load documents documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\", \"create index, which will insert documents/vectors to pinecone index = VectorStoreIndex.from_documents(documents, storage_context=storage_context) ```  If you have an existing vector store with data already loaded in,  you can connect to it and directly create a `VectorStoreIndex` as follows:  ```python index = pinecone.Index(\\\\\"quickstart\\\\\") vector_store = PineconeVectorStore(pinecone_index=index) loaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store) ```\", \"Document Stores Document stores contain ingested document chunks, which we call `Node` objects.  See the API Reference for more details.\", \"Simple Document Store By default, the `SimpleDocumentStore` stores `Node` objects in-memory.  They can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).  A more complete example can be found here\", \"MongoDB Document Store We support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested. ```python from llama_index.storage.docstore import MongoDocumentStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = MongoDocumentStore.from_uri(uri=\\\\\"\\\\\") docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes. > Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\\\\\"db_docstore\\\\\"` and `namespace=\\\\\"docstore\\\\\"`.  Note that it\\'s not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore` since data is persisted by default.   You can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.  A more complete example can be found here\", \"Redis Document Store  We support Redis as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.docstore import RedisDocumentStore from llama_index.node_parser import SimpleNodeParser\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=283 request_id=f409620f37837ebc6c855a10f1b672a5 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=283 request_id=f409620f37837ebc6c855a10f1b672a5 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = RedisDocumentStore.from_host_and_port(   host=\\\\\"127.0.0.1\\\\\",    port=\\\\\"6379\\\\\",    namespace=\\'llama_index\\' ) docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`. > Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\\\\\"docstore\\\\\"`.  You can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.  A more complete example can be found here\", \"Firestore Document Store  We support Firestore as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.docstore import FirestoreDocumentStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = FirestoreDocumentStore.from_dataabse(   project=\\\\\"project-id\\\\\",   database=\\\\\"(default)\\\\\", ) docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `FirestoreDocumentStore` connects to a firestore database in Google Cloud and adds your nodes to a namespace stored under `{namespace}/docs`. > Note: You can configure the `namespace` when instantiating `FirestoreDocumentStore`, otherwise it defaults `namespace=\\\\\"docstore\\\\\"`.  You can easily reconnect to your Firestore database and reload the index by re-initializing a `FirestoreDocumentStore` with an existing `project`, `database`, and `namespace`.  A more complete example can be found here\", \"Index Stores  Index stores contains lightweight index metadata (i.e. additional state information created when building an index).  See the API Reference for more details.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = RedisDocumentStore.from_host_and_port(   host=\\\\\"127.0.0.1\\\\\",    port=\\\\\"6379\\\\\",    namespace=\\'llama_index\\' ) docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`. > Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\\\\\"docstore\\\\\"`.  You can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.  A more complete example can be found here\", \"Firestore Document Store  We support Firestore as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.docstore import FirestoreDocumentStore from llama_index.node_parser import SimpleNodeParser\", \"create parser and parse document into nodes parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)\", \"create (or load) docstore and add nodes docstore = FirestoreDocumentStore.from_dataabse(   project=\\\\\"project-id\\\\\",   database=\\\\\"(default)\\\\\", ) docstore.add_documents(nodes)\", \"create storage context storage_context = StorageContext.from_defaults(docstore=docstore)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context) ```  Under the hood, `FirestoreDocumentStore` connects to a firestore database in Google Cloud and adds your nodes to a namespace stored under `{namespace}/docs`. > Note: You can configure the `namespace` when instantiating `FirestoreDocumentStore`, otherwise it defaults `namespace=\\\\\"docstore\\\\\"`.  You can easily reconnect to your Firestore database and reload the index by re-initializing a `FirestoreDocumentStore` with an existing `project`, `database`, and `namespace`.  A more complete example can be found here\", \"Index Stores  Index stores contains lightweight index metadata (i.e. additional state information created when building an index).  See the API Reference for more details.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=1f195d67eda18e315947dee8bdfce0f5 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=1f195d67eda18e315947dee8bdfce0f5 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Simple Index Store By default, LlamaIndex uses a simple index store backed by an in-memory key-value store. They can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\", \"MongoDB Index Store Similarly to document stores, we can also use `MongoDB` as the storage backend of the index store.   ```python from llama_index.storage.index_store import MongoIndexStore from llama_index import VectorStoreIndex\", \"create (or load) index store index_store = MongoIndexStore.from_uri(uri=\\\\\"\\\\\")\", \"create storage context storage_context = StorageContext.from_defaults(index_store=index_store)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\", \"or alternatively, load index from llama_index import load_index_from_storage index = load_index_from_storage(storage_context) ```  Under the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata. > Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\\\\\"db_docstore\\\\\"` and `namespace=\\\\\"docstore\\\\\"`.  Note that it\\'s not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore` since data is persisted by default.   You can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.  A more complete example can be found here\", \"Redis Index Store  We support Redis as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.index_store import RedisIndexStore from llama_index import VectorStoreIndex\", \"create (or load) docstore and add nodes index_store = RedisIndexStore.from_host_and_port(   host=\\\\\"127.0.0.1\\\\\",    port=\\\\\"6379\\\\\",    namespace=\\'llama_index\\' )\", \"create storage context storage_context = StorageContext.from_defaults(index_store=index_store)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Simple Index Store By default, LlamaIndex uses a simple index store backed by an in-memory key-value store. They can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\", \"MongoDB Index Store Similarly to document stores, we can also use `MongoDB` as the storage backend of the index store.   ```python from llama_index.storage.index_store import MongoIndexStore from llama_index import VectorStoreIndex\", \"create (or load) index store index_store = MongoIndexStore.from_uri(uri=\\\\\"\\\\\")\", \"create storage context storage_context = StorageContext.from_defaults(index_store=index_store)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\", \"or alternatively, load index from llama_index import load_index_from_storage index = load_index_from_storage(storage_context) ```  Under the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata. > Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\\\\\"db_docstore\\\\\"` and `namespace=\\\\\"docstore\\\\\"`.  Note that it\\'s not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore` since data is persisted by default.   You can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.  A more complete example can be found here\", \"Redis Index Store  We support Redis as an alternative document store backend that persists data as `Node` objects are ingested.  ```python from llama_index.storage.index_store import RedisIndexStore from llama_index import VectorStoreIndex\", \"create (or load) docstore and add nodes index_store = RedisIndexStore.from_host_and_port(   host=\\\\\"127.0.0.1\\\\\",    port=\\\\\"6379\\\\\",    namespace=\\'llama_index\\' )\", \"create storage context storage_context = StorageContext.from_defaults(index_store=index_store)\", \"build index index = VectorStoreIndex(nodes, storage_context=storage_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=160 request_id=514a5cd12154114aa25d5c05e9fc0d02 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=160 request_id=514a5cd12154114aa25d5c05e9fc0d02 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"or alternatively, load index from llama_index import load_index_from_storage index = load_index_from_storage(storage_context) ```  Under the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`. > Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\\\\\"index_store\\\\\"`.  You can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.  A more complete example can be found here\", \"Key-Value Stores  Key-Value stores are the underlying storage abstractions that power our Document Stores and Index Stores.  We provide the following key-value stores: - **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk. - **MongoDB Key-Value Store**: A MongoDB KV store.  See the API Reference for more details.  Note: At the moment, these storage abstractions are not externally facing.\", \"Storage\", \"Concept  LlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.  Under the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:  - **Document stores**: where ingested documents (i.e., `Node` objects) are stored, - **Index stores**: where index metadata are stored, - **Vector stores**: where embedding vectors are stored. - **Graph stores**: where knowledge graphs are stored (i.e. for `KnowledgeGraphIndex`).  The Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.  LlamaIndex supports persisting data to any storage backend supported by fsspec.  We have confirmed support for the following storage backends:  - Local filesystem - AWS S3 - Cloudflare R2   !\", \"Usage Pattern  Many vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This *also* means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.  ```python\", \"build a new index from llama_index import VectorStoreIndex, StorageContext from llama_index.vector_stores import DeepLakeVectorStore\", \"construct vector store and customize storage context vector_store = DeepLakeVectorStore(dataset_path=\\\\\"\\\\\") storage_context = StorageContext.from_defaults(     vector_store = vector_store )\", \"Load documents and build index index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"reload an existing one index = VectorStoreIndex.from_vector_store(vector_store=vector_store) ```  See our Vector Store Module Guide below for more details.   Note that in general to use storage abstractions, you need to define a `StorageContext` object:  ```python from llama_index.storage.docstore import SimpleDocumentStore from llama_index.storage.index_store import SimpleIndexStore from llama_index.vector_stores import SimpleVectorStore from llama_index.storage import StorageContext\", \"create storage context using default stores storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore(),     vector_store=SimpleVectorStore(),     index_store=SimpleIndexStore(), ) ```  More details on customization/persistence can be found in the guides below.   ```{toctree} --- maxdepth: 1 --- customization.md save_load.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"or alternatively, load index from llama_index import load_index_from_storage index = load_index_from_storage(storage_context) ```  Under the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`. > Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\\\\\"index_store\\\\\"`.  You can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.  A more complete example can be found here\", \"Key-Value Stores  Key-Value stores are the underlying storage abstractions that power our Document Stores and Index Stores.  We provide the following key-value stores: - **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk. - **MongoDB Key-Value Store**: A MongoDB KV store.  See the API Reference for more details.  Note: At the moment, these storage abstractions are not externally facing.\", \"Storage\", \"Concept  LlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.  Under the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:  - **Document stores**: where ingested documents (i.e., `Node` objects) are stored, - **Index stores**: where index metadata are stored, - **Vector stores**: where embedding vectors are stored. - **Graph stores**: where knowledge graphs are stored (i.e. for `KnowledgeGraphIndex`).  The Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.  LlamaIndex supports persisting data to any storage backend supported by fsspec.  We have confirmed support for the following storage backends:  - Local filesystem - AWS S3 - Cloudflare R2   !\", \"Usage Pattern  Many vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This *also* means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.  ```python\", \"build a new index from llama_index import VectorStoreIndex, StorageContext from llama_index.vector_stores import DeepLakeVectorStore\", \"construct vector store and customize storage context vector_store = DeepLakeVectorStore(dataset_path=\\\\\"\\\\\") storage_context = StorageContext.from_defaults(     vector_store = vector_store )\", \"Load documents and build index index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\", \"reload an existing one index = VectorStoreIndex.from_vector_store(vector_store=vector_store) ```  See our Vector Store Module Guide below for more details.   Note that in general to use storage abstractions, you need to define a `StorageContext` object:  ```python from llama_index.storage.docstore import SimpleDocumentStore from llama_index.storage.index_store import SimpleIndexStore from llama_index.vector_stores import SimpleVectorStore from llama_index.storage import StorageContext\", \"create storage context using default stores storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore(),     vector_store=SimpleVectorStore(),     index_store=SimpleIndexStore(), ) ```  More details on customization/persistence can be found in the guides below.   ```{toctree} --- maxdepth: 1 --- customization.md save_load.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=112 request_id=eb15cc437f7c12ce2fa9ab112450e7ef response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=112 request_id=eb15cc437f7c12ce2fa9ab112450e7ef response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Modules  We offer in-depth guides on the different storage components.  ```{toctree} --- maxdepth: 1 --- vector_stores.md docstores.md index_stores.md kv_stores.md /community/integrations/graph_stores.md ```\", \"Persisting & Loading Data\", \"Persisting Data By default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired: ```python storage_context.persist(persist_dir=\\\\\"\\\\\") ``` This will persist data to disk, under the specified `persist_dir` (or `./storage` by default).  Multiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID\\'s for loading.  User can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default. In this case, calling `storage_context.persist()` will do nothing.\", \"Loading Data To load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).  ```python storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\\\\\"\\\\\"),     vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\\\\\"\\\\\"),     index_store=SimpleIndexStore.from_persist_dir(persist_dir=\\\\\"\\\\\"), ) ```  We can then load specific indices from the `StorageContext` through some convenience functions below.   ```python from llama_index import load_index_from_storage, load_indices_from_storage, load_graph_from_storage\", \"load a single index index = load_index_from_storage(storage_context, index_id=\\\\\"\\\\\")\", \"don\\'t need to specify index_id if there\\'s only one index in storage context index = load_index_from_storage(storage_context)\", \"load multiple indices indices = load_indices_from_storage(storage_context) # loads all indices indices = load_indices_from_storage(storage_context, index_ids=[index_id1, ...]) # loads specific indices\", \"load composable graph graph = load_graph_from_storage(storage_context, root_id=\\\\\"\\\\\") # loads graph with the specified root_id ```  Here\\'s the full API Reference on saving and loading.\", \"Using a remote backend  By default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.  Here\\'s a simple example, instantiating a vector store: ```python import dotenv import s3fs import os dotenv.load_dotenv(\\\\\"../../../.env\\\\\")\", \"load documents documents = SimpleDirectoryReader(\\'../../../examples/paul_graham_essay/data/\\').load_data() print(len(documents)) index = VectorStoreIndex.from_documents(documents) ```  At this point, everything has been the same. Now - let\\'s instantiate a S3 filesystem and save / load from there.  ```python\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Modules  We offer in-depth guides on the different storage components.  ```{toctree} --- maxdepth: 1 --- vector_stores.md docstores.md index_stores.md kv_stores.md /community/integrations/graph_stores.md ```\", \"Persisting & Loading Data\", \"Persisting Data By default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired: ```python storage_context.persist(persist_dir=\\\\\"\\\\\") ``` This will persist data to disk, under the specified `persist_dir` (or `./storage` by default).  Multiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID\\'s for loading.  User can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default. In this case, calling `storage_context.persist()` will do nothing.\", \"Loading Data To load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).  ```python storage_context = StorageContext.from_defaults(     docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\\\\\"\\\\\"),     vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\\\\\"\\\\\"),     index_store=SimpleIndexStore.from_persist_dir(persist_dir=\\\\\"\\\\\"), ) ```  We can then load specific indices from the `StorageContext` through some convenience functions below.   ```python from llama_index import load_index_from_storage, load_indices_from_storage, load_graph_from_storage\", \"load a single index index = load_index_from_storage(storage_context, index_id=\\\\\"\\\\\")\", \"don\\'t need to specify index_id if there\\'s only one index in storage context index = load_index_from_storage(storage_context)\", \"load multiple indices indices = load_indices_from_storage(storage_context) # loads all indices indices = load_indices_from_storage(storage_context, index_ids=[index_id1, ...]) # loads specific indices\", \"load composable graph graph = load_graph_from_storage(storage_context, root_id=\\\\\"\\\\\") # loads graph with the specified root_id ```  Here\\'s the full API Reference on saving and loading.\", \"Using a remote backend  By default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.  Here\\'s a simple example, instantiating a vector store: ```python import dotenv import s3fs import os dotenv.load_dotenv(\\\\\"../../../.env\\\\\")\", \"load documents documents = SimpleDirectoryReader(\\'../../../examples/paul_graham_essay/data/\\').load_data() print(len(documents)) index = VectorStoreIndex.from_documents(documents) ```  At this point, everything has been the same. Now - let\\'s instantiate a S3 filesystem and save / load from there.  ```python\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1328 request_id=c286ff69082c4f90527a39c3f14aa2b6 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1328 request_id=c286ff69082c4f90527a39c3f14aa2b6 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"set up s3fs AWS_KEY = os.environ[\\'AWS_ACCESS_KEY_ID\\'] AWS_SECRET = os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] R2_ACCOUNT_ID = os.environ[\\'R2_ACCOUNT_ID\\']  assert AWS_KEY is not None and AWS_KEY != \\\\\"\\\\\"  s3 = s3fs.S3FileSystem(    key=AWS_KEY,    secret=AWS_SECRET,    endpoint_url=f\\'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com\\',    s3_additional_kwargs={\\'ACL\\': \\'public-read\\'} )\", \"save index to remote blob storage index.set_index_id(\\\\\"vector_index\\\\\")\", \"this is {bucket_name}/{index_name} index.storage_context.persist(\\'llama-index/storage_demo\\', fs=s3)\", \"load index from s3 sc = StorageContext.from_defaults(persist_dir=\\'llama-index/storage_demo\\', fs=s3) index2 = load_index_from_storage(sc, \\'vector_index\\') ```  By default, if you do not pass a filesystem, we will assume a local filesystem.\", \"Vector Stores  Vector stores contain embedding vectors of ingested document chunks (and sometimes the document chunks as well).\", \"Simple Vector Store  By default, LlamaIndex uses a simple in-memory vector store that\\'s great for quick experimentation. They can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).\", \"Vector Store Options & Feature Support  LlamaIndex supports over 20 different vector store options.We are actively adding more integrations and improving feature coverage for each.| Vector Store             | Type                | Metadata Filtering | Hybrid Search | Delete | Store Documents | Async | |--------------------------|---------------------|--------------------|---------------|--------|-----------------|-------| | Pinecone                 | cloud               | \\\\u2713                  | \\\\u2713             | \\\\u2713      | \\\\u2713               |       | | Weaviate                 | self-hosted / cloud | \\\\u2713                  | \\\\u2713             | \\\\u2713      | \\\\u2713               |       | | Postgres                 | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               | \\\\u2713     | | Cassandra                | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Qdrant                   | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Chroma                   | self-hosted         | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Milvus / Zilliz          | self-hosted / cloud |                    |               | \\\\u2713      | \\\\u2713               |       | | Typesense                | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Supabase                 | self-hosted / cloud | \\\\u2713                  |               |        | \\\\u2713               |       | | MongoDB Atlas            | self-hosted / cloud | \\\\u2713                  |               |\", \"\\\\u2713      | \\\\u2713               |       | | Redis                    | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Deeplake                 | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | OpenSearch               | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | DynamoDB                 | cloud               |                    |               | \\\\u2713      |                 |       | | LanceDB                  | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Metal                    | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | MyScale                  | cloud               |                    |               |        | \\\\u2713               |       | | Tair                     | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Simple                   | in-memory           |                    |               | \\\\u2713      |                 |       | | FAISS                    | in-memory           |                    |               |\", \"|                 |       | | ChatGPT Retrieval Plugin | aggregator          |                    |               | \\\\u2713      | \\\\u2713               |       | | DocArray                 | aggregator          | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       |  For more details, see Vector Store Integrations.```{toctree} --- caption: Examples maxdepth: 1 --- /examples/vector_stores/SimpleIndexDemo.ipynb /examples/vector_stores/RocksetIndexDemo.ipynb /examples/vector_stores/QdrantIndexDemo.ipynb /examples/vector_stores/FaissIndexDemo.ipynb /examples/vector_stores/DeepLakeIndexDemo.ipynb /examples/vector_stores/MyScaleIndexDemo.ipynb /examples/vector_stores/MetalIndexDemo.ipynb /examples/vector_stores/WeaviateIndexDemo.ipynb /examples/vector_stores/OpensearchDemo.ipynb /examples/vector_stores/PineconeIndexDemo.ipynb /examples/vector_stores/ChromaIndexDemo.ipynb /examples/vector_stores/LanceDBIndexDemo.ipynb /examples/vector_stores/MilvusIndexDemo.ipynb /examples/vector_stores/RedisIndexDemo.ipynb /examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb /examples/vector_stores/ZepIndexDemo.ipynb /examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb /examples/vector_stores/AsyncIndexCreationDemo.ipynb /examples/vector_stores/TairIndexDemo.ipynb /examples/vector_stores/SupabaseVectorIndexDemo.ipynb /examples/vector_stores/DocArrayHnswIndexDemo.ipynb /examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb /examples/vector_stores/MongoDBAtlasVectorSearch.ipynb /examples/vector_stores/CassandraIndexDemo.ipynb ```\", \"Modules  We support integrations with OpenAI, Azure, and anything LangChain offers.  ```{toctree} --- maxdepth: 1 --- /examples/embeddings/OpenAI.ipynb /examples/embeddings/Langchain.ipynb /examples/customization/llms/AzureOpenAI.ipynb /examples/embeddings/custom_embeddings.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"set up s3fs AWS_KEY = os.environ[\\'AWS_ACCESS_KEY_ID\\'] AWS_SECRET = os.environ[\\'AWS_SECRET_ACCESS_KEY\\'] R2_ACCOUNT_ID = os.environ[\\'R2_ACCOUNT_ID\\']  assert AWS_KEY is not None and AWS_KEY != \\\\\"\\\\\"  s3 = s3fs.S3FileSystem(    key=AWS_KEY,    secret=AWS_SECRET,    endpoint_url=f\\'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com\\',    s3_additional_kwargs={\\'ACL\\': \\'public-read\\'} )\", \"save index to remote blob storage index.set_index_id(\\\\\"vector_index\\\\\")\", \"this is {bucket_name}/{index_name} index.storage_context.persist(\\'llama-index/storage_demo\\', fs=s3)\", \"load index from s3 sc = StorageContext.from_defaults(persist_dir=\\'llama-index/storage_demo\\', fs=s3) index2 = load_index_from_storage(sc, \\'vector_index\\') ```  By default, if you do not pass a filesystem, we will assume a local filesystem.\", \"Vector Stores  Vector stores contain embedding vectors of ingested document chunks (and sometimes the document chunks as well).\", \"Simple Vector Store  By default, LlamaIndex uses a simple in-memory vector store that\\'s great for quick experimentation. They can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).\", \"Vector Store Options & Feature Support  LlamaIndex supports over 20 different vector store options.We are actively adding more integrations and improving feature coverage for each.| Vector Store             | Type                | Metadata Filtering | Hybrid Search | Delete | Store Documents | Async | |--------------------------|---------------------|--------------------|---------------|--------|-----------------|-------| | Pinecone                 | cloud               | \\\\u2713                  | \\\\u2713             | \\\\u2713      | \\\\u2713               |       | | Weaviate                 | self-hosted / cloud | \\\\u2713                  | \\\\u2713             | \\\\u2713      | \\\\u2713               |       | | Postgres                 | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               | \\\\u2713     | | Cassandra                | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Qdrant                   | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Chroma                   | self-hosted         | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Milvus / Zilliz          | self-hosted / cloud |                    |               | \\\\u2713      | \\\\u2713               |       | | Typesense                | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Supabase                 | self-hosted / cloud | \\\\u2713                  |               |        | \\\\u2713               |       | | MongoDB Atlas            | self-hosted / cloud | \\\\u2713                  |               |\", \"\\\\u2713      | \\\\u2713               |       | | Redis                    | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Deeplake                 | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | OpenSearch               | self-hosted / cloud | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | DynamoDB                 | cloud               |                    |               | \\\\u2713      |                 |       | | LanceDB                  | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Metal                    | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | MyScale                  | cloud               |                    |               |        | \\\\u2713               |       | | Tair                     | cloud               | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       | | Simple                   | in-memory           |                    |               | \\\\u2713      |                 |       | | FAISS                    | in-memory           |                    |               |\", \"|                 |       | | ChatGPT Retrieval Plugin | aggregator          |                    |               | \\\\u2713      | \\\\u2713               |       | | DocArray                 | aggregator          | \\\\u2713                  |               | \\\\u2713      | \\\\u2713               |       |  For more details, see Vector Store Integrations.```{toctree} --- caption: Examples maxdepth: 1 --- /examples/vector_stores/SimpleIndexDemo.ipynb /examples/vector_stores/RocksetIndexDemo.ipynb /examples/vector_stores/QdrantIndexDemo.ipynb /examples/vector_stores/FaissIndexDemo.ipynb /examples/vector_stores/DeepLakeIndexDemo.ipynb /examples/vector_stores/MyScaleIndexDemo.ipynb /examples/vector_stores/MetalIndexDemo.ipynb /examples/vector_stores/WeaviateIndexDemo.ipynb /examples/vector_stores/OpensearchDemo.ipynb /examples/vector_stores/PineconeIndexDemo.ipynb /examples/vector_stores/ChromaIndexDemo.ipynb /examples/vector_stores/LanceDBIndexDemo.ipynb /examples/vector_stores/MilvusIndexDemo.ipynb /examples/vector_stores/RedisIndexDemo.ipynb /examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb /examples/vector_stores/ZepIndexDemo.ipynb /examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb /examples/vector_stores/AsyncIndexCreationDemo.ipynb /examples/vector_stores/TairIndexDemo.ipynb /examples/vector_stores/SupabaseVectorIndexDemo.ipynb /examples/vector_stores/DocArrayHnswIndexDemo.ipynb /examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb /examples/vector_stores/MongoDBAtlasVectorSearch.ipynb /examples/vector_stores/CassandraIndexDemo.ipynb ```\", \"Modules  We support integrations with OpenAI, Azure, and anything LangChain offers.  ```{toctree} --- maxdepth: 1 --- /examples/embeddings/OpenAI.ipynb /examples/embeddings/Langchain.ipynb /examples/customization/llms/AzureOpenAI.ipynb /examples/embeddings/custom_embeddings.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=82 request_id=318c5ba7f0424155f483920784eaf0d0 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=82 request_id=318c5ba7f0424155f483920784eaf0d0 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Embeddings\", \"Concept Embeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!  At a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.  When calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.  There are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain here, as well as providing an easy to extend base class for implementing your own embeddings.\", \"Usage Pattern  Most commonly in LlamaIndex, embedding models will be specified in the `ServiceContext` object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on.  ```python from llama_index import ServiceContext from llama_index.embeddings import OpenAIEmbedding  embed_model = OpenAIEmbedding() service_context = ServiceContext.from_defaults(embed_model=embed_model) ```  To save costs, you may want to use a local model. ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(embed_model=\\\\\"local\\\\\") ``` This will use a well-performing and fast default from Hugging Face.  You can find more usage details and available customization options below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  We support integrations with OpenAI, Azure, and anything LangChain offers. Details below.  ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Usage Pattern\", \"Getting Started  The most common usage for an embedding model will be setting it in the service context object, and then using it to construct an index and query. The input documents will be broken into nodes, and the emedding model will generate an embedding for each node.  By default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.  ```python from llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader from llama_index.embeddings import OpenAIEmbedding  embed_model = OpenAIEmbedding() service_context = serviceContext.from_defaults(embed_model=embed_model)\", \"optionally set a global service context to avoid passing it into other objects every time from llama_index import set_global_service_context set_global_service_context(service_context)  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents) ```  Then, at query time, the embedding model will be used again to embed the query text.  ```python query_engine = index.as_query_engine()  response = query_engine.query(\\\\\"query string\\\\\") ```\", \"Customization\", \"Batch Size  By default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small.  ```python\", \"set the batch size to 42 embed_model = OpenAIEmbedding(embed_batch_size=42) ```  (local-embedding-models)=\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Embeddings\", \"Concept Embeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!  At a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.  When calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.  There are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain here, as well as providing an easy to extend base class for implementing your own embeddings.\", \"Usage Pattern  Most commonly in LlamaIndex, embedding models will be specified in the `ServiceContext` object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on.  ```python from llama_index import ServiceContext from llama_index.embeddings import OpenAIEmbedding  embed_model = OpenAIEmbedding() service_context = ServiceContext.from_defaults(embed_model=embed_model) ```  To save costs, you may want to use a local model. ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(embed_model=\\\\\"local\\\\\") ``` This will use a well-performing and fast default from Hugging Face.  You can find more usage details and available customization options below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  We support integrations with OpenAI, Azure, and anything LangChain offers. Details below.  ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Usage Pattern\", \"Getting Started  The most common usage for an embedding model will be setting it in the service context object, and then using it to construct an index and query. The input documents will be broken into nodes, and the emedding model will generate an embedding for each node.  By default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.  ```python from llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader from llama_index.embeddings import OpenAIEmbedding  embed_model = OpenAIEmbedding() service_context = serviceContext.from_defaults(embed_model=embed_model)\", \"optionally set a global service context to avoid passing it into other objects every time from llama_index import set_global_service_context set_global_service_context(service_context)  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents) ```  Then, at query time, the embedding model will be used again to embed the query text.  ```python query_engine = index.as_query_engine()  response = query_engine.query(\\\\\"query string\\\\\") ```\", \"Customization\", \"Batch Size  By default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small.  ```python\", \"set the batch size to 42 embed_model = OpenAIEmbedding(embed_batch_size=42) ```  (local-embedding-models)=\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=2768be2743a155aea4cf301724603733 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=2768be2743a155aea4cf301724603733 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Local Embedding Models  The easiest way to use a local model is:  ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(embed_model=\\\\\"local\\\\\") ```  To configure the model used (from Hugging Face hub), add the model name separated by a colon:  ```python from llama_index import ServiceContext  service_context = ServiceContext.from_defaults(   embed_model=\\\\\"local:BAAI/bge-large-en\\\\\" ) ```\", \"Embedding Model Integrations  We also support any embeddings offered by Langchain here.  The example below loads a model from Hugging Face, using Langchain\\'s embedding class.  ```python from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings from llama_index import ServiceContext  embed_model = HuggingFaceBgeEmbeddings(model_name=\\\\\"BAAI/bge-base-en\\\\\")  service_context = ServiceContext.from_defaults(embed_model=embed_model) ```\", \"Custom Embedding Model  If you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!  The example below uses Instructor Embeddings (install/setup details here), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \\\\\"instructions\\\\\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.  ```python from typing import Any, List from InstructorEmbedding import INSTRUCTOR from llama_index.embeddings.base import BaseEmbedding  class InstructorEmbeddings(BaseEmbedding):   def __init__(     self,      instructor_model_name: str = \\\\\"hkunlp/instructor-large\\\\\",     instruction: str = \\\\\"Represent the Computer Science documentation or question:\\\\\",     **kwargs: Any,   ) -> None:     self._model = INSTRUCTOR(instructor_model_name)     self._instruction = instruction     super().__init__(**kwargs)      def _get_query_embedding(self, query: str) -> List[float]:       embeddings = self._model.encode([[self._instruction, query]])       return embeddings[0]      def _get_text_embedding(self, text: str) -> List[float]:       embeddings = self._model.encode([[self._instruction, text]])       return embeddings[0]       def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:       embeddings = self._model.encode([[self._instruction, text] for text in texts])       return embeddings ```\", \"Standalone Usage  You can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.  ```python embeddings = embed_model.get_text_embedding(\\\\\"It is raining cats and dogs here!\\\\\") ```\", \"Modules  We support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\", \"OpenAI ```{toctree} --- maxdepth: 1 --- /examples/llm/openai.ipynb /examples/llm/azure_openai.ipynb  ```\", \"Anthropic ```{toctree} --- maxdepth: 1 --- /examples/llm/anthropic.ipynb  ```\", \"Hugging Face ```{toctree} --- maxdepth: 1 --- /examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb /examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb /examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb  ```\", \"PaLM  ```{toctree} --- maxdepth: 1 --- /examples/llm/palm.ipynb  ```\", \"Predibase  ```{toctree} --- maxdepth: 1 --- /examples/llm/predibase.ipynb  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Local Embedding Models  The easiest way to use a local model is:  ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(embed_model=\\\\\"local\\\\\") ```  To configure the model used (from Hugging Face hub), add the model name separated by a colon:  ```python from llama_index import ServiceContext  service_context = ServiceContext.from_defaults(   embed_model=\\\\\"local:BAAI/bge-large-en\\\\\" ) ```\", \"Embedding Model Integrations  We also support any embeddings offered by Langchain here.  The example below loads a model from Hugging Face, using Langchain\\'s embedding class.  ```python from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings from llama_index import ServiceContext  embed_model = HuggingFaceBgeEmbeddings(model_name=\\\\\"BAAI/bge-base-en\\\\\")  service_context = ServiceContext.from_defaults(embed_model=embed_model) ```\", \"Custom Embedding Model  If you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!  The example below uses Instructor Embeddings (install/setup details here), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \\\\\"instructions\\\\\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.  ```python from typing import Any, List from InstructorEmbedding import INSTRUCTOR from llama_index.embeddings.base import BaseEmbedding  class InstructorEmbeddings(BaseEmbedding):   def __init__(     self,      instructor_model_name: str = \\\\\"hkunlp/instructor-large\\\\\",     instruction: str = \\\\\"Represent the Computer Science documentation or question:\\\\\",     **kwargs: Any,   ) -> None:     self._model = INSTRUCTOR(instructor_model_name)     self._instruction = instruction     super().__init__(**kwargs)      def _get_query_embedding(self, query: str) -> List[float]:       embeddings = self._model.encode([[self._instruction, query]])       return embeddings[0]      def _get_text_embedding(self, text: str) -> List[float]:       embeddings = self._model.encode([[self._instruction, text]])       return embeddings[0]       def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:       embeddings = self._model.encode([[self._instruction, text] for text in texts])       return embeddings ```\", \"Standalone Usage  You can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.  ```python embeddings = embed_model.get_text_embedding(\\\\\"It is raining cats and dogs here!\\\\\") ```\", \"Modules  We support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\", \"OpenAI ```{toctree} --- maxdepth: 1 --- /examples/llm/openai.ipynb /examples/llm/azure_openai.ipynb  ```\", \"Anthropic ```{toctree} --- maxdepth: 1 --- /examples/llm/anthropic.ipynb  ```\", \"Hugging Face ```{toctree} --- maxdepth: 1 --- /examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb /examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb /examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb  ```\", \"PaLM  ```{toctree} --- maxdepth: 1 --- /examples/llm/palm.ipynb  ```\", \"Predibase  ```{toctree} --- maxdepth: 1 --- /examples/llm/predibase.ipynb  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=981 request_id=66df18418d6310b3d56c38be314c25d1 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=981 request_id=66df18418d6310b3d56c38be314c25d1 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Replicate  ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_2.ipynb /examples/llm/vicuna.ipynb /examples/vector_stores/SimpleIndexDemoLlama2.ipynb ```\", \"LangChain  ```{toctree} --- maxdepth: 1 --- /examples/llm/langchain.ipynb ```\", \"Llama API ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_api.ipynb ```\", \"Llama CPP  ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_2_llama_cpp.ipynb ```\", \"Xorbits Inference  ```{toctree} --- maxdepth: 1 --- /examples/llm/XinferenceLocalDeployment.ipynb ```\", \"MonsterAPI  ```{toctree} --- maxdepth: 1 --- /examples/llm/monsterapi.ipynb  ```\", \"LLM\", \"Concept Picking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.  LLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.  LlamaIndex provides a unified interface for defining LLM modules, whether it\\'s from OpenAI, Hugging Face, or LangChain, so that you  don\\'t have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below): - Support for **text completion** and **chat** endpoints (details below) - Support for **streaming** and **non-streaming** endpoints - Support for **synchronous** and **asynchronous** endpoints\", \"Usage Pattern  The following code snippet shows how you can get started using LLMs.  ```python from llama_index.llms import OpenAI\", \"non-streaming resp = OpenAI().complete(\\'Paul Graham is \\') print(resp) ```  ```{toctree} --- maxdepth: 1 --- usage_standalone.md usage_custom.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Replicate  ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_2.ipynb /examples/llm/vicuna.ipynb /examples/vector_stores/SimpleIndexDemoLlama2.ipynb ```\", \"LangChain  ```{toctree} --- maxdepth: 1 --- /examples/llm/langchain.ipynb ```\", \"Llama API ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_api.ipynb ```\", \"Llama CPP  ```{toctree} --- maxdepth: 1 --- /examples/llm/llama_2_llama_cpp.ipynb ```\", \"Xorbits Inference  ```{toctree} --- maxdepth: 1 --- /examples/llm/XinferenceLocalDeployment.ipynb ```\", \"MonsterAPI  ```{toctree} --- maxdepth: 1 --- /examples/llm/monsterapi.ipynb  ```\", \"LLM\", \"Concept Picking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.  LLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.  LlamaIndex provides a unified interface for defining LLM modules, whether it\\'s from OpenAI, Hugging Face, or LangChain, so that you  don\\'t have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below): - Support for **text completion** and **chat** endpoints (details below) - Support for **streaming** and **non-streaming** endpoints - Support for **synchronous** and **asynchronous** endpoints\", \"Usage Pattern  The following code snippet shows how you can get started using LLMs.  ```python from llama_index.llms import OpenAI\", \"non-streaming resp = OpenAI().complete(\\'Paul Graham is \\') print(resp) ```  ```{toctree} --- maxdepth: 1 --- usage_standalone.md usage_custom.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=95 request_id=416ab2b969efa00d946f99aade229e5f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=95 request_id=416ab2b969efa00d946f99aade229e5f response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Modules  We support integrations with OpenAI, Hugging Face, PaLM, and more.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Customizing LLMs within LlamaIndex Abstractions  You can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.  By default, we use OpenAI\\'s `text-davinci-003` model. But you may choose to customize the underlying LLM being used.  Below we show a few examples of LLM customization. This includes  - changing the underlying LLM - changing the number of output tokens (for OpenAI, Cohere, or AI21) - having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\", \"Example: Changing the underlying LLM  An example snippet of customizing the LLM being used is shown below. In this example, we use `gpt-4` instead of `text-davinci-003`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.   Note that you may also plug in any LLM shown on Langchain\\'s LLM page.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     LLMPredictor,     ServiceContext ) from llama_index.llms import OpenAI\", \"alternatively  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"define LLM llm = OpenAI(temperature=0.1, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(llm=llm)\", \"build index index = KeywordTableIndex.from_documents(documents, service_context=service_context)\", \"get response from query query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do after his time at Y Combinator?\\\\\")  ```\", \"Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)  The number of output tokens is usually set to some low number by default (for instance, with OpenAI the default is 256).  For OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter (or maxTokens for AI21). We will handle text chunking/calculations under the hood.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     ServiceContext ) from llama_index.llms import OpenAI  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"define LLM llm = OpenAI(temperature=0, model=\\\\\"text-davinci-002\\\\\", max_tokens=512) service_context = ServiceContext.from_defaults(llm=llm)  ```\", \"Example: Explicitly configure `context_window` and `num_output`  If you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     ServiceContext ) from llama_index.llms import OpenAI\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Modules  We support integrations with OpenAI, Hugging Face, PaLM, and more.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Customizing LLMs within LlamaIndex Abstractions  You can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.  By default, we use OpenAI\\'s `text-davinci-003` model. But you may choose to customize the underlying LLM being used.  Below we show a few examples of LLM customization. This includes  - changing the underlying LLM - changing the number of output tokens (for OpenAI, Cohere, or AI21) - having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\", \"Example: Changing the underlying LLM  An example snippet of customizing the LLM being used is shown below. In this example, we use `gpt-4` instead of `text-davinci-003`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`.   Note that you may also plug in any LLM shown on Langchain\\'s LLM page.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     LLMPredictor,     ServiceContext ) from llama_index.llms import OpenAI\", \"alternatively  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"define LLM llm = OpenAI(temperature=0.1, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(llm=llm)\", \"build index index = KeywordTableIndex.from_documents(documents, service_context=service_context)\", \"get response from query query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do after his time at Y Combinator?\\\\\")  ```\", \"Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)  The number of output tokens is usually set to some low number by default (for instance, with OpenAI the default is 256).  For OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter (or maxTokens for AI21). We will handle text chunking/calculations under the hood.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     ServiceContext ) from llama_index.llms import OpenAI  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"define LLM llm = OpenAI(temperature=0, model=\\\\\"text-davinci-002\\\\\", max_tokens=512) service_context = ServiceContext.from_defaults(llm=llm)  ```\", \"Example: Explicitly configure `context_window` and `num_output`  If you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.  ```python  from llama_index import (     KeywordTableIndex,     SimpleDirectoryReader,     ServiceContext ) from llama_index.llms import OpenAI\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=159 request_id=9f1cf166792d18d8d2ef3f01d1b3264f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=159 request_id=9f1cf166792d18d8d2ef3f01d1b3264f response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"alternatively  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"set context window context_window = 4096\", \"set number of output tokens num_output = 256\", \"define LLM llm = OpenAI(     temperature=0,      model=\\\\\"text-davinci-002\\\\\",      max_tokens=num_output, )  service_context = ServiceContext.from_defaults(     llm=llm,     context_window=context_window,     num_output=num_output, )  ```\", \"Example: Using a HuggingFace LLM  LlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.  Many open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.  Below, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found here.  ```python from llama_index.prompts import PromptTemplate  system_prompt = \\\\\"\\\\\"\\\\\"# StableLM Tuned (Alpha version) - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI. - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user. - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes. - StableLM will refuse to participate in anything that could harm a human. \\\\\"\\\\\"\\\\\"\", \"This will wrap the default prompts that are internal to llama-index query_wrapper_prompt = PromptTemplate(\\\\\"{query_str}\\\\\")  import torch from llama_index.llms import HuggingFaceLLM llm = HuggingFaceLLM(     context_window=4096,      max_new_tokens=256,     generate_kwargs={\\\\\"temperature\\\\\": 0.7, \\\\\"do_sample\\\\\": False},     system_prompt=system_prompt,     query_wrapper_prompt=query_wrapper_prompt,     tokenizer_name=\\\\\"StabilityAI/stablelm-tuned-alpha-3b\\\\\",     model_name=\\\\\"StabilityAI/stablelm-tuned-alpha-3b\\\\\",     device_map=\\\\\"auto\\\\\",     stopping_ids=[50278, 50279, 50277, 1, 0],     tokenizer_kwargs={\\\\\"max_length\\\\\": 4096},     # uncomment this if using CUDA to reduce memory usage     # model_kwargs={\\\\\"torch_dtype\\\\\": torch.float16} ) service_context = ServiceContext.from_defaults(     chunk_size=1024,      llm=llm, ) ```  Some models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:  ```python HuggingFaceLLM(     ...     tokenizer_outputs_to_remove=[\\\\\"token_type_ids\\\\\"] ) ```  A full API reference can be found here.  Several example notebooks are also listed below:  - StableLM - Camel\", \"Example: Using a Custom LLM Model - Advanced  To use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface) You will be responsible for passing the text to the model and returning the newly generated tokens.  Note that for a completely private experience, also setup a local embedding model (example here).  Here is a small example using locally running facebook/OPT model and Huggingface\\'s pipeline abstraction:  ```python import torch from transformers import pipeline from typing import Optional, List, Mapping, Any  from llama_index import (     ServiceContext,      SimpleDirectoryReader,      LangchainEmbedding,      ListIndex ) from llama_index.callbacks import CallbackManager from llama_index.llms import (     CustomLLM,      CompletionResponse,      CompletionResponseGen,     LLMMetadata, ) from llama_index.llms.base import llm_completion_callback\", \"set context window size context_window = 2048\", \"set number of output tokens num_output = 256\", \"store the pipeline/model outisde of the LLM class to avoid memory issues model_name = \\\\\"facebook/opt-iml-max-30b\\\\\" pipeline = pipeline(\\\\\"text-generation\\\\\", model=model_name, device=\\\\\"cuda:0\\\\\", model_kwargs={\\\\\"torch_dtype\\\\\":torch.bfloat16})  class OurLLM(CustomLLM):      @property     def metadata(self) -> LLMMetadata:         \\\\\"\\\\\"\\\\\"Get LLM metadata.\\\\\"\\\\\"\\\\\"         return LLMMetadata(             context_window=context_window,             num_output=num_output,             model_name=model_name         )      @llm_completion_callback()     def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:         prompt_length = len(prompt)         response = pipeline(prompt, max_new_tokens=num_output)[0][\\\\\"generated_text\\\\\"]          # only return newly generated tokens         text = response[prompt_length:]         return CompletionResponse(text=text)          @llm_completion_callback()     def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:         raise NotImplementedError()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"alternatively  documents = SimpleDirectoryReader(\\'data\\').load_data()\", \"set context window context_window = 4096\", \"set number of output tokens num_output = 256\", \"define LLM llm = OpenAI(     temperature=0,      model=\\\\\"text-davinci-002\\\\\",      max_tokens=num_output, )  service_context = ServiceContext.from_defaults(     llm=llm,     context_window=context_window,     num_output=num_output, )  ```\", \"Example: Using a HuggingFace LLM  LlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model as in this example.  Many open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.  Below, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found here.  ```python from llama_index.prompts import PromptTemplate  system_prompt = \\\\\"\\\\\"\\\\\"# StableLM Tuned (Alpha version) - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI. - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user. - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes. - StableLM will refuse to participate in anything that could harm a human. \\\\\"\\\\\"\\\\\"\", \"This will wrap the default prompts that are internal to llama-index query_wrapper_prompt = PromptTemplate(\\\\\"{query_str}\\\\\")  import torch from llama_index.llms import HuggingFaceLLM llm = HuggingFaceLLM(     context_window=4096,      max_new_tokens=256,     generate_kwargs={\\\\\"temperature\\\\\": 0.7, \\\\\"do_sample\\\\\": False},     system_prompt=system_prompt,     query_wrapper_prompt=query_wrapper_prompt,     tokenizer_name=\\\\\"StabilityAI/stablelm-tuned-alpha-3b\\\\\",     model_name=\\\\\"StabilityAI/stablelm-tuned-alpha-3b\\\\\",     device_map=\\\\\"auto\\\\\",     stopping_ids=[50278, 50279, 50277, 1, 0],     tokenizer_kwargs={\\\\\"max_length\\\\\": 4096},     # uncomment this if using CUDA to reduce memory usage     # model_kwargs={\\\\\"torch_dtype\\\\\": torch.float16} ) service_context = ServiceContext.from_defaults(     chunk_size=1024,      llm=llm, ) ```  Some models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:  ```python HuggingFaceLLM(     ...     tokenizer_outputs_to_remove=[\\\\\"token_type_ids\\\\\"] ) ```  A full API reference can be found here.  Several example notebooks are also listed below:  - StableLM - Camel\", \"Example: Using a Custom LLM Model - Advanced  To use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface) You will be responsible for passing the text to the model and returning the newly generated tokens.  Note that for a completely private experience, also setup a local embedding model (example here).  Here is a small example using locally running facebook/OPT model and Huggingface\\'s pipeline abstraction:  ```python import torch from transformers import pipeline from typing import Optional, List, Mapping, Any  from llama_index import (     ServiceContext,      SimpleDirectoryReader,      LangchainEmbedding,      ListIndex ) from llama_index.callbacks import CallbackManager from llama_index.llms import (     CustomLLM,      CompletionResponse,      CompletionResponseGen,     LLMMetadata, ) from llama_index.llms.base import llm_completion_callback\", \"set context window size context_window = 2048\", \"set number of output tokens num_output = 256\", \"store the pipeline/model outisde of the LLM class to avoid memory issues model_name = \\\\\"facebook/opt-iml-max-30b\\\\\" pipeline = pipeline(\\\\\"text-generation\\\\\", model=model_name, device=\\\\\"cuda:0\\\\\", model_kwargs={\\\\\"torch_dtype\\\\\":torch.bfloat16})  class OurLLM(CustomLLM):      @property     def metadata(self) -> LLMMetadata:         \\\\\"\\\\\"\\\\\"Get LLM metadata.\\\\\"\\\\\"\\\\\"         return LLMMetadata(             context_window=context_window,             num_output=num_output,             model_name=model_name         )      @llm_completion_callback()     def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:         prompt_length = len(prompt)         response = pipeline(prompt, max_new_tokens=num_output)[0][\\\\\"generated_text\\\\\"]          # only return newly generated tokens         text = response[prompt_length:]         return CompletionResponse(text=text)          @llm_completion_callback()     def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:         raise NotImplementedError()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=ab5f66829b1de65edec2304688da2653 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=ab5f66829b1de65edec2304688da2653 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"define our LLM llm = OurLLM()  service_context = ServiceContext.from_defaults(     llm=llm,      context_window=context_window,      num_output=num_output )\", \"Load the your data documents = SimpleDirectoryReader(\\'./data\\').load_data() index = ListIndex.from_documents(documents, service_context=service_context)\", \"Query and print response query_engine = index.as_query_engine() response = query_engine.query(\\\\\"\\\\\") print(response) ```  Using this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.  The decorator is optional, but provides observability via callbacks on the LLM calls.  Note that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it\\'s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.  A list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\", \"Using LLMs as standalone modules  You can use our LLM modules on their own.\", \"Text Completion Example  ```python from llama_index.llms import OpenAI\", \"non-streaming resp = OpenAI().complete(\\'Paul Graham is \\') print(resp)\", \"using streaming endpoint from llama_index.llms import OpenAI llm = OpenAI() resp = llm.stream_complete(\\'Paul Graham is \\') for delta in resp:     print(delta, end=\\'\\') ```\", \"Chat Example  ```python from llama_index.llms import ChatMessage, OpenAI  messages = [     ChatMessage(role=\\\\\"system\\\\\", content=\\\\\"You are a pirate with a colorful personality\\\\\"),     ChatMessage(role=\\\\\"user\\\\\", content=\\\\\"What is your name\\\\\"), ] resp = OpenAI().chat(messages) print(resp) ```  Check out our modules section for usage guides for each LLM.\", \"Prompts\", \"Concept  Prompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion,  perform traversal during querying, and to synthesize the final answer.  LlamaIndex uses a set of default prompt templates that work well out of the box.  In addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` here.  Users may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"define our LLM llm = OurLLM()  service_context = ServiceContext.from_defaults(     llm=llm,      context_window=context_window,      num_output=num_output )\", \"Load the your data documents = SimpleDirectoryReader(\\'./data\\').load_data() index = ListIndex.from_documents(documents, service_context=service_context)\", \"Query and print response query_engine = index.as_query_engine() response = query_engine.query(\\\\\"\\\\\") print(response) ```  Using this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.  The decorator is optional, but provides observability via callbacks on the LLM calls.  Note that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it\\'s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.  A list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.\", \"Using LLMs as standalone modules  You can use our LLM modules on their own.\", \"Text Completion Example  ```python from llama_index.llms import OpenAI\", \"non-streaming resp = OpenAI().complete(\\'Paul Graham is \\') print(resp)\", \"using streaming endpoint from llama_index.llms import OpenAI llm = OpenAI() resp = llm.stream_complete(\\'Paul Graham is \\') for delta in resp:     print(delta, end=\\'\\') ```\", \"Chat Example  ```python from llama_index.llms import ChatMessage, OpenAI  messages = [     ChatMessage(role=\\\\\"system\\\\\", content=\\\\\"You are a pirate with a colorful personality\\\\\"),     ChatMessage(role=\\\\\"user\\\\\", content=\\\\\"What is your name\\\\\"), ] resp = OpenAI().chat(messages) print(resp) ```  Check out our modules section for usage guides for each LLM.\", \"Prompts\", \"Concept  Prompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion,  perform traversal during querying, and to synthesize the final answer.  LlamaIndex uses a set of default prompt templates that work well out of the box.  In addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` here.  Users may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=81 request_id=39f8195ddae8b0163c1ca74dfe5f9dc6 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=81 request_id=39f8195ddae8b0163c1ca74dfe5f9dc6 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern\", \"Defining a custom prompt  Defining a custom prompt is as simple as creating a format string  ```python from llama_index.prompts import PromptTemplate  template = (     \\\\\"We have provided context information below. \\\\\\\\n\\\\\"     \\\\\"---------------------\\\\\\\\n\\\\\"     \\\\\"{context_str}\\\\\"     \\\\\"\\\\\\\\n---------------------\\\\\\\\n\\\\\"     \\\\\"Given this information, please answer the question: {query_str}\\\\\\\\n\\\\\" ) qa_template = PromptTemplate(template)\", \"you can create text prompt (for completion API) prompt = qa_template.format(context_str=..., query_str=...)\", \"or easily convert to message prompts (for chat API) messages = qa_template.format_messages(context_str=..., query_str=...) ```  > Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `PromptTemplate`). Now you can directly specify `PromptTemplate(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.  You can also define a template from chat messages  ```python from llama_index.prompts import ChatPromptTemplate, ChatMessage, MessageRole  message_templates = [     ChatMessage(content=\\\\\"You are an expert system.\\\\\", role=MessageRole.SYSTEM),     ChatMessage(         content=\\\\\"Generate a short story about {topic}\\\\\",         role=MessageRole.USER,     ), ] chat_template = ChatPromptTemplate(message_templates=message_templates)\", \"you can create message prompts (for chat API) messages = chat_template.format_messages(topic=...)\", \"or easily convert to text prompt (for completion API) prompt = chat_template.format(topic=...) ```\", \"Passing custom prompts into the pipeline  Since LlamaIndex is a multi-step pipeline, it\\'s important to identify the operation that you want to modify and pass in the custom prompt at the right place.  At a high-level, prompts are used in 1) index construction, and 2) query engine execution  The most commonly used prompts will be the `text_qa_template` and the `refine_template`.   - `text_qa_template` - used to get an initial answer to a query using retrieved nodes - `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\\\\\"compact\\\\\"` (the default), or when more than one node is retrieved using `response_mode=\\\\\"refine\\\\\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\", \"Modify prompts used in index construction Different indices use different types of prompts during construction (some don\\'t use prompts at all).  For instance, `TreeIndex` uses a summary prompt to hierarchically summarize the nodes, and `KeywordTableIndex` uses a keyword extract prompt to extract keywords.  There are two equivalent ways to override the prompts:  1. via the default nodes constructor   ```python index = TreeIndex(nodes, summary_template=) ``` 2. via the documents constructor.  ```python index = TreeIndex.from_documents(docs, summary_template=) ```  For more details on which index uses which prompts, please visit Index class references.\", \"Modify prompts used in query engine More commonly, prompts are used at query-time (i.e. for executing a query against an index and synthesizing the final response).   There are also two equivalent ways to override the prompts:  1. via the high-level API ```python query_engine = index.as_query_engine(     text_qa_template=,     refine_template= ) ``` 2. via the low-level composition API  ```python retriever = index.as_retriever() synth = get_response_synthesizer(     text_qa_template=,     refine_template= ) query_engine = RetrieverQueryEngine(retriever, response_synthesizer) ```  The two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity. You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.   For more details on which classes use which prompts, please visit Query class references.  Check out the reference documentation for a full set of all prompts.\", \"Modules  ```{toctree} --- maxdepth: 1 --- /examples/customization/prompts/completion_prompts.ipynb /examples/customization/prompts/chat_prompts.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern\", \"Defining a custom prompt  Defining a custom prompt is as simple as creating a format string  ```python from llama_index.prompts import PromptTemplate  template = (     \\\\\"We have provided context information below. \\\\\\\\n\\\\\"     \\\\\"---------------------\\\\\\\\n\\\\\"     \\\\\"{context_str}\\\\\"     \\\\\"\\\\\\\\n---------------------\\\\\\\\n\\\\\"     \\\\\"Given this information, please answer the question: {query_str}\\\\\\\\n\\\\\" ) qa_template = PromptTemplate(template)\", \"you can create text prompt (for completion API) prompt = qa_template.format(context_str=..., query_str=...)\", \"or easily convert to message prompts (for chat API) messages = qa_template.format_messages(context_str=..., query_str=...) ```  > Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `PromptTemplate`). Now you can directly specify `PromptTemplate(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.  You can also define a template from chat messages  ```python from llama_index.prompts import ChatPromptTemplate, ChatMessage, MessageRole  message_templates = [     ChatMessage(content=\\\\\"You are an expert system.\\\\\", role=MessageRole.SYSTEM),     ChatMessage(         content=\\\\\"Generate a short story about {topic}\\\\\",         role=MessageRole.USER,     ), ] chat_template = ChatPromptTemplate(message_templates=message_templates)\", \"you can create message prompts (for chat API) messages = chat_template.format_messages(topic=...)\", \"or easily convert to text prompt (for completion API) prompt = chat_template.format(topic=...) ```\", \"Passing custom prompts into the pipeline  Since LlamaIndex is a multi-step pipeline, it\\'s important to identify the operation that you want to modify and pass in the custom prompt at the right place.  At a high-level, prompts are used in 1) index construction, and 2) query engine execution  The most commonly used prompts will be the `text_qa_template` and the `refine_template`.   - `text_qa_template` - used to get an initial answer to a query using retrieved nodes - `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\\\\\"compact\\\\\"` (the default), or when more than one node is retrieved using `response_mode=\\\\\"refine\\\\\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\", \"Modify prompts used in index construction Different indices use different types of prompts during construction (some don\\'t use prompts at all).  For instance, `TreeIndex` uses a summary prompt to hierarchically summarize the nodes, and `KeywordTableIndex` uses a keyword extract prompt to extract keywords.  There are two equivalent ways to override the prompts:  1. via the default nodes constructor   ```python index = TreeIndex(nodes, summary_template=) ``` 2. via the documents constructor.  ```python index = TreeIndex.from_documents(docs, summary_template=) ```  For more details on which index uses which prompts, please visit Index class references.\", \"Modify prompts used in query engine More commonly, prompts are used at query-time (i.e. for executing a query against an index and synthesizing the final response).   There are also two equivalent ways to override the prompts:  1. via the high-level API ```python query_engine = index.as_query_engine(     text_qa_template=,     refine_template= ) ``` 2. via the low-level composition API  ```python retriever = index.as_retriever() synth = get_response_synthesizer(     text_qa_template=,     refine_template= ) query_engine = RetrieverQueryEngine(retriever, response_synthesizer) ```  The two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity. You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.   For more details on which classes use which prompts, please visit Query class references.  Check out the reference documentation for a full set of all prompts.\", \"Modules  ```{toctree} --- maxdepth: 1 --- /examples/customization/prompts/completion_prompts.ipynb /examples/customization/prompts/chat_prompts.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=82 request_id=e76ef18efdf640ed6e301280dc7475d9 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=82 request_id=e76ef18efdf640ed6e301280dc7475d9 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Module Guides  We provide a few simple implementations to start, with more sophisticated modes coming soon!    More specifically, the `SimpleChatEngine` does not make use of a knowledge base,  whereas all others make use of a query engine over knowledge base.  ```{toctree} --- maxdepth: 1 --- ReAct Chat Engine  OpenAI Chat Engine  Context Chat Engine  Condense Question Chat Engine  Simple Chat Engine  ```\", \"Chat Engine\", \"Concept Chat engine is a high-level interface for having a conversation with your data (multiple back-and-forth instead of a single question & answer). Think ChatGPT, but augmented with your knowledge base.    Conceptually, it is a **stateful** analogy of a Query Engine.  By keeping track of the conversation history, it can answer questions with past context in mind.     ```{tip} If you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.   ```\", \"Usage Pattern Get started with: ```python chat_engine = index.as_chat_engine() response = chat_engine.chat(\\\\\"Tell me a joke.\\\\\") ```  To stream response: ```python chat_engine = index.as_chat_engine() streaming_response = chat_engine.stream_chat(\\\\\"Tell me a joke.\\\\\") for token in streaming_response.response_gen:     print(token, end=\\\\\"\\\\\") ```   ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find corresponding tutorials to see the available chat engines in action.   ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started  Build a chat engine from index: ```python chat_engine = index.as_chat_engine() ```  ```{tip} To learn how to build an index, see Index ```  Have a conversation with your data: ```python response = chat_engine.chat(\\\\\"Tell me a joke.\\\\\") ```  Reset chat history to start a new conversation: ```python chat_engine.reset() ```  Enter an interactive chat REPL: ```python chat_engine.chat_repl() ```\", \"Configuring a Chat Engine Configuring a chat engine is very similar to configuring a query engine.\", \"High-Level API You can directly build and configure a chat engine from an index in 1 line of code: ```python chat_engine = index.as_chat_engine(     chat_mode=\\'condense_question\\',      verbose=True ) ``` > Note: you can access different chat engines by specifying the `chat_mode` as a kwarg. `condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`, `context` corresponds to a `ContextChatEngine`.  > Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.\", \"Available Chat Modes  - `best` - Turn the query engine into a tool, for use with a `ReAct` data agent or an `OpenAI` data agent, depending on what your LLM supports. `OpenAI` data agents require `gpt-3.5-turbo` or `gpt-4` as they use the function calling API from OpenAI. - `context` - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine. - `condense_question` - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine. - `simple` - A simple chat with the LLM directly, no query engine involved. - `react` - Same as `best`, but forces a `ReAct` data agent. - `openai` - Same as `best`, but forces an `OpenAI` data agent.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Module Guides  We provide a few simple implementations to start, with more sophisticated modes coming soon!    More specifically, the `SimpleChatEngine` does not make use of a knowledge base,  whereas all others make use of a query engine over knowledge base.  ```{toctree} --- maxdepth: 1 --- ReAct Chat Engine  OpenAI Chat Engine  Context Chat Engine  Condense Question Chat Engine  Simple Chat Engine  ```\", \"Chat Engine\", \"Concept Chat engine is a high-level interface for having a conversation with your data (multiple back-and-forth instead of a single question & answer). Think ChatGPT, but augmented with your knowledge base.    Conceptually, it is a **stateful** analogy of a Query Engine.  By keeping track of the conversation history, it can answer questions with past context in mind.     ```{tip} If you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.   ```\", \"Usage Pattern Get started with: ```python chat_engine = index.as_chat_engine() response = chat_engine.chat(\\\\\"Tell me a joke.\\\\\") ```  To stream response: ```python chat_engine = index.as_chat_engine() streaming_response = chat_engine.stream_chat(\\\\\"Tell me a joke.\\\\\") for token in streaming_response.response_gen:     print(token, end=\\\\\"\\\\\") ```   ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find corresponding tutorials to see the available chat engines in action.   ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started  Build a chat engine from index: ```python chat_engine = index.as_chat_engine() ```  ```{tip} To learn how to build an index, see Index ```  Have a conversation with your data: ```python response = chat_engine.chat(\\\\\"Tell me a joke.\\\\\") ```  Reset chat history to start a new conversation: ```python chat_engine.reset() ```  Enter an interactive chat REPL: ```python chat_engine.chat_repl() ```\", \"Configuring a Chat Engine Configuring a chat engine is very similar to configuring a query engine.\", \"High-Level API You can directly build and configure a chat engine from an index in 1 line of code: ```python chat_engine = index.as_chat_engine(     chat_mode=\\'condense_question\\',      verbose=True ) ``` > Note: you can access different chat engines by specifying the `chat_mode` as a kwarg. `condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`, `context` corresponds to a `ContextChatEngine`.  > Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.\", \"Available Chat Modes  - `best` - Turn the query engine into a tool, for use with a `ReAct` data agent or an `OpenAI` data agent, depending on what your LLM supports. `OpenAI` data agents require `gpt-3.5-turbo` or `gpt-4` as they use the function calling API from OpenAI. - `context` - Retrieve nodes from the index using every user message. The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine. - `condense_question` - Look at the chat history and re-write the user message to be a query for the index. Return the response after reading the response from the query engine. - `simple` - A simple chat with the LLM directly, no query engine involved. - `react` - Same as `best`, but forces a `ReAct` data agent. - `openai` - Same as `best`, but forces an `OpenAI` data agent.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=190 request_id=b9a516cb462069209720feccffd7f32f response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=190 request_id=b9a516cb462069209720feccffd7f32f response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Low-Level Composition API  You can use the low-level composition API if you need more granular control. Concretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`. > Note: You may need to look at API references or example notebooks.  Here\\'s an example where we configure the following: * configure the condense question prompt,  * initialize the conversation with some existing history, * print verbose debug message.  ```python from llama_index.prompts  import PromptTemplate from llama_index.llms import ChatMessage, MessageRole  custom_prompt = PromptTemplate(\\\\\"\\\\\"\\\\\"\\\\\\\\ Given a conversation (between Human and Assistant) and a follow up message from Human, \\\\\\\\ rewrite the message to be a standalone question that captures all relevant context \\\\\\\\ from the conversation.    {chat_history}   {question}   \\\\\"\\\\\"\\\\\")\", \"list of `ChatMessage` objects custom_chat_history = [     ChatMessage(         role=MessageRole.USER,          content=\\'Hello assistant, we are having a insightful discussion about Paul Graham today.\\'     ),      ChatMessage(         role=MessageRole.ASSISTANT,          content=\\'Okay, sounds good.\\'     ) ]  query_engine = index.as_query_engine() chat_engine = CondenseQuestionChatEngine.from_defaults(     query_engine=query_engine,      condense_question_prompt=custom_prompt,     chat_history=custom_chat_history,     verbose=True ) ```\", \"Streaming To enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.   ```{warning} This somewhat inconsistent with query engine (where you pass in a `streaming=True` flag). We are working on making the behavior more consistent!  ```  ```python chat_engine = index.as_chat_engine() streaming_response = chat_engine.stream_chat(\\\\\"Tell me a joke.\\\\\") for token in streaming_response.response_gen:     print(token, end=\\\\\"\\\\\") ```  See an end-to-end tutorial\", \"Modules\", \"SimilarityPostprocessor  Used to remove nodes that are below a similarity score threshold.  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor  postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)  postprocessor.postprocess_nodes(nodes) ```\", \"KeywordNodePostprocessor  Used to ensure certain keywords are either excluded or included.  ```python from llama_index.indices.postprocessor import KeywordNodePostprocessor  postprocessor = KeywordNodePostprocessor(   required_keywords=[\\\\\"word1\\\\\", \\\\\"word2\\\\\"],   exclude_keywords=[\\\\\"word3\\\\\", \\\\\"word4\\\\\"] )  postprocessor.postprocess_nodes(nodes) ```\", \"MetadataReplacementPostProcessor  Used to replace the node content with a field from the node metadata. If the field is not present in the metadata, then the node text remains unchanged. Most useful when used in combination with the `SentenceWindowNodeParser`.  ```python from llama_index.indices.postprocessor import MetadataReplacementPostProcessor  postprocessor = KeywordNodePostprocessor(   target_metadata_key=\\\\\"window\\\\\", )  postprocessor.postprocess_nodes(nodes) ```\", \"SentenceEmbeddingOptimizer  This postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).  The percentile cutoff is a measure for using the top percentage of relevant sentences.  The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.  ```python from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer  postprocessor = SentenceEmbeddingOptimizer(   embed_model=service_context.embed_model,   percentile_cutoff=0.5,   # threshold_cutoff=0.7 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide can be found here\", \"CohereRerank  Uses the \\\\\"Cohere ReRank\\\\\" functionality to re-order nodes, and returns the top N nodes.  ```python from llama_index.indices import CohereRerank  postprocessor = CohereRerank(   top_n=2   model=\\\\\"rerank-english-v2.0\\\\\",   api_key=\\\\\"YOUR COHERE API KEY\\\\\" )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available here.\", \"SentenceTransformerRerank  Uses the cross-encoders from the `sentence-transformer` package to re-order nodes, and returns the top N nodes.  ```python from llama_index.indices.postprocessor import SentenceTransformerRerank\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Low-Level Composition API  You can use the low-level composition API if you need more granular control. Concretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`. > Note: You may need to look at API references or example notebooks.  Here\\'s an example where we configure the following: * configure the condense question prompt,  * initialize the conversation with some existing history, * print verbose debug message.  ```python from llama_index.prompts  import PromptTemplate from llama_index.llms import ChatMessage, MessageRole  custom_prompt = PromptTemplate(\\\\\"\\\\\"\\\\\"\\\\\\\\ Given a conversation (between Human and Assistant) and a follow up message from Human, \\\\\\\\ rewrite the message to be a standalone question that captures all relevant context \\\\\\\\ from the conversation.    {chat_history}   {question}   \\\\\"\\\\\"\\\\\")\", \"list of `ChatMessage` objects custom_chat_history = [     ChatMessage(         role=MessageRole.USER,          content=\\'Hello assistant, we are having a insightful discussion about Paul Graham today.\\'     ),      ChatMessage(         role=MessageRole.ASSISTANT,          content=\\'Okay, sounds good.\\'     ) ]  query_engine = index.as_query_engine() chat_engine = CondenseQuestionChatEngine.from_defaults(     query_engine=query_engine,      condense_question_prompt=custom_prompt,     chat_history=custom_chat_history,     verbose=True ) ```\", \"Streaming To enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.   ```{warning} This somewhat inconsistent with query engine (where you pass in a `streaming=True` flag). We are working on making the behavior more consistent!  ```  ```python chat_engine = index.as_chat_engine() streaming_response = chat_engine.stream_chat(\\\\\"Tell me a joke.\\\\\") for token in streaming_response.response_gen:     print(token, end=\\\\\"\\\\\") ```  See an end-to-end tutorial\", \"Modules\", \"SimilarityPostprocessor  Used to remove nodes that are below a similarity score threshold.  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor  postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)  postprocessor.postprocess_nodes(nodes) ```\", \"KeywordNodePostprocessor  Used to ensure certain keywords are either excluded or included.  ```python from llama_index.indices.postprocessor import KeywordNodePostprocessor  postprocessor = KeywordNodePostprocessor(   required_keywords=[\\\\\"word1\\\\\", \\\\\"word2\\\\\"],   exclude_keywords=[\\\\\"word3\\\\\", \\\\\"word4\\\\\"] )  postprocessor.postprocess_nodes(nodes) ```\", \"MetadataReplacementPostProcessor  Used to replace the node content with a field from the node metadata. If the field is not present in the metadata, then the node text remains unchanged. Most useful when used in combination with the `SentenceWindowNodeParser`.  ```python from llama_index.indices.postprocessor import MetadataReplacementPostProcessor  postprocessor = KeywordNodePostprocessor(   target_metadata_key=\\\\\"window\\\\\", )  postprocessor.postprocess_nodes(nodes) ```\", \"SentenceEmbeddingOptimizer  This postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).  The percentile cutoff is a measure for using the top percentage of relevant sentences.  The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.  ```python from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer  postprocessor = SentenceEmbeddingOptimizer(   embed_model=service_context.embed_model,   percentile_cutoff=0.5,   # threshold_cutoff=0.7 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide can be found here\", \"CohereRerank  Uses the \\\\\"Cohere ReRank\\\\\" functionality to re-order nodes, and returns the top N nodes.  ```python from llama_index.indices import CohereRerank  postprocessor = CohereRerank(   top_n=2   model=\\\\\"rerank-english-v2.0\\\\\",   api_key=\\\\\"YOUR COHERE API KEY\\\\\" )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available here.\", \"SentenceTransformerRerank  Uses the cross-encoders from the `sentence-transformer` package to re-order nodes, and returns the top N nodes.  ```python from llama_index.indices.postprocessor import SentenceTransformerRerank\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=371 request_id=2055e203979a353c1575037000ccd619 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=371 request_id=2055e203979a353c1575037000ccd619 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"We choose a model with relatively high speed and decent accuracy. postprocessor = SentenceTransformerRerank(   model=\\\\\"cross-encoder/ms-marco-MiniLM-L-2-v2\\\\\",    top_n=3 )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available here.  Please also refer to the `sentence-transformer` docs for a more complete list of models (and also shows tradeoffs in speed/accuracy). The default model is `cross-encoder/ms-marco-TinyBERT-L-2-v2`, which provides the most speed.\", \"LLM Rerank  Uses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are. Returns the top N ranked nodes.  ```python from llama_index.indices.postprocessor import LLMRerank  postprocessor = LLMRerank(   top_n=2   service_context=service_context, )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available her for Gatsby and here for Lyft 10K documents.\", \"FixedRecencyPostprocessor  This postproccesor returns the top K nodes sorted by date. This assumes there is a `date` field to parse in the metadata of each node.  ```python from llama_index.indices.postprocessor import FixedRecencyPostprocessor  postprocessor = FixedRecencyPostprocessor(   tok_k=1,   date_key=\\\\\"date\\\\\"  # the key in the metadata to find the date )  postprocessor.postprocess_nodes(nodes) ```  !  A full notebook guide is available here.\", \"EmbeddingRecencyPostprocessor  This postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.  ```python from llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor  postprocessor = EmbeddingRecencyPostprocessor(   service_context=service_context,   date_key=\\\\\"date\\\\\",   similarity_cutoff=0.7 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide is available here.\", \"TimeWeightedPostprocessor  This postproccesor returns the top K nodes applying a time-weighted rerank to each node. Each time a node is retrieved, the time it was retrieved is recorded. This biases search to favor information that has not be returned in a query yet.  ```python from llama_index.indices.postprocessor import TimeWeightedPostprocessor  postprocessor = TimeWeightedPostprocessor(   time_decay=0.99,   top_k=1 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide is available here.\", \"(Beta) PIINodePostprocessor  The PII (Personal Identifiable Information) postprocssor removes information that might be a security risk. It does this by using NER (either with a dedicated NER model, or with a local LLM model).\", \"LLM Version  ```python from llama_index.indices.postprocessor import PIINodePostprocessor  postprocessor = PIINodePostprocessor(   service_context=service_context,  # this should be setup with an LLM you trust )  postprocessor.postprocess_nodes(nodes) ```\", \"NER Version  This version uses the default local model from Hugging Face that is loaded when you run `pipline(\\\\\"ner\\\\\")`.  ```python from llama_index.indices.postprocessor import NERPIINodePostprocessor  postprocessor = NERPIINodePostprocessor()  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide for both can be found here.\", \"(Beta) PrevNextNodePostprocessor  Uses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.  This is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.  ```python from llama_index.indices.postprocessor import PrevNextNodePostprocessor  postprocessor = PrevNextNodePostprocessor(   docstore=index.docstore,   num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards   mode=\\\\\"next\\\\\"   # can be either \\'next\\', \\'previous\\', or \\'both\\' )  postprocessor.postprocess_nodes(nodes) ```  !\", \"(Beta) AutoPrevNextNodePostprocessor  The same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).  ```python from llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor  postprocessor = AutoPrevNextNodePostprocessor(   docstore=index.docstore,   service_context=service_context   num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)  postprocessor.postprocess_nodes(nodes) ```  A full example notebook is available here.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"We choose a model with relatively high speed and decent accuracy. postprocessor = SentenceTransformerRerank(   model=\\\\\"cross-encoder/ms-marco-MiniLM-L-2-v2\\\\\",    top_n=3 )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available here.  Please also refer to the `sentence-transformer` docs for a more complete list of models (and also shows tradeoffs in speed/accuracy). The default model is `cross-encoder/ms-marco-TinyBERT-L-2-v2`, which provides the most speed.\", \"LLM Rerank  Uses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are. Returns the top N ranked nodes.  ```python from llama_index.indices.postprocessor import LLMRerank  postprocessor = LLMRerank(   top_n=2   service_context=service_context, )  postprocessor.postprocess_nodes(nodes) ```  Full notebook guide is available her for Gatsby and here for Lyft 10K documents.\", \"FixedRecencyPostprocessor  This postproccesor returns the top K nodes sorted by date. This assumes there is a `date` field to parse in the metadata of each node.  ```python from llama_index.indices.postprocessor import FixedRecencyPostprocessor  postprocessor = FixedRecencyPostprocessor(   tok_k=1,   date_key=\\\\\"date\\\\\"  # the key in the metadata to find the date )  postprocessor.postprocess_nodes(nodes) ```  !  A full notebook guide is available here.\", \"EmbeddingRecencyPostprocessor  This postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.  ```python from llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor  postprocessor = EmbeddingRecencyPostprocessor(   service_context=service_context,   date_key=\\\\\"date\\\\\",   similarity_cutoff=0.7 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide is available here.\", \"TimeWeightedPostprocessor  This postproccesor returns the top K nodes applying a time-weighted rerank to each node. Each time a node is retrieved, the time it was retrieved is recorded. This biases search to favor information that has not be returned in a query yet.  ```python from llama_index.indices.postprocessor import TimeWeightedPostprocessor  postprocessor = TimeWeightedPostprocessor(   time_decay=0.99,   top_k=1 )  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide is available here.\", \"(Beta) PIINodePostprocessor  The PII (Personal Identifiable Information) postprocssor removes information that might be a security risk. It does this by using NER (either with a dedicated NER model, or with a local LLM model).\", \"LLM Version  ```python from llama_index.indices.postprocessor import PIINodePostprocessor  postprocessor = PIINodePostprocessor(   service_context=service_context,  # this should be setup with an LLM you trust )  postprocessor.postprocess_nodes(nodes) ```\", \"NER Version  This version uses the default local model from Hugging Face that is loaded when you run `pipline(\\\\\"ner\\\\\")`.  ```python from llama_index.indices.postprocessor import NERPIINodePostprocessor  postprocessor = NERPIINodePostprocessor()  postprocessor.postprocess_nodes(nodes) ```  A full notebook guide for both can be found here.\", \"(Beta) PrevNextNodePostprocessor  Uses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.  This is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.  ```python from llama_index.indices.postprocessor import PrevNextNodePostprocessor  postprocessor = PrevNextNodePostprocessor(   docstore=index.docstore,   num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards   mode=\\\\\"next\\\\\"   # can be either \\'next\\', \\'previous\\', or \\'both\\' )  postprocessor.postprocess_nodes(nodes) ```  !\", \"(Beta) AutoPrevNextNodePostprocessor  The same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).  ```python from llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor  postprocessor = AutoPrevNextNodePostprocessor(   docstore=index.docstore,   service_context=service_context   num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)  postprocessor.postprocess_nodes(nodes) ```  A full example notebook is available here.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=2df199416671bc053543ef8ddb54bbd4 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=2df199416671bc053543ef8ddb54bbd4 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"All Notebooks  ```{toctree} --- maxdepth: 1 --- /examples/node_postprocessor/OptimizerDemo.ipynb /examples/node_postprocessor/CohereRerank.ipynb /examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb /examples/node_postprocessor/LLMReranker-Gatsby.ipynb /examples/node_postprocessor/RecencyPostprocessorDemo.ipynb /examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb /examples/node_postprocessor/PII.ipynb /examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb /examples/node_postprocessor/MetadataReplacementDemo.ipynb ```\", \"Node Postprocessor\", \"Concept Node postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.  In LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.  LlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.  ```{tip} Confused about where node postprocessor fits in the pipeline? Read about high-level concepts ```\", \"Usage Pattern  An example of using a node postprocessors is below:  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor from llama_index.schema import Node, NodeWithScore  nodes = [   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.7),   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.8) ]\", \"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```  You can find more details using post processors and how to build your own below.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find guides for each node postprocessor.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern  Most commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\", \"Using with a Query Engine  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.indices.postprocessor import TimeWeightedPostprocessor  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents)  query_engine = index.as_query_engine(   node_postprocessors=[     TimeWeightedPostprocessor(         time_decay=0.5, time_access_refresh=False, top_k=1     )   ] )\", \"all node post-processors will be applied during each query response = query_engine.query(\\\\\"query string\\\\\") ```\", \"Using with Retrieved Nodes  Or used as a standalone object for filtering retrieved nodes:  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor  nodes = index.as_retriever().retrieve(\\\\\"test query str\\\\\")\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"All Notebooks  ```{toctree} --- maxdepth: 1 --- /examples/node_postprocessor/OptimizerDemo.ipynb /examples/node_postprocessor/CohereRerank.ipynb /examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb /examples/node_postprocessor/LLMReranker-Gatsby.ipynb /examples/node_postprocessor/RecencyPostprocessorDemo.ipynb /examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb /examples/node_postprocessor/PII.ipynb /examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb /examples/node_postprocessor/MetadataReplacementDemo.ipynb ```\", \"Node Postprocessor\", \"Concept Node postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.  In LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.  LlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.  ```{tip} Confused about where node postprocessor fits in the pipeline? Read about high-level concepts ```\", \"Usage Pattern  An example of using a node postprocessors is below:  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor from llama_index.schema import Node, NodeWithScore  nodes = [   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.7),   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.8) ]\", \"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```  You can find more details using post processors and how to build your own below.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find guides for each node postprocessor.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern  Most commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\", \"Using with a Query Engine  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.indices.postprocessor import TimeWeightedPostprocessor  documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents)  query_engine = index.as_query_engine(   node_postprocessors=[     TimeWeightedPostprocessor(         time_decay=0.5, time_access_refresh=False, top_k=1     )   ] )\", \"all node post-processors will be applied during each query response = query_engine.query(\\\\\"query string\\\\\") ```\", \"Using with Retrieved Nodes  Or used as a standalone object for filtering retrieved nodes:  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor  nodes = index.as_retriever().retrieve(\\\\\"test query str\\\\\")\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=047245cb34e76660b9495489e705b859 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=047245cb34e76660b9495489e705b859 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```\", \"Using with your own nodes  As you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor from llama_index.schema import Node, NodeWithScore  nodes = [   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.7),   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.8) ]\", \"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```\", \"Custom Node PostProcessor  The base class is `BaseNodePostprocessor`, and the API interface is very simple:   ```python class BaseNodePostprocessor:     \\\\\"\\\\\"\\\\\"Node postprocessor.\\\\\"\\\\\"\\\\\"      @abstractmethod     def postprocess_nodes(         self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]     ) -> List[NodeWithScore]:         \\\\\"\\\\\"\\\\\"Postprocess nodes.\\\\\"\\\\\"\\\\\" ```  A dummy node-postprocessor can be implemented in just a few lines of code:  ```python from llama_index import QueryBundle from llama_index.indices.postprocessor.base import BaseNodePostprocessor from llama_index.schema import NodeWithScore  class DummyNodePostprocessor:      def postprocess_nodes(         self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]     ) -> List[NodeWithScore]:                  # subtracts 1 from the score         for n in nodes:             n.score -= 1          return nodes ```\", \"Query Transformations   LlamaIndex allows you to perform *query transformations* over your index structures. Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index.   They can also be **multi-step**, as in:  1. The query is transformed, executed against an index,  2. The response is retrieved. 3. Subsequent queries are transformed/executed in a sequential fashion.  We list some of our query transformations in more detail below.\", \"Use Cases Query transformations have multiple use cases: - Transforming an initial query into a form that can be more easily embedded (e.g. HyDE) - Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition) - Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\", \"HyDE (Hypothetical Document Embeddings)  HyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.  To use HyDE, an example code snippet is shown below.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.indices.query.query_transform.base import HyDEQueryTransform from llama_index.query_engine.transform_query_engine import TransformQueryEngine\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex(documents)\", \"run query with HyDE query transform query_str = \\\\\"what did paul graham do after going to RISD\\\\\" hyde = HyDEQueryTransform(include_original=True) query_engine = index.as_query_engine() query_engine = TransformQueryEngine(query_engine, query_transform=hyde) response = query_engine.query(query_str) print(response)  ```  Check out our example notebook for a full walkthrough.\", \"Single-Step Query Decomposition  Some recent approaches (e.g. self-ask, ReAct) have suggested that LLM\\'s  perform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.  If your query is complex, different parts of your knowledge base may answer different \\\\\"subqueries\\\\\" around the overall query.  Our single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.  This is especially helpful over a composed graph. Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.  An example image is shown below.  !   Here\\'s a corresponding example code snippet over a composed graph.  ```python\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```\", \"Using with your own nodes  As you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.  ```python from llama_index.indices.postprocessor import SimilarityPostprocessor from llama_index.schema import Node, NodeWithScore  nodes = [   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.7),   NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=0.8) ]\", \"filter nodes below 0.75 similarity score processor = SimilarityPostprocessor(similarity_cutoff=0.75) filtered_nodes = processor.postprocess_nodes(nodes) ```\", \"Custom Node PostProcessor  The base class is `BaseNodePostprocessor`, and the API interface is very simple:   ```python class BaseNodePostprocessor:     \\\\\"\\\\\"\\\\\"Node postprocessor.\\\\\"\\\\\"\\\\\"      @abstractmethod     def postprocess_nodes(         self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]     ) -> List[NodeWithScore]:         \\\\\"\\\\\"\\\\\"Postprocess nodes.\\\\\"\\\\\"\\\\\" ```  A dummy node-postprocessor can be implemented in just a few lines of code:  ```python from llama_index import QueryBundle from llama_index.indices.postprocessor.base import BaseNodePostprocessor from llama_index.schema import NodeWithScore  class DummyNodePostprocessor:      def postprocess_nodes(         self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]     ) -> List[NodeWithScore]:                  # subtracts 1 from the score         for n in nodes:             n.score -= 1          return nodes ```\", \"Query Transformations   LlamaIndex allows you to perform *query transformations* over your index structures. Query transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index.   They can also be **multi-step**, as in:  1. The query is transformed, executed against an index,  2. The response is retrieved. 3. Subsequent queries are transformed/executed in a sequential fashion.  We list some of our query transformations in more detail below.\", \"Use Cases Query transformations have multiple use cases: - Transforming an initial query into a form that can be more easily embedded (e.g. HyDE) - Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition) - Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\", \"HyDE (Hypothetical Document Embeddings)  HyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.  To use HyDE, an example code snippet is shown below.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.indices.query.query_transform.base import HyDEQueryTransform from llama_index.query_engine.transform_query_engine import TransformQueryEngine\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex(documents)\", \"run query with HyDE query transform query_str = \\\\\"what did paul graham do after going to RISD\\\\\" hyde = HyDEQueryTransform(include_original=True) query_engine = index.as_query_engine() query_engine = TransformQueryEngine(query_engine, query_transform=hyde) response = query_engine.query(query_str) print(response)  ```  Check out our example notebook for a full walkthrough.\", \"Single-Step Query Decomposition  Some recent approaches (e.g. self-ask, ReAct) have suggested that LLM\\'s  perform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.  If your query is complex, different parts of your knowledge base may answer different \\\\\"subqueries\\\\\" around the overall query.  Our single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.  This is especially helpful over a composed graph. Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.  An example image is shown below.  !   Here\\'s a corresponding example code snippet over a composed graph.  ```python\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=107 request_id=99258cb9dd514bb8175969b09b504d78 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=107 request_id=99258cb9dd514bb8175969b09b504d78 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Setting: a list index composed over multiple vector indices from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     llm_predictor_chatgpt, verbose=True )\", \"initialize indexes and graph ...\", \"configure retrievers vector_query_engine = vector_index.as_query_engine() vector_query_engine = TransformQueryEngine(     vector_query_engine,      query_transform=decompose_transform     transform_extra_info={\\'index_summary\\': vector_index.index_struct.summary} ) custom_query_engines = {     vector_index.index_id: vector_query_engine }\", \"query query_str = (     \\\\\"Compare and contrast the airports in Seattle, Houston, and Toronto. \\\\\" ) query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines) response = query_engine.query(query_str) ```  Check out our example notebook for a full walkthrough.\", \"Multi-Step Query Transformations  Multi-step query transformations are a generalization on top of existing single-step query transformation approaches.  Given an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query.  Given the response (along with prior responses) and the query, followup questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.  An example image is shown below.  !   Here\\'s a corresponding example code snippet.  ```python from llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\", \"gpt-4 step_decompose_transform = StepDecomposeQueryTransform(     llm_predictor, verbose=True )  query_engine = index.as_query_engine() query_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)  response = query_engine.query(     \\\\\"Who was in the first batch of the accelerator program the author started?\\\\\", ) print(str(response))  ```  Check out our example notebook for a full walkthrough.   ```{toctree} --- caption: Examples maxdepth: 1 --- /examples/query_transformations/HyDEQueryTransformDemo.ipynb /examples/query_transformations/SimpleIndexDemo-multistep.ipynb ```\", \"Module Guides\", \"Basic ```{toctree} --- maxdepth: 1 --- Retriever Query Engine  ```\", \"Structured & Semi-Structured Data ```{toctree} --- maxdepth: 1 --- /examples/query_engine/json_query_engine.ipynb /examples/query_engine/pandas_query_engine.ipynb /examples/query_engine/knowledge_graph_query_engine.ipynb /examples/query_engine/knowledge_graph_rag_query_engine.ipynb ```\", \"Advanced ```{toctree} --- maxdepth: 1 --- /examples/query_engine/RouterQueryEngine.ipynb /examples/query_engine/RetrieverRouterQueryEngine.ipynb /examples/query_engine/JointQASummary.ipynb /examples/query_engine/sub_question_query_engine.ipynb /examples/query_transformations/SimpleIndexDemo-multistep.ipynb /examples/query_engine/SQLRouterQueryEngine.ipynb /examples/query_engine/SQLAutoVectorQueryEngine.ipynb /examples/query_engine/SQLJoinQueryEngine.ipynb /examples/index_structs/struct_indices/duckdb_sql_query.ipynb Retry Query Engine  Retry Source Query Engine  Retry Guideline Query Engine  /examples/query_engine/citation_query_engine.ipynb /examples/query_engine/pdf_tables/recursive_retriever.ipynb /examples/query_engine/recursive_retriever_agents.ipynb /examples/query_engine/ensemble_query_engine.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Setting: a list index composed over multiple vector indices from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     llm_predictor_chatgpt, verbose=True )\", \"initialize indexes and graph ...\", \"configure retrievers vector_query_engine = vector_index.as_query_engine() vector_query_engine = TransformQueryEngine(     vector_query_engine,      query_transform=decompose_transform     transform_extra_info={\\'index_summary\\': vector_index.index_struct.summary} ) custom_query_engines = {     vector_index.index_id: vector_query_engine }\", \"query query_str = (     \\\\\"Compare and contrast the airports in Seattle, Houston, and Toronto. \\\\\" ) query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines) response = query_engine.query(query_str) ```  Check out our example notebook for a full walkthrough.\", \"Multi-Step Query Transformations  Multi-step query transformations are a generalization on top of existing single-step query transformation approaches.  Given an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query.  Given the response (along with prior responses) and the query, followup questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.  An example image is shown below.  !   Here\\'s a corresponding example code snippet.  ```python from llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\", \"gpt-4 step_decompose_transform = StepDecomposeQueryTransform(     llm_predictor, verbose=True )  query_engine = index.as_query_engine() query_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)  response = query_engine.query(     \\\\\"Who was in the first batch of the accelerator program the author started?\\\\\", ) print(str(response))  ```  Check out our example notebook for a full walkthrough.   ```{toctree} --- caption: Examples maxdepth: 1 --- /examples/query_transformations/HyDEQueryTransformDemo.ipynb /examples/query_transformations/SimpleIndexDemo-multistep.ipynb ```\", \"Module Guides\", \"Basic ```{toctree} --- maxdepth: 1 --- Retriever Query Engine  ```\", \"Structured & Semi-Structured Data ```{toctree} --- maxdepth: 1 --- /examples/query_engine/json_query_engine.ipynb /examples/query_engine/pandas_query_engine.ipynb /examples/query_engine/knowledge_graph_query_engine.ipynb /examples/query_engine/knowledge_graph_rag_query_engine.ipynb ```\", \"Advanced ```{toctree} --- maxdepth: 1 --- /examples/query_engine/RouterQueryEngine.ipynb /examples/query_engine/RetrieverRouterQueryEngine.ipynb /examples/query_engine/JointQASummary.ipynb /examples/query_engine/sub_question_query_engine.ipynb /examples/query_transformations/SimpleIndexDemo-multistep.ipynb /examples/query_engine/SQLRouterQueryEngine.ipynb /examples/query_engine/SQLAutoVectorQueryEngine.ipynb /examples/query_engine/SQLJoinQueryEngine.ipynb /examples/index_structs/struct_indices/duckdb_sql_query.ipynb Retry Query Engine  Retry Source Query Engine  Retry Guideline Query Engine  /examples/query_engine/citation_query_engine.ipynb /examples/query_engine/pdf_tables/recursive_retriever.ipynb /examples/query_engine/recursive_retriever_agents.ipynb /examples/query_engine/ensemble_query_engine.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=43 request_id=ae9d149fe7776cd93d27b63b612bb573 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=43 request_id=ae9d149fe7776cd93d27b63b612bb573 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Experimental ```{toctree} --- maxdepth: 1 --- /examples/query_engine/flare_query_engine.ipynb ```\", \"Response Modes  Right now, we support the following options:  - `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk.      This makes a separate LLM call per Node/retrieved chunk.       **Details:** the first chunk is used in a query using the      `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used      in another query with the `refine_template` prompt. And so on until all chunks have been parsed.       If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`     (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks     of the original chunks collection (and thus queried with the `refine_template` as well).      Good for more detailed answers. - `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.      **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window      (considering the maximum prompt size between `text_qa_template` and `refine_template`).     If the text is too long to fit in one prompt, it is splitted in as many parts as needed      (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).           Each text part is considered a \\\\\"chunk\\\\\" and is sent to the `refine` synthesizer.           In short, it is like `refine`, but with less LLM calls. - `tree_summarize`: Query the LLM using the `text_qa_template` prompt as many times as needed so that all concatenated chunks    have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call     and so on, until there\\'s only one chunk left, and thus only one final answer.     **Details:** concatenate the chunks as much as possible to fit within the context window using the `text_qa_template` prompt,     and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against     `text_qa_template` (there is no ***refine*** query !) and get as many answers.         If there is only one answer (because there was only one chunk), then it\\'s the final answer.         If there are more than one answer, these themselves are considered as chunks and sent recursively     to the `tree_summarize` process (concatenated/splitted-to-fit/queried).        Good for summarization purposes. - `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick     summarization purposes, but may lose detail due to truncation. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,      without actually sending them. Then can be inspected by checking `response.source_nodes`. - `accumulate`: Given a set of text chunks and the query, apply the query to each text     chunk while accumulating the responses into an array. Returns a concatenated string of all     responses. Good for when you need to run the same query separately against each text     chunk. - `compact_accumulate`: The same as accumulate, but will \\\\\"compact\\\\\" each LLM prompt similar to     `compact`, and run the same query against each text chunk.  See Response Synthesizer to learn more.\", \"Query Engine\", \"Concept Query engine is a generic interface that allows you to ask question over your data.  A query engine takes in a natural language query, and returns a rich response. It is most often (but not always) built on one or many Indices via Retrievers. You can compose multiple query engines to achieve more advanced capability.  ```{tip} If you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine   ```\", \"Usage Pattern Get started with: ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"Who is Paul Graham.\\\\\") ```  To stream response: ```python query_engine = index.as_query_engine(streaming=True) streaming_response = query_engine.query(\\\\\"Who is Paul Graham.\\\\\") streaming_response.print_response_stream()  ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules ```{toctree} --- maxdepth: 3 --- modules.md ```\", \"Supporting Modules ```{toctree} --- maxdepth: 2 --- supporting_modules.md ```\", \"Streaming  LlamaIndex supports streaming the response as it\\'s being generated. This allows you to start printing or processing the beginning of the response before the full response is finished. This can drastically reduce the perceived latency of queries.\", \"Setup To enable streaming, you need to use an LLM that supports streaming. Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).  Configure query engine to use streaming:  If you are using the high-level API, set `streaming=True` when building a query engine. ```python query_engine = index.as_query_engine(     streaming=True,     similarity_top_k=1 ) ```  If you are using the low-level API to compose the query engine, pass `streaming=True` when constructing the `Response Synthesizer`: ```python from llama_index import get_response_synthesizer synth = get_response_synthesizer(streaming=True, ...) query_engine = RetrieverQueryEngine(response_synthesizer=synth, ...) ```\", \"Streaming Response After properly configuring both the LLM and the query engine, calling `query` now returns a `StreamingResponse` object.  ```python streaming_response = query_engine.query(     \\\\\"What did the author do growing up?\\\\\",  ) ```  The response is returned immediately when the LLM call *starts*, without having to wait for the full completion.  > Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.  You can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive: ```python for text in streaming_response.response_gen:     # do something with text as they arrive. ```  Alternatively, if you just want to print the text as they arrive: ``` streaming_response.print_response_stream()  ```  See an end-to-end example\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Experimental ```{toctree} --- maxdepth: 1 --- /examples/query_engine/flare_query_engine.ipynb ```\", \"Response Modes  Right now, we support the following options:  - `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk.      This makes a separate LLM call per Node/retrieved chunk.       **Details:** the first chunk is used in a query using the      `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used      in another query with the `refine_template` prompt. And so on until all chunks have been parsed.       If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`     (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks     of the original chunks collection (and thus queried with the `refine_template` as well).      Good for more detailed answers. - `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.      **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window      (considering the maximum prompt size between `text_qa_template` and `refine_template`).     If the text is too long to fit in one prompt, it is splitted in as many parts as needed      (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).           Each text part is considered a \\\\\"chunk\\\\\" and is sent to the `refine` synthesizer.           In short, it is like `refine`, but with less LLM calls. - `tree_summarize`: Query the LLM using the `text_qa_template` prompt as many times as needed so that all concatenated chunks    have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call     and so on, until there\\'s only one chunk left, and thus only one final answer.     **Details:** concatenate the chunks as much as possible to fit within the context window using the `text_qa_template` prompt,     and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against     `text_qa_template` (there is no ***refine*** query !) and get as many answers.         If there is only one answer (because there was only one chunk), then it\\'s the final answer.         If there are more than one answer, these themselves are considered as chunks and sent recursively     to the `tree_summarize` process (concatenated/splitted-to-fit/queried).        Good for summarization purposes. - `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick     summarization purposes, but may lose detail due to truncation. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,      without actually sending them. Then can be inspected by checking `response.source_nodes`. - `accumulate`: Given a set of text chunks and the query, apply the query to each text     chunk while accumulating the responses into an array. Returns a concatenated string of all     responses. Good for when you need to run the same query separately against each text     chunk. - `compact_accumulate`: The same as accumulate, but will \\\\\"compact\\\\\" each LLM prompt similar to     `compact`, and run the same query against each text chunk.  See Response Synthesizer to learn more.\", \"Query Engine\", \"Concept Query engine is a generic interface that allows you to ask question over your data.  A query engine takes in a natural language query, and returns a rich response. It is most often (but not always) built on one or many Indices via Retrievers. You can compose multiple query engines to achieve more advanced capability.  ```{tip} If you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine   ```\", \"Usage Pattern Get started with: ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"Who is Paul Graham.\\\\\") ```  To stream response: ```python query_engine = index.as_query_engine(streaming=True) streaming_response = query_engine.query(\\\\\"Who is Paul Graham.\\\\\") streaming_response.print_response_stream()  ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules ```{toctree} --- maxdepth: 3 --- modules.md ```\", \"Supporting Modules ```{toctree} --- maxdepth: 2 --- supporting_modules.md ```\", \"Streaming  LlamaIndex supports streaming the response as it\\'s being generated. This allows you to start printing or processing the beginning of the response before the full response is finished. This can drastically reduce the perceived latency of queries.\", \"Setup To enable streaming, you need to use an LLM that supports streaming. Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).  Configure query engine to use streaming:  If you are using the high-level API, set `streaming=True` when building a query engine. ```python query_engine = index.as_query_engine(     streaming=True,     similarity_top_k=1 ) ```  If you are using the low-level API to compose the query engine, pass `streaming=True` when constructing the `Response Synthesizer`: ```python from llama_index import get_response_synthesizer synth = get_response_synthesizer(streaming=True, ...) query_engine = RetrieverQueryEngine(response_synthesizer=synth, ...) ```\", \"Streaming Response After properly configuring both the LLM and the query engine, calling `query` now returns a `StreamingResponse` object.  ```python streaming_response = query_engine.query(     \\\\\"What did the author do growing up?\\\\\",  ) ```  The response is returned immediately when the LLM call *starts*, without having to wait for the full completion.  > Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.  You can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive: ```python for text in streaming_response.response_gen:     # do something with text as they arrive. ```  Alternatively, if you just want to print the text as they arrive: ``` streaming_response.print_response_stream()  ```  See an end-to-end example\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=49 request_id=850128a9aa7e5cb94a2ea442d3ddf4af response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=49 request_id=850128a9aa7e5cb94a2ea442d3ddf4af response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Supporting Modules  ```{toctree} --- maxdepth: 1 --- advanced/query_transformations.md ```\", \"Usage Pattern\", \"Get Started Build a query engine from index: ```python query_engine = index.as_query_engine() ```  ```{tip} To learn how to build an index, see Index ```  Ask a question over your data ```python response = query_engine.query(\\'Who is Paul Graham?\\') ```\", \"Configuring a Query Engine You can directly build and configure a query engine from an index in 1 line of code: ```python query_engine = index.as_query_engine(     response_mode=\\'tree_summarize\\',     verbose=True, ) ``` > Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.    See **Response Modes** for a full list of response modes and what they do.  ```{toctree} --- maxdepth: 1 hidden: --- response_modes.md streaming.md ```\", \"Low-Level Composition API  You can use the low-level composition API if you need more granular control. Concretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`. > Note: You may need to look at API references or example notebooks.   ```python from llama_index import (     VectorStoreIndex,     get_response_synthesizer, ) from llama_index.retrievers import VectorIndexRetriever from llama_index.query_engine import RetrieverQueryEngine\", \"build index index = VectorStoreIndex.from_documents(documents)\", \"configure retriever retriever = VectorIndexRetriever(     index=index,      similarity_top_k=2, )\", \"configure response synthesizer response_synthesizer = get_response_synthesizer(     response_mode=\\\\\"tree_summarize\\\\\", )\", \"assemble query engine query_engine = RetrieverQueryEngine(     retriever=retriever,     response_synthesizer=response_synthesizer, )\", \"query response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Supporting Modules  ```{toctree} --- maxdepth: 1 --- advanced/query_transformations.md ```\", \"Usage Pattern\", \"Get Started Build a query engine from index: ```python query_engine = index.as_query_engine() ```  ```{tip} To learn how to build an index, see Index ```  Ask a question over your data ```python response = query_engine.query(\\'Who is Paul Graham?\\') ```\", \"Configuring a Query Engine You can directly build and configure a query engine from an index in 1 line of code: ```python query_engine = index.as_query_engine(     response_mode=\\'tree_summarize\\',     verbose=True, ) ``` > Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.    See **Response Modes** for a full list of response modes and what they do.  ```{toctree} --- maxdepth: 1 hidden: --- response_modes.md streaming.md ```\", \"Low-Level Composition API  You can use the low-level composition API if you need more granular control. Concretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`. > Note: You may need to look at API references or example notebooks.   ```python from llama_index import (     VectorStoreIndex,     get_response_synthesizer, ) from llama_index.retrievers import VectorIndexRetriever from llama_index.query_engine import RetrieverQueryEngine\", \"build index index = VectorStoreIndex.from_documents(documents)\", \"configure retriever retriever = VectorIndexRetriever(     index=index,      similarity_top_k=2, )\", \"configure response synthesizer response_synthesizer = get_response_synthesizer(     response_mode=\\\\\"tree_summarize\\\\\", )\", \"assemble query engine query_engine = RetrieverQueryEngine(     retriever=retriever,     response_synthesizer=response_synthesizer, )\", \"query response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=178 request_id=42be3ff00984e20ec40c1a1b458ed758 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=178 request_id=42be3ff00984e20ec40c1a1b458ed758 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Streaming To enable streaming, you simply need to pass in a `streaming=True` flag  ```python query_engine = index.as_query_engine(     streaming=True, ) streaming_response = query_engine.query(     \\\\\"What did the author do growing up?\\\\\",  ) streaming_response.print_response_stream()  ```  * Read the full streaming guide * See an end-to-end example\", \"Module Guide  Detailed inputs/outputs for each response synthesizer are found below.\", \"API Example  The following shows the setup for utilizing all kwargs.  - `response_mode` specifies which response synthesizer to use - `service_context` defines the LLM and related settings for synthesis - `text_qa_template` and `refine_template` are the prompts used at various stages - `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree - `streaming` configures whether to return a streaming response object or not - `structured_answer_filtering` enables the active filtering of text chunks that are not relevant to a given question  In the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.  ```python from llama_index.schema import Node, NodeWithScore from llama_index import get_response_synthesizer  response_synthesizer = get_response_synthesizer(   response_mode=\\\\\"refine\\\\\",   service_context=service_context,   text_qa_template=text_qa_template,   refine_template=refine_template,   use_async=False,   streaming=False )\", \"synchronous response = response_synthesizer.synthesize(   \\\\\"query string\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],   additional_source_nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],  )\", \"asynchronous response = await response_synthesizer.asynthesize(   \\\\\"query string\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],   additional_source_nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],  ) ```  You can also directly return a string, using the lower-level `get_response` and `aget_response` functions  ```python response_str = response_synthesizer.get_response(   \\\\\"query string\\\\\",    text_chunks=[\\\\\"text1\\\\\", \\\\\"text2\\\\\", ...] ) ```\", \"Example Notebooks  ```{toctree} --- maxdepth: 1 --- /examples/response_synthesizers/refine.ipynb /examples/response_synthesizers/structured_refine.ipynb /examples/response_synthesizers/tree_summarize.ipynb ```\", \"Response Synthesizer\", \"Concept A `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.  The method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.  When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.  ```{tip} Confused about where response synthesizer fits in the pipeline? Read the high-level concepts ```\", \"Usage Pattern Use a response synthesizer on it\\'s own:  ```python from llama_index.schema import Node from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer  response_synthesizer = get_response_synthesizer(response_mode=ResponseMode.COMPACT)  response = response_synthesizer.synthesize(\\\\\"query text\\\\\", nodes=[Node(text=\\\\\"text\\\\\"), ...]) ```  Or in a query engine after you\\'ve created an index:  ```python query_engine = index.as_query_engine(response_synthesizer=response_synthesizer) response = query_engine.query(\\\\\"query_text\\\\\") ```  You can find more details on all available response synthesizers, modes, and how to build your own below.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find detailed API information for each response synthesis module.  ```{toctree} --- maxdepth: 1 --- modules.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Streaming To enable streaming, you simply need to pass in a `streaming=True` flag  ```python query_engine = index.as_query_engine(     streaming=True, ) streaming_response = query_engine.query(     \\\\\"What did the author do growing up?\\\\\",  ) streaming_response.print_response_stream()  ```  * Read the full streaming guide * See an end-to-end example\", \"Module Guide  Detailed inputs/outputs for each response synthesizer are found below.\", \"API Example  The following shows the setup for utilizing all kwargs.  - `response_mode` specifies which response synthesizer to use - `service_context` defines the LLM and related settings for synthesis - `text_qa_template` and `refine_template` are the prompts used at various stages - `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree - `streaming` configures whether to return a streaming response object or not - `structured_answer_filtering` enables the active filtering of text chunks that are not relevant to a given question  In the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.  ```python from llama_index.schema import Node, NodeWithScore from llama_index import get_response_synthesizer  response_synthesizer = get_response_synthesizer(   response_mode=\\\\\"refine\\\\\",   service_context=service_context,   text_qa_template=text_qa_template,   refine_template=refine_template,   use_async=False,   streaming=False )\", \"synchronous response = response_synthesizer.synthesize(   \\\\\"query string\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],   additional_source_nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],  )\", \"asynchronous response = await response_synthesizer.asynthesize(   \\\\\"query string\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],   additional_source_nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..],  ) ```  You can also directly return a string, using the lower-level `get_response` and `aget_response` functions  ```python response_str = response_synthesizer.get_response(   \\\\\"query string\\\\\",    text_chunks=[\\\\\"text1\\\\\", \\\\\"text2\\\\\", ...] ) ```\", \"Example Notebooks  ```{toctree} --- maxdepth: 1 --- /examples/response_synthesizers/refine.ipynb /examples/response_synthesizers/structured_refine.ipynb /examples/response_synthesizers/tree_summarize.ipynb ```\", \"Response Synthesizer\", \"Concept A `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.  The method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.  When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.  ```{tip} Confused about where response synthesizer fits in the pipeline? Read the high-level concepts ```\", \"Usage Pattern Use a response synthesizer on it\\'s own:  ```python from llama_index.schema import Node from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer  response_synthesizer = get_response_synthesizer(response_mode=ResponseMode.COMPACT)  response = response_synthesizer.synthesize(\\\\\"query text\\\\\", nodes=[Node(text=\\\\\"text\\\\\"), ...]) ```  Or in a query engine after you\\'ve created an index:  ```python query_engine = index.as_query_engine(response_synthesizer=response_synthesizer) response = query_engine.query(\\\\\"query_text\\\\\") ```  You can find more details on all available response synthesizers, modes, and how to build your own below.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find detailed API information for each response synthesis module.  ```{toctree} --- maxdepth: 1 --- modules.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=143 request_id=7abecaaa82b14132db4d224e47e74de5 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=143 request_id=7abecaaa82b14132db4d224e47e74de5 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern\", \"Get Started  Configuring the response synthesizer for a query engine using `response_mode`:  ```python from llama_index.schema import Node, NodeWithScore from llama_index.response_synthesizers import get_response_synthesizer  response_synthesizer = get_response_synthesizer(response_mode=\\'compact\\')  response = response_synthesizer.synthesize(   \\\\\"query text\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..] ) ```  Or, more commonly, in a query engine after you\\'ve created an index:  ```python query_engine = index.as_query_engine(response_synthesizer=response_synthesizer) response = query_engine.query(\\\\\"query_text\\\\\") ```  ```{tip} To learn how to build an index, see Index ```\", \"Configuring the Response Mode Response synthesizers are typically specified through a `response_mode` kwarg setting.  Several response synthesizers are implemented already in LlamaIndex:  - `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk.      This makes a separate LLM call per Node/retrieved chunk.       **Details:** the first chunk is used in a query using the      `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used      in another query with the `refine_template` prompt. And so on until all chunks have been parsed.       If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`     (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks     of the original chunks collection (and thus queried with the `refine_template` as well).      Good for more detailed answers. - `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.      **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window      (considering the maximum prompt size between `text_qa_template` and `refine_template`).     If the text is too long to fit in one prompt, it is splitted in as many parts as needed      (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).           Each text part is considered a \\\\\"chunk\\\\\" and is sent to the `refine` synthesizer.           In short, it is like `refine`, but with less LLM calls. - `tree_summarize`: Query the LLM using the `text_qa_template` prompt as many times as needed so that all concatenated chunks    have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call     and so on, until there\\'s only one chunk left, and thus only one final answer.     **Details:** concatenate the chunks as much as possible to fit within the context window using the `text_qa_template` prompt,     and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against     `text_qa_template` (there is no ***refine*** query !) and get as many answers.         If there is only one answer (because there was only one chunk), then it\\'s the final answer.         If there are more than one answer, these themselves are considered as chunks and sent recursively     to the `tree_summarize` process (concatenated/splitted-to-fit/queried).        Good for summarization purposes. - `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick     summarization purposes, but may lose detail due to truncation. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,      without actually sending them. Then can be inspected by checking `response.source_nodes`. - `accumulate`: Given a set of text chunks and the query, apply the query to each text     chunk while accumulating the responses into an array. Returns a concatenated string of all     responses. Good for when you need to run the same query separately against each text     chunk. - `compact_accumulate`: The same as accumulate, but will \\\\\"compact\\\\\" each LLM prompt similar to     `compact`, and run the same query against each text chunk.\", \"Custom Response Synthesizers  Each response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.  Maybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it\\'s own.  Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.  ```python class BaseSynthesizer(ABC):     \\\\\"\\\\\"\\\\\"Response builder class.\\\\\"\\\\\"\\\\\"      def __init__(         self,         service_context: Optional[ServiceContext] = None,         streaming: bool = False,     ) -> None:         \\\\\"\\\\\"\\\\\"Init params.\\\\\"\\\\\"\\\\\"         self._service_context = service_context or ServiceContext.from_defaults()         self._callback_manager = self._service_context.callback_manager         self._streaming = streaming      @abstractmethod     def get_response(         self,         query_str: str,         text_chunks: Sequence[str],         **response_kwargs: Any,     ) -> RESPONSE_TEXT_TYPE:         \\\\\"\\\\\"\\\\\"Get response.\\\\\"\\\\\"\\\\\"         ...      @abstractmethod     async def aget_response(         self,         query_str: str,         text_chunks: Sequence[str],         **response_kwargs: Any,     ) -> RESPONSE_TEXT_TYPE:         \\\\\"\\\\\"\\\\\"Get response.\\\\\"\\\\\"\\\\\"         ... ```\", \"Using Structured Answer Filtering When using either the `\\\\\"refine\\\\\"` or `\\\\\"compact\\\\\"` response synthesis modules, you may find it beneficial to experiment with the `structured_answer_filtering` option.  ``` from llama_index.response_synthesizers import get_response_synthesizer  response_synthesizer = get_response_synthesizer(structured_answer_filtering=True) ```  With `structured_answer_filtering` set to `True`, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query.  This option is particularly useful if you\\'re using an OpenAI model that supports function calling. Other LLM providers or models that don\\'t have native function calling support may be less reliable in producing the structured response this feature relies on.\", \"Module Guides We are actively adding more tailored retrieval guides. In the meanwhile, please take a look at the API References.\", \"Index Retrievers  Please see the retriever modes for more details on how to get a retriever from any given index.  If you want to import the corresponding retrievers directly, please check out our API reference.\", \"Advanced Retriever Guides  Check out our comprehensive guides on various retriever modules, many of which cover advanced concepts (auto-retrieval, routing, ensembling, and more).\", \"External Retrievers ```{toctree} --- maxdepth: 1 --- /examples/retrievers/bm25_retriever.ipynb  ```\", \"Knowledge Graph Retrievers ```{toctree} --- maxdepth: 1 --- Custom Retriever (KG Index and Vector Store Index)  Knowledge Graph RAG Retriever  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern\", \"Get Started  Configuring the response synthesizer for a query engine using `response_mode`:  ```python from llama_index.schema import Node, NodeWithScore from llama_index.response_synthesizers import get_response_synthesizer  response_synthesizer = get_response_synthesizer(response_mode=\\'compact\\')  response = response_synthesizer.synthesize(   \\\\\"query text\\\\\",    nodes=[NodeWithScore(node=Node(text=\\\\\"text\\\\\"), score=1.0), ..] ) ```  Or, more commonly, in a query engine after you\\'ve created an index:  ```python query_engine = index.as_query_engine(response_synthesizer=response_synthesizer) response = query_engine.query(\\\\\"query_text\\\\\") ```  ```{tip} To learn how to build an index, see Index ```\", \"Configuring the Response Mode Response synthesizers are typically specified through a `response_mode` kwarg setting.  Several response synthesizers are implemented already in LlamaIndex:  - `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk.      This makes a separate LLM call per Node/retrieved chunk.       **Details:** the first chunk is used in a query using the      `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used      in another query with the `refine_template` prompt. And so on until all chunks have been parsed.       If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`     (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks     of the original chunks collection (and thus queried with the `refine_template` as well).      Good for more detailed answers. - `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.      **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window      (considering the maximum prompt size between `text_qa_template` and `refine_template`).     If the text is too long to fit in one prompt, it is splitted in as many parts as needed      (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).           Each text part is considered a \\\\\"chunk\\\\\" and is sent to the `refine` synthesizer.           In short, it is like `refine`, but with less LLM calls. - `tree_summarize`: Query the LLM using the `text_qa_template` prompt as many times as needed so that all concatenated chunks    have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call     and so on, until there\\'s only one chunk left, and thus only one final answer.     **Details:** concatenate the chunks as much as possible to fit within the context window using the `text_qa_template` prompt,     and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against     `text_qa_template` (there is no ***refine*** query !) and get as many answers.         If there is only one answer (because there was only one chunk), then it\\'s the final answer.         If there are more than one answer, these themselves are considered as chunks and sent recursively     to the `tree_summarize` process (concatenated/splitted-to-fit/queried).        Good for summarization purposes. - `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick     summarization purposes, but may lose detail due to truncation. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,      without actually sending them. Then can be inspected by checking `response.source_nodes`. - `accumulate`: Given a set of text chunks and the query, apply the query to each text     chunk while accumulating the responses into an array. Returns a concatenated string of all     responses. Good for when you need to run the same query separately against each text     chunk. - `compact_accumulate`: The same as accumulate, but will \\\\\"compact\\\\\" each LLM prompt similar to     `compact`, and run the same query against each text chunk.\", \"Custom Response Synthesizers  Each response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.  Maybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it\\'s own.  Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.  ```python class BaseSynthesizer(ABC):     \\\\\"\\\\\"\\\\\"Response builder class.\\\\\"\\\\\"\\\\\"      def __init__(         self,         service_context: Optional[ServiceContext] = None,         streaming: bool = False,     ) -> None:         \\\\\"\\\\\"\\\\\"Init params.\\\\\"\\\\\"\\\\\"         self._service_context = service_context or ServiceContext.from_defaults()         self._callback_manager = self._service_context.callback_manager         self._streaming = streaming      @abstractmethod     def get_response(         self,         query_str: str,         text_chunks: Sequence[str],         **response_kwargs: Any,     ) -> RESPONSE_TEXT_TYPE:         \\\\\"\\\\\"\\\\\"Get response.\\\\\"\\\\\"\\\\\"         ...      @abstractmethod     async def aget_response(         self,         query_str: str,         text_chunks: Sequence[str],         **response_kwargs: Any,     ) -> RESPONSE_TEXT_TYPE:         \\\\\"\\\\\"\\\\\"Get response.\\\\\"\\\\\"\\\\\"         ... ```\", \"Using Structured Answer Filtering When using either the `\\\\\"refine\\\\\"` or `\\\\\"compact\\\\\"` response synthesis modules, you may find it beneficial to experiment with the `structured_answer_filtering` option.  ``` from llama_index.response_synthesizers import get_response_synthesizer  response_synthesizer = get_response_synthesizer(structured_answer_filtering=True) ```  With `structured_answer_filtering` set to `True`, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query.  This option is particularly useful if you\\'re using an OpenAI model that supports function calling. Other LLM providers or models that don\\'t have native function calling support may be less reliable in producing the structured response this feature relies on.\", \"Module Guides We are actively adding more tailored retrieval guides. In the meanwhile, please take a look at the API References.\", \"Index Retrievers  Please see the retriever modes for more details on how to get a retriever from any given index.  If you want to import the corresponding retrievers directly, please check out our API reference.\", \"Advanced Retriever Guides  Check out our comprehensive guides on various retriever modules, many of which cover advanced concepts (auto-retrieval, routing, ensembling, and more).\", \"External Retrievers ```{toctree} --- maxdepth: 1 --- /examples/retrievers/bm25_retriever.ipynb  ```\", \"Knowledge Graph Retrievers ```{toctree} --- maxdepth: 1 --- Custom Retriever (KG Index and Vector Store Index)  Knowledge Graph RAG Retriever  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=670 request_id=17f826efab9ab748ec7aa934aa4b860a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=670 request_id=17f826efab9ab748ec7aa934aa4b860a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Composed Retrievers ```{toctree} --- maxdepth: 1 --- Auto-Retrieval (with Chroma)  Auto-Retrieval (with BagelDB)  /examples/query_engine/pdf_tables/recursive_retriever.ipynb /examples/retrievers/router_retriever.ipynb /examples/retrievers/ensemble_retrieval.ipynb /examples/retrievers/auto_merging_retriever.ipynb ```\", \"Retriever Modes Here we show the mapping from `retriever_mode` configuration to the selected retriever class. > Note that `retriever_mode` can mean different thing for different index classes.\", \"Vector Index Specifying `retriever_mode` has no effect (silently ignored). `vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\", \"List Index * `default`: ListIndexRetriever  * `embedding`: ListIndexEmbeddingRetriever  * `llm`: ListIndexLLMRetriever\", \"Tree Index * `select_leaf`: TreeSelectLeafRetriever * `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever * `all_leaf`: TreeAllLeafRetriever * `root`: TreeRootRetriever\", \"Keyword Table Index * `default`: KeywordTableGPTRetriever * `simple`: KeywordTableSimpleRetriever * `rake`: KeywordTableRAKERetriever\", \"Knowledge Graph Index * `keyword`: KGTableRetriever * `embedding`: KGTableRetriever * `hybrid`: KGTableRetriever\", \"Document Summary Index * `default`: DocumentSummaryIndexRetriever * `embedding`: DocumentSummaryIndexEmbeddingRetrievers\", \"Retriever\", \"Concept  Retrievers are responsible for fetching the most relevant context given a user query (or chat message).    It can be built on top of Indices, but can also be defined independently. It is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.  ```{tip} Confused about where retriever fits in the pipeline? Read about high-level concepts ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Composed Retrievers ```{toctree} --- maxdepth: 1 --- Auto-Retrieval (with Chroma)  Auto-Retrieval (with BagelDB)  /examples/query_engine/pdf_tables/recursive_retriever.ipynb /examples/retrievers/router_retriever.ipynb /examples/retrievers/ensemble_retrieval.ipynb /examples/retrievers/auto_merging_retriever.ipynb ```\", \"Retriever Modes Here we show the mapping from `retriever_mode` configuration to the selected retriever class. > Note that `retriever_mode` can mean different thing for different index classes.\", \"Vector Index Specifying `retriever_mode` has no effect (silently ignored). `vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\", \"List Index * `default`: ListIndexRetriever  * `embedding`: ListIndexEmbeddingRetriever  * `llm`: ListIndexLLMRetriever\", \"Tree Index * `select_leaf`: TreeSelectLeafRetriever * `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever * `all_leaf`: TreeAllLeafRetriever * `root`: TreeRootRetriever\", \"Keyword Table Index * `default`: KeywordTableGPTRetriever * `simple`: KeywordTableSimpleRetriever * `rake`: KeywordTableRAKERetriever\", \"Knowledge Graph Index * `keyword`: KGTableRetriever * `embedding`: KGTableRetriever * `hybrid`: KGTableRetriever\", \"Document Summary Index * `default`: DocumentSummaryIndexRetriever * `embedding`: DocumentSummaryIndexEmbeddingRetrievers\", \"Retriever\", \"Concept  Retrievers are responsible for fetching the most relevant context given a user query (or chat message).    It can be built on top of Indices, but can also be defined independently. It is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.  ```{tip} Confused about where retriever fits in the pipeline? Read about high-level concepts ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=78 request_id=26f80612177ef4c5777ded90d9800e87 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=78 request_id=26f80612177ef4c5777ded90d9800e87 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern  Get started with: ```python retriever = index.as_retriever() nodes = retriever.retrieve(\\\\\"Who is Paul Graham?\\\\\") ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started Get a retriever from index: ```python retriever = index.as_retriever() ```  Retrieve relevant context for a question: ```python nodes = retriever.retrieve(\\'Who is Paul Graham?\\') ```  > Note: To learn how to build an index, see Index\", \"High-Level API\", \"Selecting a Retriever  You can select the index-specific retriever class via `retriever_mode`.  For example, with a `ListIndex`: ```python retriever = list_index.as_retriever(     retriever_mode=\\'llm\\', ) ``` This creates a ListIndexLLMRetriever on top of the list index.  See **Retriever Modes** for a full list of (index-specific) retriever modes and the retriever classes they map to.  ```{toctree} --- maxdepth: 1 hidden: --- retriever_modes.md ```\", \"Configuring a Retriever In the same way, you can pass kwargs to configure the selected retriever. > Note: take a look at the API reference for the selected retriever class\\' constructor parameters for a list of valid kwargs.  For example, if we selected the \\\\\"llm\\\\\" retriever mode, we might do the following: ```python retriever = list_index.as_retriever(     retriever_mode=\\'llm\\',     choice_batch_size=5, )  ```\", \"Low-Level Composition API You can use the low-level composition API if you need more granular control.    To achieve the same outcome as above, you can directly import and construct the desired retriever class: ```python from llama_index.indices.list import ListIndexLLMRetriever  retriever = ListIndexLLMRetriever(     index=list_index,     choice_batch_size=5, ) ```\", \"Advanced  ```{toctree} --- maxdepth: 1 --- Define Custom Retriever  BM25 Hybrid Retriever  ```\", \"Modules  ```{toctree} --- maxdepth: 1 --- /examples/query_engine/RouterQueryEngine.ipynb /examples/query_engine/RetrieverRouterQueryEngine.ipynb /examples/query_engine/SQLRouterQueryEngine.ipynb /examples/retrievers/router_retriever.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern  Get started with: ```python retriever = index.as_retriever() nodes = retriever.retrieve(\\\\\"Who is Paul Graham?\\\\\") ```  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern\", \"Get Started Get a retriever from index: ```python retriever = index.as_retriever() ```  Retrieve relevant context for a question: ```python nodes = retriever.retrieve(\\'Who is Paul Graham?\\') ```  > Note: To learn how to build an index, see Index\", \"High-Level API\", \"Selecting a Retriever  You can select the index-specific retriever class via `retriever_mode`.  For example, with a `ListIndex`: ```python retriever = list_index.as_retriever(     retriever_mode=\\'llm\\', ) ``` This creates a ListIndexLLMRetriever on top of the list index.  See **Retriever Modes** for a full list of (index-specific) retriever modes and the retriever classes they map to.  ```{toctree} --- maxdepth: 1 hidden: --- retriever_modes.md ```\", \"Configuring a Retriever In the same way, you can pass kwargs to configure the selected retriever. > Note: take a look at the API reference for the selected retriever class\\' constructor parameters for a list of valid kwargs.  For example, if we selected the \\\\\"llm\\\\\" retriever mode, we might do the following: ```python retriever = list_index.as_retriever(     retriever_mode=\\'llm\\',     choice_batch_size=5, )  ```\", \"Low-Level Composition API You can use the low-level composition API if you need more granular control.    To achieve the same outcome as above, you can directly import and construct the desired retriever class: ```python from llama_index.indices.list import ListIndexLLMRetriever  retriever = ListIndexLLMRetriever(     index=list_index,     choice_batch_size=5, ) ```\", \"Advanced  ```{toctree} --- maxdepth: 1 --- Define Custom Retriever  BM25 Hybrid Retriever  ```\", \"Modules  ```{toctree} --- maxdepth: 1 --- /examples/query_engine/RouterQueryEngine.ipynb /examples/query_engine/RetrieverRouterQueryEngine.ipynb /examples/query_engine/SQLRouterQueryEngine.ipynb /examples/retrievers/router_retriever.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=11d47975c32d96d7c693d7b7067d95d7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=11d47975c32d96d7c693d7b7067d95d7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Routers\", \"Concept Routers are modules that take in a user query and a set of \\\\\"choices\\\\\" (defined by metadata), and returns one or more selected choices.  They can be used on their own (as \\\\\"selector modules\\\\\"), or used as a query engine or retriever (e.g. on top of other query engines/retrievers).  They are simple but powerful modules that use LLMs for decision making capabilities. They can be used for the following use cases and more: - Selecting the right data source among a diverse range of data sources - Deciding whether to do summarization (e.g. using list index query engine) or semantic search (e.g. using vector index query engine) - Deciding whether to \\\\\"try\\\\\" out a bunch of choices at once and combine the results (using multi-routing capabilities).  The core router modules exist in the following forms: - LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions - Pydantic selectors pass choices as Pydantic schemas into a function calling endpoint, and return Pydantic objects\", \"Usage Pattern  A simple example of using our router module as part of a query engine is given below.  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector from llama_index.tools.query_engine import QueryEngineTool   list_tool = QueryEngineTool.from_defaults(     query_engine=list_query_engine,     description=\\\\\"Useful for summarization questions related to the data source\\\\\", ) vector_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=\\\\\"Useful for retrieving specific context related to the data source\\\\\", )  query_engine = RouterQueryEngine(     selector=PydanticSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) query_engine.query(\\\\\"\\\\\") ```  You can find more details using routers as standalone modules, as part of a query engine, and as part of a retriever below in the usage pattern guide.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find extensive guides using routers in different settings.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern  Defining a \\\\\"selector\\\\\" is at the core of defining a router.  You can easily use our routers as a query engine or a retriever. In these cases, the router will be responsible for \\\\\"selecting\\\\\" query engine(s) or retriever(s) to route the user query to.  We also highlight our `ToolRetrieverRouterQueryEngine` for retrieval-augmented routing - this is the case where the set of choices themselves may be very big and may need to be indexed. **NOTE**: this is a beta feature.  We also highlight using our router as a standalone module.\", \"Defining a selector  Some examples are given below with LLM and Pydantic based single/multi selectors:  ```python from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector from llama_index.selectors.pydantic_selectors import (     PydanticMultiSelector,     PydanticSingleSelector, )\", \"pydantic selectors feed in pydantic objects to a function calling API selector = PydanticSingleSelector.from_defaults()\", \"multi selector (pydantic) selector = PydanticMultiSelector.from_defaults()\", \"LLM selectors use text completion endpoints selector = LLMSingleSelector.from_defaults()\", \"multi selector (LLM) selector = LLMMultiSelector.from_defaults()  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Routers\", \"Concept Routers are modules that take in a user query and a set of \\\\\"choices\\\\\" (defined by metadata), and returns one or more selected choices.  They can be used on their own (as \\\\\"selector modules\\\\\"), or used as a query engine or retriever (e.g. on top of other query engines/retrievers).  They are simple but powerful modules that use LLMs for decision making capabilities. They can be used for the following use cases and more: - Selecting the right data source among a diverse range of data sources - Deciding whether to do summarization (e.g. using list index query engine) or semantic search (e.g. using vector index query engine) - Deciding whether to \\\\\"try\\\\\" out a bunch of choices at once and combine the results (using multi-routing capabilities).  The core router modules exist in the following forms: - LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions - Pydantic selectors pass choices as Pydantic schemas into a function calling endpoint, and return Pydantic objects\", \"Usage Pattern  A simple example of using our router module as part of a query engine is given below.  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector from llama_index.tools.query_engine import QueryEngineTool   list_tool = QueryEngineTool.from_defaults(     query_engine=list_query_engine,     description=\\\\\"Useful for summarization questions related to the data source\\\\\", ) vector_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=\\\\\"Useful for retrieving specific context related to the data source\\\\\", )  query_engine = RouterQueryEngine(     selector=PydanticSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) query_engine.query(\\\\\"\\\\\") ```  You can find more details using routers as standalone modules, as part of a query engine, and as part of a retriever below in the usage pattern guide.  ```{toctree} --- maxdepth: 2 --- usage_pattern.md ```\", \"Modules Below you can find extensive guides using routers in different settings.  ```{toctree} --- maxdepth: 2 --- modules.md ```\", \"Usage Pattern  Defining a \\\\\"selector\\\\\" is at the core of defining a router.  You can easily use our routers as a query engine or a retriever. In these cases, the router will be responsible for \\\\\"selecting\\\\\" query engine(s) or retriever(s) to route the user query to.  We also highlight our `ToolRetrieverRouterQueryEngine` for retrieval-augmented routing - this is the case where the set of choices themselves may be very big and may need to be indexed. **NOTE**: this is a beta feature.  We also highlight using our router as a standalone module.\", \"Defining a selector  Some examples are given below with LLM and Pydantic based single/multi selectors:  ```python from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector from llama_index.selectors.pydantic_selectors import (     PydanticMultiSelector,     PydanticSingleSelector, )\", \"pydantic selectors feed in pydantic objects to a function calling API selector = PydanticSingleSelector.from_defaults()\", \"multi selector (pydantic) selector = PydanticMultiSelector.from_defaults()\", \"LLM selectors use text completion endpoints selector = LLMSingleSelector.from_defaults()\", \"multi selector (LLM) selector = LLMMultiSelector.from_defaults()  ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=a7e687c35b8af3c28edb44bdb0f08aab response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=a7e687c35b8af3c28edb44bdb0f08aab response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Using as a Query Engine  A `RouterQueryEngine` is composed on top of other query engines as tools.   ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector, Pydantic from llama_index.tools.query_engine import QueryEngineTool from llama_index import (     VectorStoreIndex,     ListIndex, )\", \"define query engines ...\", \"initialize tools list_tool = QueryEngineTool.from_defaults(     query_engine=list_query_engine,     description=\\\\\"Useful for summarization questions related to the data source\\\\\", ) vector_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=\\\\\"Useful for retrieving specific context related to the data source\\\\\", )\", \"initialize router query engine (single selection, pydantic) query_engine = RouterQueryEngine(     selector=PydanticSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) query_engine.query(\\\\\"\\\\\")  ```\", \"Using as a Retriever  Similarly, a `RouterRetriever` is composed on top of other retrievers as tools. An example is given below:  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector from llama_index.tools import RetrieverTool\", \"define indices ...\", \"define retrievers vector_retriever = vector_index.as_retriever() keyword_retriever = keyword_index.as_retriever()\", \"initialize tools vector_tool = RetrieverTool.from_defaults(     retriever=vector_retriever,     description=\\\\\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\\\\\", ) keyword_tool = RetrieverTool.from_defaults(     retriever=keyword_retriever,     description=\\\\\"Useful for retrieving specific context from Paul Graham essay on What I Worked On (using entities mentioned in query)\\\\\", )\", \"define retriever retriever = RouterRetriever(     selector=PydanticSingleSelector.from_defaults(llm=llm),     retriever_tools=[         list_tool,         vector_tool,     ], )  ```\", \"Using selector as a standalone module  You can use the selectors as standalone modules. Define choices as either a list of `ToolMetadata` or as a list of strings.  ```python from llama_index.tools import ToolMetadata from llama_index.selectors.llm_selectors import LLMSingleSelector\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Using as a Query Engine  A `RouterQueryEngine` is composed on top of other query engines as tools.   ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector, Pydantic from llama_index.tools.query_engine import QueryEngineTool from llama_index import (     VectorStoreIndex,     ListIndex, )\", \"define query engines ...\", \"initialize tools list_tool = QueryEngineTool.from_defaults(     query_engine=list_query_engine,     description=\\\\\"Useful for summarization questions related to the data source\\\\\", ) vector_tool = QueryEngineTool.from_defaults(     query_engine=vector_query_engine,     description=\\\\\"Useful for retrieving specific context related to the data source\\\\\", )\", \"initialize router query engine (single selection, pydantic) query_engine = RouterQueryEngine(     selector=PydanticSingleSelector.from_defaults(),     query_engine_tools=[         list_tool,         vector_tool,     ], ) query_engine.query(\\\\\"\\\\\")  ```\", \"Using as a Retriever  Similarly, a `RouterRetriever` is composed on top of other retrievers as tools. An example is given below:  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.pydantic_selectors import PydanticSingleSelector from llama_index.tools import RetrieverTool\", \"define indices ...\", \"define retrievers vector_retriever = vector_index.as_retriever() keyword_retriever = keyword_index.as_retriever()\", \"initialize tools vector_tool = RetrieverTool.from_defaults(     retriever=vector_retriever,     description=\\\\\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\\\\\", ) keyword_tool = RetrieverTool.from_defaults(     retriever=keyword_retriever,     description=\\\\\"Useful for retrieving specific context from Paul Graham essay on What I Worked On (using entities mentioned in query)\\\\\", )\", \"define retriever retriever = RouterRetriever(     selector=PydanticSingleSelector.from_defaults(llm=llm),     retriever_tools=[         list_tool,         vector_tool,     ], )  ```\", \"Using selector as a standalone module  You can use the selectors as standalone modules. Define choices as either a list of `ToolMetadata` or as a list of strings.  ```python from llama_index.tools import ToolMetadata from llama_index.selectors.llm_selectors import LLMSingleSelector\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=ba130c36586a263eb01d0a296d4f1741 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=ba130c36586a263eb01d0a296d4f1741 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"choices as a list of tool metadata choices = [     ToolMetadata(description=\\\\\"description for choice 1\\\\\", name=\\\\\"choice_1\\\\\"),     ToolMetadata(description=\\\\\"description for choice 2\\\\\", name=\\\\\"choice_2\\\\\"), ]\", \"choices as a list of strings choices = [\\\\\"choice 1 - description for choice 1\\\\\", \\\\\"choice 2: description for choice 2\\\\\"]  selector = LLMSingleSelector.from_defaults() selector_result = selector.select(choices, query=\\\\\"What\\'s revenue growth for IBM in 2007?\\\\\") print(selector_result.selections)  ```\", \"Output Parsing  LlamaIndex supports integrations with output parsing modules offered by other frameworks. These output parsing modules can be used in the following ways: - To provide formatting instructions for any prompt / query (through `output_parser.format`) - To provide \\\\\"parsing\\\\\" for LLM outputs (through `output_parser.parse`)\", \"Guardrails  Guardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.   ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.output_parsers import GuardrailsOutputParser from llama_index.llm_predictor import StructuredLLMPredictor from llama_index.prompts import PromptTemplate from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex(documents, chunk_size=512) llm_predictor = StructuredLLMPredictor()\", \"specify StructuredLLMPredictor\", \"define query / output spec rail_spec = (\\\\\"\\\\\"\\\\\"                                                                           Query string here.  @xml_prefix_prompt  {output_schema}  @json_suffix_prompt_v2_wo_none   \\\\\"\\\\\"\\\\\")\", \"define output parser output_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\", \"format each prompt with output parser instructions fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL) fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)  qa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser) refine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\", \"obtain a structured response query_engine = index.as_query_engine(     service_context=ServiceContext.from_defaults(         llm_predictor=llm_predictor     ),     text_qa_template=qa_prompt,      refine_template=refine_prompt,  ) response = query_engine.query(     \\\\\"What are the three items the author did growing up?\\\\\",  ) print(response)  ```  Output: ``` {\\'points\\': [{\\'explanation\\': \\'Writing short stories\\', \\'explanation2\\': \\'Programming on an IBM 1401\\', \\'explanation3\\': \\'Using microcomputers\\'}]} ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"choices as a list of tool metadata choices = [     ToolMetadata(description=\\\\\"description for choice 1\\\\\", name=\\\\\"choice_1\\\\\"),     ToolMetadata(description=\\\\\"description for choice 2\\\\\", name=\\\\\"choice_2\\\\\"), ]\", \"choices as a list of strings choices = [\\\\\"choice 1 - description for choice 1\\\\\", \\\\\"choice 2: description for choice 2\\\\\"]  selector = LLMSingleSelector.from_defaults() selector_result = selector.select(choices, query=\\\\\"What\\'s revenue growth for IBM in 2007?\\\\\") print(selector_result.selections)  ```\", \"Output Parsing  LlamaIndex supports integrations with output parsing modules offered by other frameworks. These output parsing modules can be used in the following ways: - To provide formatting instructions for any prompt / query (through `output_parser.format`) - To provide \\\\\"parsing\\\\\" for LLM outputs (through `output_parser.parse`)\", \"Guardrails  Guardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.   ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.output_parsers import GuardrailsOutputParser from llama_index.llm_predictor import StructuredLLMPredictor from llama_index.prompts import PromptTemplate from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex(documents, chunk_size=512) llm_predictor = StructuredLLMPredictor()\", \"specify StructuredLLMPredictor\", \"define query / output spec rail_spec = (\\\\\"\\\\\"\\\\\"                                                                           Query string here.  @xml_prefix_prompt  {output_schema}  @json_suffix_prompt_v2_wo_none   \\\\\"\\\\\"\\\\\")\", \"define output parser output_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\", \"format each prompt with output parser instructions fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL) fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)  qa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser) refine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\", \"obtain a structured response query_engine = index.as_query_engine(     service_context=ServiceContext.from_defaults(         llm_predictor=llm_predictor     ),     text_qa_template=qa_prompt,      refine_template=refine_prompt,  ) response = query_engine.query(     \\\\\"What are the three items the author did growing up?\\\\\",  ) print(response)  ```  Output: ``` {\\'points\\': [{\\'explanation\\': \\'Writing short stories\\', \\'explanation2\\': \\'Programming on an IBM 1401\\', \\'explanation3\\': \\'Using microcomputers\\'}]} ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=c336848da57bef205eb932139adfc6ec response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=106 request_id=c336848da57bef205eb932139adfc6ec response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Langchain  Langchain also offers output parsing modules that you can use within LlamaIndex.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.output_parsers import LangchainOutputParser from llama_index.llm_predictor import StructuredLLMPredictor from llama_index.prompts import PromptTemplate from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL from langchain.output_parsers import StructuredOutputParser, ResponseSchema\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents) llm_predictor = StructuredLLMPredictor()\", \"define output schema response_schemas = [     ResponseSchema(name=\\\\\"Education\\\\\", description=\\\\\"Describes the author\\'s educational experience/background.\\\\\"),     ResponseSchema(name=\\\\\"Work\\\\\", description=\\\\\"Describes the author\\'s work experience/background.\\\\\") ]\", \"define output parser lc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas) output_parser = LangchainOutputParser(lc_output_parser)\", \"format each prompt with output parser instructions fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL) fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL) qa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser) refine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\", \"query index query_engine = index.as_query_engine(     service_context=ServiceContext.from_defaults(         llm_predictor=llm_predictor     ),     text_qa_template=qa_prompt,      refine_template=refine_prompt,  ) response = query_engine.query(     \\\\\"What are a few things the author did growing up?\\\\\",  ) print(str(response)) ```  Output:  ``` {\\'Education\\': \\'Before college, the author wrote short stories and experimented with programming on an IBM 1401.\\', \\'Work\\': \\'The author worked on writing and programming outside of school.\\'} ```\", \"Guides  ```{toctree} --- caption: Examples maxdepth: 1 ---  /examples/output_parsing/GuardrailsDemo.ipynb /examples/output_parsing/LangchainOutputParserDemo.ipynb /examples/output_parsing/guidance_pydantic_program.ipynb /examples/output_parsing/guidance_sub_question.ipynb /examples/output_parsing/openai_pydantic_program.ipynb ```\", \"Pydantic Program  A pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.  Because this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.  There\\'s a few general types of Pydantic Programs: - **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing. - **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API. - **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\", \"LLM Text Completion Pydantic Programs TODO: Coming soon!\", \"LLM Function Calling Pydantic Programs ```{toctree} --- maxdepth: 1 --- /examples/output_parsing/openai_pydantic_program.ipynb /examples/output_parsing/guidance_pydantic_program.ipynb /examples/output_parsing/guidance_sub_question.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Langchain  Langchain also offers output parsing modules that you can use within LlamaIndex.  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader from llama_index.output_parsers import LangchainOutputParser from llama_index.llm_predictor import StructuredLLMPredictor from llama_index.prompts import PromptTemplate from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL from langchain.output_parsers import StructuredOutputParser, ResponseSchema\", \"load documents, build index documents = SimpleDirectoryReader(\\'../paul_graham_essay/data\\').load_data() index = VectorStoreIndex.from_documents(documents) llm_predictor = StructuredLLMPredictor()\", \"define output schema response_schemas = [     ResponseSchema(name=\\\\\"Education\\\\\", description=\\\\\"Describes the author\\'s educational experience/background.\\\\\"),     ResponseSchema(name=\\\\\"Work\\\\\", description=\\\\\"Describes the author\\'s work experience/background.\\\\\") ]\", \"define output parser lc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas) output_parser = LangchainOutputParser(lc_output_parser)\", \"format each prompt with output parser instructions fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL) fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL) qa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser) refine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\", \"query index query_engine = index.as_query_engine(     service_context=ServiceContext.from_defaults(         llm_predictor=llm_predictor     ),     text_qa_template=qa_prompt,      refine_template=refine_prompt,  ) response = query_engine.query(     \\\\\"What are a few things the author did growing up?\\\\\",  ) print(str(response)) ```  Output:  ``` {\\'Education\\': \\'Before college, the author wrote short stories and experimented with programming on an IBM 1401.\\', \\'Work\\': \\'The author worked on writing and programming outside of school.\\'} ```\", \"Guides  ```{toctree} --- caption: Examples maxdepth: 1 ---  /examples/output_parsing/GuardrailsDemo.ipynb /examples/output_parsing/LangchainOutputParserDemo.ipynb /examples/output_parsing/guidance_pydantic_program.ipynb /examples/output_parsing/guidance_sub_question.ipynb /examples/output_parsing/openai_pydantic_program.ipynb ```\", \"Pydantic Program  A pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.  Because this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.  There\\'s a few general types of Pydantic Programs: - **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing. - **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API. - **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\", \"LLM Text Completion Pydantic Programs TODO: Coming soon!\", \"LLM Function Calling Pydantic Programs ```{toctree} --- maxdepth: 1 --- /examples/output_parsing/openai_pydantic_program.ipynb /examples/output_parsing/guidance_pydantic_program.ipynb /examples/output_parsing/guidance_sub_question.ipynb ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=41 request_id=145e192c9ad0b036ffe20e1caf1c050a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=41 request_id=145e192c9ad0b036ffe20e1caf1c050a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Prepackaged Pydantic Programs ```{toctree} --- maxdepth: 1 --- /examples/output_parsing/df_program.ipynb /examples/output_parsing/evaporate_program.ipynb ```\", \"Structured Outputs  The ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values.  LlamaIndex itself also relies on structured output in the following ways. - **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \\\\\"ANSWER: (number)\\\\\". - **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)  LlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction: - **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box). - **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers. - **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).  See the sections below for an overview of output parsers and Pydantic programs.\", \"\\\\ud83d\\\\udd2c Anatomy of a Structured Output Function  Here we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you\\'re using a **generic LLM text completion API** or an **LLM function calling API**.  !  With generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can append format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.  With function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\", \"Output Parser Modules  ```{toctree} --- maxdepth: 2 --- output_parser.md ```\", \"Pydantic Program Modules  ```{toctree} --- maxdepth: 2 --- pydantic_program.md ```\", \"Callbacks\", \"Concept LlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.  Using the callback manager, as many callbacks as needed can be added.  In addition to logging data related to events, you can also track the duration and number of occurances of each event.   Furthermore, a trace map of events is also recorded, and callbacks can use this data however they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events after most operations.  **Callback Event Types**   While each callback may not leverage each event type, the following events are available to be tracked:  - `CHUNKING` -> Logs for the before and after of text splitting. - `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into. - `EMBEDDING` -> Logs for the number of texts embedded. - `LLM` -> Logs for the template and response of LLM calls. - `QUERY` -> Keeps track of the start and end of each query. - `RETRIEVE` -> Logs for the nodes retrieved for a query. - `SYNTHESIZE` -> Logs for the result for synthesize calls. - `TREE` -> Logs for the summary and level of summaries generated. - `SUB_QUESTION` -> Log for a generated sub question and answer.  You can implement your own callback to track and trace these events, or use an existing callback.\", \"Modules  Currently supported callbacks are as follows:  - TokenCountingHandler -> Flexible token counting for prompt, completion, and embedding token usage. See the migration details here - LlamaDebugHanlder -> Basic tracking and tracing for events. Example usage can be found in the notebook below. - WandbCallbackHandler -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at Wandb - AimCallback -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below. - OpenInferenceCallbackHandler -> Tracking of AI model inferences. Example usage can be found in the notebook below. - OpenAIFineTuningHandler -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.   ```{toctree} --- maxdepth: 1 hidden: --- /examples/callbacks/TokenCountingHandler.ipynb /examples/callbacks/LlamaDebugHandler.ipynb /examples/callbacks/WandbCallbackHandler.ipynb /examples/callbacks/AimCallback.ipynb /examples/callbacks/OpenInferenceCallback.ipynb token_counting_migration.md ```\", \"Token Counting - Migration Guide  The existing token counting implementation has been __deprecated__.   We know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition.   Previously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.  Going forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separete token counters for different indexes.  Here is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:  ```python import tiktoken from llama_index.callbacks import CallbackManager, TokenCountingHandler from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\", \"you can set a tokenizer directly, or optionally let it default token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model(\\\\\"text-davinci-003\\\\\").encode     verbose=False  # set to true to see usage printed to the console )  callback_manager = CallbackManager([token_counter])  service_context = ServiceContext.from_defaults(callback_manager=callback_manager)  document = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Prepackaged Pydantic Programs ```{toctree} --- maxdepth: 1 --- /examples/output_parsing/df_program.ipynb /examples/output_parsing/evaporate_program.ipynb ```\", \"Structured Outputs  The ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values.  LlamaIndex itself also relies on structured output in the following ways. - **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \\\\\"ANSWER: (number)\\\\\". - **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)  LlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction: - **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box). - **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers. - **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).  See the sections below for an overview of output parsers and Pydantic programs.\", \"\\\\ud83d\\\\udd2c Anatomy of a Structured Output Function  Here we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you\\'re using a **generic LLM text completion API** or an **LLM function calling API**.  !  With generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can append format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.  With function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\", \"Output Parser Modules  ```{toctree} --- maxdepth: 2 --- output_parser.md ```\", \"Pydantic Program Modules  ```{toctree} --- maxdepth: 2 --- pydantic_program.md ```\", \"Callbacks\", \"Concept LlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.  Using the callback manager, as many callbacks as needed can be added.  In addition to logging data related to events, you can also track the duration and number of occurances of each event.   Furthermore, a trace map of events is also recorded, and callbacks can use this data however they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events after most operations.  **Callback Event Types**   While each callback may not leverage each event type, the following events are available to be tracked:  - `CHUNKING` -> Logs for the before and after of text splitting. - `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into. - `EMBEDDING` -> Logs for the number of texts embedded. - `LLM` -> Logs for the template and response of LLM calls. - `QUERY` -> Keeps track of the start and end of each query. - `RETRIEVE` -> Logs for the nodes retrieved for a query. - `SYNTHESIZE` -> Logs for the result for synthesize calls. - `TREE` -> Logs for the summary and level of summaries generated. - `SUB_QUESTION` -> Log for a generated sub question and answer.  You can implement your own callback to track and trace these events, or use an existing callback.\", \"Modules  Currently supported callbacks are as follows:  - TokenCountingHandler -> Flexible token counting for prompt, completion, and embedding token usage. See the migration details here - LlamaDebugHanlder -> Basic tracking and tracing for events. Example usage can be found in the notebook below. - WandbCallbackHandler -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at Wandb - AimCallback -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below. - OpenInferenceCallbackHandler -> Tracking of AI model inferences. Example usage can be found in the notebook below. - OpenAIFineTuningHandler -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.   ```{toctree} --- maxdepth: 1 hidden: --- /examples/callbacks/TokenCountingHandler.ipynb /examples/callbacks/LlamaDebugHandler.ipynb /examples/callbacks/WandbCallbackHandler.ipynb /examples/callbacks/AimCallback.ipynb /examples/callbacks/OpenInferenceCallback.ipynb token_counting_migration.md ```\", \"Token Counting - Migration Guide  The existing token counting implementation has been __deprecated__.   We know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition.   Previously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.  Going forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separete token counters for different indexes.  Here is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:  ```python import tiktoken from llama_index.callbacks import CallbackManager, TokenCountingHandler from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\", \"you can set a tokenizer directly, or optionally let it default token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model(\\\\\"text-davinci-003\\\\\").encode     verbose=False  # set to true to see usage printed to the console )  callback_manager = CallbackManager([token_counter])  service_context = ServiceContext.from_defaults(callback_manager=callback_manager)  document = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=d175000eacb7d504286b37426320c1da response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=d175000eacb7d504286b37426320c1da response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"if verbose is turned on, you will see embedding token usage printed index = VectorStoreIndex.from_documents(documents, service_context=service_context)\", \"otherwise, you can access the count directly print(token_counter.total_embedding_token_count)\", \"reset the counts at your discretion! token_counter.reset_counts()\", \"also track prompt, completion, and total LLM tokens, in addition to embeddings response = index.as_query_engine().query(\\\\\"What did the author do growing up?\\\\\") print(\\'Embedding Tokens: \\', token_counter.total_embedding_token_count, \\'\\\\\\\\n\\',       \\'LLM Prompt Tokens: \\', token_counter.prompt_llm_token_count, \\'\\\\\\\\n\\',       \\'LLM Completion Tokens: \\', token_counter.completion_llm_token_count, \\'\\\\\\\\n\\',       \\'Total LLM Token Count: \\', token_counter.total_llm_token_count) ```\", \"Cost Analysis\", \"Concept Each call to an LLM will cost some amount of money - for instance, OpenAI\\'s gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on   - the type of LLM used - the type of data structure used - parameters used during building  - parameters used during querying  The cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:  1. A high-level overview of the cost structure of the indices. 2. A token predictor that you can use directly within LlamaIndex!\", \"Overview of Cost Structure\", \"Indices with no LLM calls The following indices don\\'t require LLM calls at all during building (0 cost): - `ListIndex` - `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document - `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\", \"Indices with LLM calls The following indices do require LLM calls during build time: - `TreeIndex` - use LLM to hierarchically summarize the text to build the tree - `KeywordTableIndex` - use LLM to extract keywords from each document\", \"Query Time  There will always be >= 1 LLM call during query time, in order to synthesize the final answer.  Some indices contain cost tradeoffs between index building and querying. `ListIndex`, for instance, is free to build, but running a query over a list index (without filtering or embedding lookups), will call the LLM {math}`N` times.  Here are some notes regarding each of the indices: - `ListIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes. - `TreeIndex`: by default requires {math}`\\\\\\\\log (N)` LLM calls, where N is the number of leaf nodes.      - Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node. - `KeywordTableIndex`: by default requires an LLM call to extract query keywords.     - Can do `index.as_retriever(retriever_mode=\\\\\"simple\\\\\")` or `index.as_retriever(retriever_mode=\\\\\"rake\\\\\")` to also use regex/RAKE keyword extractors on your query text. -  `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"if verbose is turned on, you will see embedding token usage printed index = VectorStoreIndex.from_documents(documents, service_context=service_context)\", \"otherwise, you can access the count directly print(token_counter.total_embedding_token_count)\", \"reset the counts at your discretion! token_counter.reset_counts()\", \"also track prompt, completion, and total LLM tokens, in addition to embeddings response = index.as_query_engine().query(\\\\\"What did the author do growing up?\\\\\") print(\\'Embedding Tokens: \\', token_counter.total_embedding_token_count, \\'\\\\\\\\n\\',       \\'LLM Prompt Tokens: \\', token_counter.prompt_llm_token_count, \\'\\\\\\\\n\\',       \\'LLM Completion Tokens: \\', token_counter.completion_llm_token_count, \\'\\\\\\\\n\\',       \\'Total LLM Token Count: \\', token_counter.total_llm_token_count) ```\", \"Cost Analysis\", \"Concept Each call to an LLM will cost some amount of money - for instance, OpenAI\\'s gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on   - the type of LLM used - the type of data structure used - parameters used during building  - parameters used during querying  The cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:  1. A high-level overview of the cost structure of the indices. 2. A token predictor that you can use directly within LlamaIndex!\", \"Overview of Cost Structure\", \"Indices with no LLM calls The following indices don\\'t require LLM calls at all during building (0 cost): - `ListIndex` - `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document - `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\", \"Indices with LLM calls The following indices do require LLM calls during build time: - `TreeIndex` - use LLM to hierarchically summarize the text to build the tree - `KeywordTableIndex` - use LLM to extract keywords from each document\", \"Query Time  There will always be >= 1 LLM call during query time, in order to synthesize the final answer.  Some indices contain cost tradeoffs between index building and querying. `ListIndex`, for instance, is free to build, but running a query over a list index (without filtering or embedding lookups), will call the LLM {math}`N` times.  Here are some notes regarding each of the indices: - `ListIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes. - `TreeIndex`: by default requires {math}`\\\\\\\\log (N)` LLM calls, where N is the number of leaf nodes.      - Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node. - `KeywordTableIndex`: by default requires an LLM call to extract query keywords.     - Can do `index.as_retriever(retriever_mode=\\\\\"simple\\\\\")` or `index.as_retriever(retriever_mode=\\\\\"rake\\\\\")` to also use regex/RAKE keyword extractors on your query text. -  `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=68 request_id=3c74cd9dbd734151bbfc7371b7baab69 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=68 request_id=3c74cd9dbd734151bbfc7371b7baab69 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Usage Pattern  LlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls. This allows you to estimate your costs during 1) index construction, and 2) index querying, before any respective LLM calls are made.  Tokens are counted using the `TokenCountingHandler` callback. See the example notebook for details on the setup.\", \"Using MockLLM  To predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \\\\\"worst case\\\\\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.  ```python from llama_index import ServiceContext, set_global_service_context from llama_index.llms import MockLLM  llm = MockLLM(max_tokens=256)  service_context = ServiceContext.from_defaults(llm=llm)\", \"optionally set a global service context set_global_service_context(service_context) ```  You can then use this predictor during both index construction and querying.\", \"Using MockEmbedding  You may also predict the token usage of embedding calls with `MockEmbedding`.   ```python from llama_index import ServiceContext, set_global_service_context from llama_index import MockEmbedding\", \"specify a MockLLMPredictor embed_model = MockEmbedding(embed_dim=1536)  service_context = ServiceContext.from_defaults(embed_model=embed_model)\", \"optionally set a global service context set_global_service_context(service_context) ```\", \"Usage Pattern  Read about the full usage pattern below!  ```{toctree} --- caption: Examples maxdepth: 1 --- usage_pattern.md ```\", \"Usage Pattern\", \"Estimating LLM and Embedding Token Counts  In order to measure LLM and Embedding token counts, you\\'ll need to  1. Setup `MockLLM` and `MockEmbedding` objects  ```python from llama_index.llms import MockLLM from llama_index import MockEmbedding  llm = MockLLM(max_tokens=256) embed_model = MockEmbedding(embed_dim=1536) ```  2. Setup the `TokenCountingCallback` handler  ```python import tiktoken from llama_index.callbacks import CallbackManager, TokenCountingHandler  token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  callback_manager = CallbackManager([token_counter]) ```  3. Add them to the global `ServiceContext`  ```python from llama_index import ServiceContext, set_global_service_context  set_global_service_context(     ServiceContext.from_defaults(         llm=llm,          embed_model=embed_model,          callback_manager=callback_manager     ) ) ```  4. Construct an Index   ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\\\\"./docs/examples/data/paul_graham\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents) ```  5. Measure the counts!  ```python print(     \\\\\"Embedding Tokens: \\\\\",     token_counter.total_embedding_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Prompt Tokens: \\\\\",     token_counter.prompt_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Completion Tokens: \\\\\",     token_counter.completion_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"Total LLM Token Count: \\\\\",     token_counter.total_llm_token_count,     \\\\\"\\\\\\\\n\\\\\", )\", \"reset counts token_counter.reset_counts() ```  6. Run a query, mesaure again  ```python query_engine = index.as_query_engine()  response = query_engine.query(\\\\\"query\\\\\")  print(     \\\\\"Embedding Tokens: \\\\\",     token_counter.total_embedding_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Prompt Tokens: \\\\\",     token_counter.prompt_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Completion Tokens: \\\\\",     token_counter.completion_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"Total LLM Token Count: \\\\\",     token_counter.total_llm_token_count,     \\\\\"\\\\\\\\n\\\\\", ) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Usage Pattern  LlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls. This allows you to estimate your costs during 1) index construction, and 2) index querying, before any respective LLM calls are made.  Tokens are counted using the `TokenCountingHandler` callback. See the example notebook for details on the setup.\", \"Using MockLLM  To predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \\\\\"worst case\\\\\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.  ```python from llama_index import ServiceContext, set_global_service_context from llama_index.llms import MockLLM  llm = MockLLM(max_tokens=256)  service_context = ServiceContext.from_defaults(llm=llm)\", \"optionally set a global service context set_global_service_context(service_context) ```  You can then use this predictor during both index construction and querying.\", \"Using MockEmbedding  You may also predict the token usage of embedding calls with `MockEmbedding`.   ```python from llama_index import ServiceContext, set_global_service_context from llama_index import MockEmbedding\", \"specify a MockLLMPredictor embed_model = MockEmbedding(embed_dim=1536)  service_context = ServiceContext.from_defaults(embed_model=embed_model)\", \"optionally set a global service context set_global_service_context(service_context) ```\", \"Usage Pattern  Read about the full usage pattern below!  ```{toctree} --- caption: Examples maxdepth: 1 --- usage_pattern.md ```\", \"Usage Pattern\", \"Estimating LLM and Embedding Token Counts  In order to measure LLM and Embedding token counts, you\\'ll need to  1. Setup `MockLLM` and `MockEmbedding` objects  ```python from llama_index.llms import MockLLM from llama_index import MockEmbedding  llm = MockLLM(max_tokens=256) embed_model = MockEmbedding(embed_dim=1536) ```  2. Setup the `TokenCountingCallback` handler  ```python import tiktoken from llama_index.callbacks import CallbackManager, TokenCountingHandler  token_counter = TokenCountingHandler(     tokenizer=tiktoken.encoding_for_model(\\\\\"gpt-3.5-turbo\\\\\").encode )  callback_manager = CallbackManager([token_counter]) ```  3. Add them to the global `ServiceContext`  ```python from llama_index import ServiceContext, set_global_service_context  set_global_service_context(     ServiceContext.from_defaults(         llm=llm,          embed_model=embed_model,          callback_manager=callback_manager     ) ) ```  4. Construct an Index   ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\\\\"./docs/examples/data/paul_graham\\\\\").load_data()  index = VectorStoreIndex.from_documents(documents) ```  5. Measure the counts!  ```python print(     \\\\\"Embedding Tokens: \\\\\",     token_counter.total_embedding_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Prompt Tokens: \\\\\",     token_counter.prompt_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Completion Tokens: \\\\\",     token_counter.completion_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"Total LLM Token Count: \\\\\",     token_counter.total_llm_token_count,     \\\\\"\\\\\\\\n\\\\\", )\", \"reset counts token_counter.reset_counts() ```  6. Run a query, mesaure again  ```python query_engine = index.as_query_engine()  response = query_engine.query(\\\\\"query\\\\\")  print(     \\\\\"Embedding Tokens: \\\\\",     token_counter.total_embedding_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Prompt Tokens: \\\\\",     token_counter.prompt_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"LLM Completion Tokens: \\\\\",     token_counter.completion_llm_token_count,     \\\\\"\\\\\\\\n\\\\\",     \\\\\"Total LLM Token Count: \\\\\",     token_counter.total_llm_token_count,     \\\\\"\\\\\\\\n\\\\\", ) ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=11ce99074f52a5de9d3044b0a2b6255a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=40 request_id=11ce99074f52a5de9d3044b0a2b6255a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Modules  Notebooks with usage of these components can be found below.  ```{toctree} --- maxdepth: 1 ---  ../../../examples/evaluation/TestNYC-Evaluation.ipynb ../../../examples/evaluation/TestNYC-Evaluation-Query.ipynb ../../../examples/evaluation/QuestionGeneration.ipynb ```\", \"Evaluation\", \"Concept Evaluation in generative AI and retrieval is a difficult task. Due to the unpredictable nature of text, and a general lack of \\\\\"expected\\\\\" outcomes to compare against, there are many blockers to getting started with evaluation.  However, LlamaIndex offers a few key modules for evaluating the quality of both Document retrieval and response synthesis. Here are some key questions for each component:  - **Document retrieval**: Are the sources relevant to the query? - **Response synthesis**: Does the response match the retrieved context? Does it also match the query?   This guide describes how the evaluation components within LlamaIndex work. Note that our current evaluation modules do *not* require ground-truth labels. Evaluation can be done with some combination of the query, context, response, and combine these with LLM calls.\", \"Evaluation of the Response + Context  Each response from a `query_engine.query` calls returns both the synthesized response as well as source documents.  We can evaluate the response against the retrieved sources - without taking into account the query!  This allows you to measure hallucination - if the response does not match the retrieved sources, this means that the model may be \\\\\"hallucinating\\\\\" an answer since it is not rooting the answer in the context provided to it in the prompt.  There are two sub-modes of evaluation here. We can either get a binary response \\\\\"YES\\\\\"/\\\\\"NO\\\\\" on whether response matches *any* source context, and also get a response list across sources to see which sources match.  The `ResponseEvaluator` handles both modes for evaluating in this context.\", \"Evaluation of the Query + Response + Source Context  This is similar to the above section, except now we also take into account the query. The goal is to determine if the response + source context answers the query.  As with the above, there are two submodes of evaluation.  - We can either get a binary response \\\\\"YES\\\\\"/\\\\\"NO\\\\\" on whether the response matches the query, and whether any source node also matches the query. - We can also ignore the synthesized response, and check every source node to see if it matches the query.\", \"Question Generation  In addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\", \"Usage Pattern  For full usage details, see the usage pattern below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  Notebooks with usage of these components can be found below.  ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Usage Pattern\", \"Evaluating Response for Hallucination\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Modules  Notebooks with usage of these components can be found below.  ```{toctree} --- maxdepth: 1 ---  ../../../examples/evaluation/TestNYC-Evaluation.ipynb ../../../examples/evaluation/TestNYC-Evaluation-Query.ipynb ../../../examples/evaluation/QuestionGeneration.ipynb ```\", \"Evaluation\", \"Concept Evaluation in generative AI and retrieval is a difficult task. Due to the unpredictable nature of text, and a general lack of \\\\\"expected\\\\\" outcomes to compare against, there are many blockers to getting started with evaluation.  However, LlamaIndex offers a few key modules for evaluating the quality of both Document retrieval and response synthesis. Here are some key questions for each component:  - **Document retrieval**: Are the sources relevant to the query? - **Response synthesis**: Does the response match the retrieved context? Does it also match the query?   This guide describes how the evaluation components within LlamaIndex work. Note that our current evaluation modules do *not* require ground-truth labels. Evaluation can be done with some combination of the query, context, response, and combine these with LLM calls.\", \"Evaluation of the Response + Context  Each response from a `query_engine.query` calls returns both the synthesized response as well as source documents.  We can evaluate the response against the retrieved sources - without taking into account the query!  This allows you to measure hallucination - if the response does not match the retrieved sources, this means that the model may be \\\\\"hallucinating\\\\\" an answer since it is not rooting the answer in the context provided to it in the prompt.  There are two sub-modes of evaluation here. We can either get a binary response \\\\\"YES\\\\\"/\\\\\"NO\\\\\" on whether response matches *any* source context, and also get a response list across sources to see which sources match.  The `ResponseEvaluator` handles both modes for evaluating in this context.\", \"Evaluation of the Query + Response + Source Context  This is similar to the above section, except now we also take into account the query. The goal is to determine if the response + source context answers the query.  As with the above, there are two submodes of evaluation.  - We can either get a binary response \\\\\"YES\\\\\"/\\\\\"NO\\\\\" on whether the response matches the query, and whether any source node also matches the query. - We can also ignore the synthesized response, and check every source node to see if it matches the query.\", \"Question Generation  In addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\", \"Usage Pattern  For full usage details, see the usage pattern below.  ```{toctree} --- maxdepth: 1 --- usage_pattern.md ```\", \"Modules  Notebooks with usage of these components can be found below.  ```{toctree} --- maxdepth: 1 --- modules.md ```\", \"Usage Pattern\", \"Evaluating Response for Hallucination\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=71 request_id=758def131270babcc297638bc56f92bd response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=71 request_id=758def131270babcc297638bc56f92bd response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Binary Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" if the synthesized response matches any source context.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = ResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() response = query_engine.query(\\\\\"What battles took place in New York City in the American Revolution?\\\\\") eval_result = evaluator.evaluate(response) print(str(eval_result))  ```  You\\'ll get back either a `YES` or `NO` response.  !\", \"Sources Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" for every source node.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = ResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() response = query_engine.query(\\\\\"What battles took place in New York City in the American Revolution?\\\\\") eval_result = evaluator.evaluate_source_nodes(response) print(str(eval_result))  ```  You\\'ll get back a list of \\\\\"YES\\\\\"/\\\\\"NO\\\\\", corresponding to each source node in `response.source_nodes`.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Binary Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" if the synthesized response matches any source context.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = ResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() response = query_engine.query(\\\\\"What battles took place in New York City in the American Revolution?\\\\\") eval_result = evaluator.evaluate(response) print(str(eval_result))  ```  You\\'ll get back either a `YES` or `NO` response.  !\", \"Sources Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" for every source node.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = ResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() response = query_engine.query(\\\\\"What battles took place in New York City in the American Revolution?\\\\\") eval_result = evaluator.evaluate_source_nodes(response) print(str(eval_result))  ```  You\\'ll get back a list of \\\\\"YES\\\\\"/\\\\\"NO\\\\\", corresponding to each source node in `response.source_nodes`.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=45 request_id=9bd7b07a04b034d5d624a385f3c0cb17 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=45 request_id=9bd7b07a04b034d5d624a385f3c0cb17 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Evaluting Query + Response for Answer Quality\", \"Binary Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" if the synthesized response matches the query + any source context.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import QueryResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = QueryResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() query = \\\\\"What battles took place in New York City in the American Revolution?\\\\\" response = query_engine.query(query) eval_result = evaluator.evaluate(query, response) print(str(eval_result))  ```  !\", \"Sources Evaluation  This mode of evaluation will look at each source node, and see if each source node contains an answer to the query.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import QueryResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = QueryResponseEvaluator(service_context=service_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Evaluting Query + Response for Answer Quality\", \"Binary Evaluation  This mode of evaluation will return \\\\\"YES\\\\\"/\\\\\"NO\\\\\" if the synthesized response matches the query + any source context.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import QueryResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = QueryResponseEvaluator(service_context=service_context)\", \"query index query_engine = vector_index.as_query_engine() query = \\\\\"What battles took place in New York City in the American Revolution?\\\\\" response = query_engine.query(query) eval_result = evaluator.evaluate(query, response) print(str(eval_result))  ```  !\", \"Sources Evaluation  This mode of evaluation will look at each source node, and see if each source node contains an answer to the query.  ```python from llama_index import VectorStoreIndex, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import QueryResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build index ...\", \"define evaluator evaluator = QueryResponseEvaluator(service_context=service_context)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=304 request_id=2ac6d46e132b62d3bc83bed5d0887442 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=304 request_id=2ac6d46e132b62d3bc83bed5d0887442 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"query index query_engine = vector_index.as_query_engine() query = \\\\\"What battles took place in New York City in the American Revolution?\\\\\" response = query_engine.query(query) eval_result = evaluator.evaluate_source_nodes(query, response) print(str(eval_result)) ```  !\", \"Question Generation  LlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.  ```python from llama_index import SimpleDirectoryReader, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build documents documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\", \"define genertor, generate questions data_generator = DatasetGenerator.from_documents(documents)  eval_questions = data_generator.generate_questions_from_nodes() ```\", \"Playground\", \"Concept  The Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.  For each combination, you\\'ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.  You may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\", \"Usage Pattern  A sample usage is given below.  ```python from llama_index import download_loader from llama_index.indices.vector_store import VectorStoreIndex from llama_index.indices.tree.base import TreeIndex from llama_index.playground import Playground\", \"load data WikipediaReader = download_loader(\\\\\"WikipediaReader\\\\\") loader = WikipediaReader() documents = loader.load_data(pages=[\\'Berlin\\'])\", \"define multiple index data structures (vector index, list index) indices = [VectorStoreIndex(documents), TreeIndex(documents)]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"query index query_engine = vector_index.as_query_engine() query = \\\\\"What battles took place in New York City in the American Revolution?\\\\\" response = query_engine.query(query) eval_result = evaluator.evaluate_source_nodes(query, response) print(str(eval_result)) ```  !\", \"Question Generation  LlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.  ```python from llama_index import SimpleDirectoryReader, ServiceContext from llama_index.llms import OpenAI from llama_index.evaluation import ResponseEvaluator\", \"build service context llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0.0) service_context = ServiceContext.from_defaults(llm=llm)\", \"build documents documents = SimpleDirectoryReader(\\\\\"./data\\\\\").load_data()\", \"define genertor, generate questions data_generator = DatasetGenerator.from_documents(documents)  eval_questions = data_generator.generate_questions_from_nodes() ```\", \"Playground\", \"Concept  The Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.  For each combination, you\\'ll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.  You may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\", \"Usage Pattern  A sample usage is given below.  ```python from llama_index import download_loader from llama_index.indices.vector_store import VectorStoreIndex from llama_index.indices.tree.base import TreeIndex from llama_index.playground import Playground\", \"load data WikipediaReader = download_loader(\\\\\"WikipediaReader\\\\\") loader = WikipediaReader() documents = loader.load_data(pages=[\\'Berlin\\'])\", \"define multiple index data structures (vector index, list index) indices = [VectorStoreIndex(documents), TreeIndex(documents)]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=67 request_id=3cb187eaaf50c28a0b0fc301c26d9672 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=67 request_id=3cb187eaaf50c28a0b0fc301c26d9672 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"initialize playground playground = Playground(indices=indices)\", \"playground compare playground.compare(\\\\\"What is the population of Berlin?\\\\\")  ```\", \"Modules  ```{toctree} --- maxdepth: 1 --- ../../../examples/analysis/PlaygroundDemo.ipynb ```\", \"ServiceContext\", \"Concept The `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the global configuration, as well as local configurations at specific parts of the pipeline.\", \"Usage Pattern\", \"Configuring the service context The `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.  ```python @dataclass class ServiceContext:     # The LLM used to generate natural language responses to queries.     # If not provided, defaults to gpt-3.5-turbo from OpenAI     # If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp     llm: LLM      # The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM\\'s context window.     prompt_helper: PromptHelper      # The embedding model used to generate vector representations of text.     # If not provided, defaults to text-embedding-ada-002     # If your OpenAI key is not set, defaults to BAAI/bge-small-en     embed_model: BaseEmbedding      # The parser that converts documents into nodes.     node_parser: NodeParser      # The callback manager object that calls it\\'s handlers on events. Provides basic logging and tracing capabilities.     callback_manager: CallbackManager      @classmethod     def from_defaults(cls, ...) -> \\\\\"ServiceContext\\\\\":       ...  ```  ```{tip} Learn how to configure specific modules: - LLM - Embedding Model - Node Parser  ```  We also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method for convenience (so you don\\'t have to manually construct them).   **Kwargs for node parser**: - `chunk_size`: The size of the text chunk for a node . Is used for the node parser when they aren\\'t provided. - `chunk_overlap`: The amount of overlap between nodes (i.e. text chunks).  **Kwargs for prompt helper**: - `context_window`: The size of the context window of the LLM. Typically we set this    automatically with the model metadata. But we also allow explicit override via this parameter   for additional control (or in case the default is not available for certain latest   models) - `num_output`: The number of maximum output from the LLM. Typically we set this   automatically given the model metadata. This parameter does not actually limit the model   output, it affects the amount of \\\\\"space\\\\\" we save for the output, when computing    available context window size for packing text from retrieved Nodes.  Here\\'s a complete example that sets up all objects using their default settings:  ```python from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper from llama_index.llms import OpenAI from llama_index.text_splitter import TokenTextSplitter from llama_index.node_parser import SimpleNodeParser  llm = OpenAI(model=\\'text-davinci-003\\', temperature=0, max_tokens=256) embed_model = OpenAIEmbedding() node_parser = SimpleNodeParser.from_defaults(   text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20) ) prompt_helper = PromptHelper(   context_window=4096,    num_output=256,    chunk_overlap_ratio=0.1,    chunk_size_limit=None )  service_context = ServiceContext.from_defaults(   llm=llm,   embed_model=embed_model,   node_parser=node_parser,   prompt_helper=prompt_helper ) ```\", \"Setting global configuration You can set a service context as the global default that applies to the entire LlamaIndex pipeline:  ```python from llama_index import set_global_service_context set_global_service_context(service_context) ```\", \"Setting local configuration You can pass in a service context to specific part of the pipeline to override the default configuration:   ```python query_engine = index.as_query_engine(service_context=service_context) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```\", \"Deprecated Terms  As LlamaIndex continues to evolve, many class names and APIs have been adjusted, improved, and deprecated.  The following is a list of previously popular terms that have been deprecated, with links to their replacements.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"initialize playground playground = Playground(indices=indices)\", \"playground compare playground.compare(\\\\\"What is the population of Berlin?\\\\\")  ```\", \"Modules  ```{toctree} --- maxdepth: 1 --- ../../../examples/analysis/PlaygroundDemo.ipynb ```\", \"ServiceContext\", \"Concept The `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the global configuration, as well as local configurations at specific parts of the pipeline.\", \"Usage Pattern\", \"Configuring the service context The `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.  ```python @dataclass class ServiceContext:     # The LLM used to generate natural language responses to queries.     # If not provided, defaults to gpt-3.5-turbo from OpenAI     # If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp     llm: LLM      # The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM\\'s context window.     prompt_helper: PromptHelper      # The embedding model used to generate vector representations of text.     # If not provided, defaults to text-embedding-ada-002     # If your OpenAI key is not set, defaults to BAAI/bge-small-en     embed_model: BaseEmbedding      # The parser that converts documents into nodes.     node_parser: NodeParser      # The callback manager object that calls it\\'s handlers on events. Provides basic logging and tracing capabilities.     callback_manager: CallbackManager      @classmethod     def from_defaults(cls, ...) -> \\\\\"ServiceContext\\\\\":       ...  ```  ```{tip} Learn how to configure specific modules: - LLM - Embedding Model - Node Parser  ```  We also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method for convenience (so you don\\'t have to manually construct them).   **Kwargs for node parser**: - `chunk_size`: The size of the text chunk for a node . Is used for the node parser when they aren\\'t provided. - `chunk_overlap`: The amount of overlap between nodes (i.e. text chunks).  **Kwargs for prompt helper**: - `context_window`: The size of the context window of the LLM. Typically we set this    automatically with the model metadata. But we also allow explicit override via this parameter   for additional control (or in case the default is not available for certain latest   models) - `num_output`: The number of maximum output from the LLM. Typically we set this   automatically given the model metadata. This parameter does not actually limit the model   output, it affects the amount of \\\\\"space\\\\\" we save for the output, when computing    available context window size for packing text from retrieved Nodes.  Here\\'s a complete example that sets up all objects using their default settings:  ```python from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper from llama_index.llms import OpenAI from llama_index.text_splitter import TokenTextSplitter from llama_index.node_parser import SimpleNodeParser  llm = OpenAI(model=\\'text-davinci-003\\', temperature=0, max_tokens=256) embed_model = OpenAIEmbedding() node_parser = SimpleNodeParser.from_defaults(   text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20) ) prompt_helper = PromptHelper(   context_window=4096,    num_output=256,    chunk_overlap_ratio=0.1,    chunk_size_limit=None )  service_context = ServiceContext.from_defaults(   llm=llm,   embed_model=embed_model,   node_parser=node_parser,   prompt_helper=prompt_helper ) ```\", \"Setting global configuration You can set a service context as the global default that applies to the entire LlamaIndex pipeline:  ```python from llama_index import set_global_service_context set_global_service_context(service_context) ```\", \"Setting local configuration You can pass in a service context to specific part of the pipeline to override the default configuration:   ```python query_engine = index.as_query_engine(service_context=service_context) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```\", \"Deprecated Terms  As LlamaIndex continues to evolve, many class names and APIs have been adjusted, improved, and deprecated.  The following is a list of previously popular terms that have been deprecated, with links to their replacements.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=73 request_id=5d1f9b1eefafb4c3766127113e8002fa response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=73 request_id=5d1f9b1eefafb4c3766127113e8002fa response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"GPTSimpleVectorIndex  This has been renamed to `VectorStoreIndex`, as well as unifying all vector indexes to a single unified interface. You can integrate with various vector databases by modifying the underlying `vector_store`.   Please see the following links for more details on usage.  - Index Usage Pattern - Vector Store Guide - Vector Store Integrations\", \"GPTVectorStoreIndex  This has been renamed to `VectorStoreIndex`, but it is only a cosmetic change. Please see the following links for more details on usage.  - Index Usage Pattern - Vector Store Guide - Vector Store Integrations\", \"LLMPredictor  The `LLMPredictor` object is no longer intended to be used by users. Instead, you can setup an LLM directly and pass it into the `ServiceContext`.  - LLMs in LlamaIndex - Setting LLMs in the ServiceContext\", \"PromptHelper and max_input_size/  The `max_input_size` parameter for the prompt helper has since been replaced with `context_window`.  The `PromptHelper` in general has been deprecated in favour of specifying parameters directly in the `service_context` and `node_parser`.  See the following links for more details.  - Configuring settings in the Service Context - Parsing Documents into Nodes\", \".. mdinclude:: ../../CHANGELOG.md\", \".. mdinclude:: ../../CONTRIBUTING.md\", \".. mdinclude:: ../DOCS_README.md\", \"Privacy and Security By default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\", \"Data Privacy Regarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI\\'s policies. And each custom service other than OpenAI have their own policies as well.\", \"Vector stores LLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"GPTSimpleVectorIndex  This has been renamed to `VectorStoreIndex`, as well as unifying all vector indexes to a single unified interface. You can integrate with various vector databases by modifying the underlying `vector_store`.   Please see the following links for more details on usage.  - Index Usage Pattern - Vector Store Guide - Vector Store Integrations\", \"GPTVectorStoreIndex  This has been renamed to `VectorStoreIndex`, but it is only a cosmetic change. Please see the following links for more details on usage.  - Index Usage Pattern - Vector Store Guide - Vector Store Integrations\", \"LLMPredictor  The `LLMPredictor` object is no longer intended to be used by users. Instead, you can setup an LLM directly and pass it into the `ServiceContext`.  - LLMs in LlamaIndex - Setting LLMs in the ServiceContext\", \"PromptHelper and max_input_size/  The `max_input_size` parameter for the prompt helper has since been replaced with `context_window`.  The `PromptHelper` in general has been deprecated in favour of specifying parameters directly in the `service_context` and `node_parser`.  See the following links for more details.  - Configuring settings in the Service Context - Parsing Documents into Nodes\", \".. mdinclude:: ../../CHANGELOG.md\", \".. mdinclude:: ../../CONTRIBUTING.md\", \".. mdinclude:: ../DOCS_README.md\", \"Privacy and Security By default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\", \"Data Privacy Regarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI\\'s policies. And each custom service other than OpenAI have their own policies as well.\", \"Vector stores LLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=147 request_id=99cbf6bfc341c555997628fa12dfec51 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=147 request_id=99cbf6bfc341c555997628fa12dfec51 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Agents\", \"Context An \\\\\"agent\\\\\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing that query in order to return the correct result. The key agent components can include, but are not limited to: - Breaking down a complex question into smaller ones - Choosing an external Tool to use + coming up with parameters for calling the Tool - Planning out a set of tasks - Storing previously completed tasks in a memory module  Research developments in LLMs (e.g. ChatGPT Plugins), LLM research (ReAct, Toolformer) and LLM tooling (LangChain, Semantic Kernel) have popularized the concept of agents.\", \"Agents + LlamaIndex  LlamaIndex provides some amazing tools to manage and interact with your data within your LLM application. And it can be a core tool that you use while building an agent-based app. - On one hand, some components within LlamaIndex are \\\\\"agent-like\\\\\" - these make automated decisions to help a particular use case over your data. - On the other hand, LlamaIndex can be used as a core Tool within another agent framework.  In general, LlamaIndex components offer more explicit, constrained behavior for more specific use cases. Agent frameworks such as ReAct (implemented in LangChain) offer agents that are more unconstrained +  capable of general reasoning.   There are tradeoffs for using both - less-capable LLMs typically do better with more constraints. Take a look at our blog post on this for  a more information + a detailed analysis.\", \"\\\\\"Agent-like\\\\\" Components within LlamaIndex  LlamaIndex provides core modules capable of automated reasoning for different use cases over your data. Please check out our use cases doc for more details on high-level use cases that LlamaIndex can help fulfill.  Some of these core modules are shown below along with example tutorials (not comprehensive, please click into the guides/how-tos for more details).  **SubQuestionQueryEngine for Multi-Document Analysis** - Usage - Sub Question Query Engine (Intro) - 10Q Analysis (Uber) - 10K Analysis (Uber and Lyft)   **Query Transformations** - How-To - Multi-Step Query Decomposition (Notebook)  **Routing** - Usage - Router Query Engine Guide (Notebook)  **LLM Reranking** - Second Stage Processing How-To - LLM Reranking Guide (Great Gatsby)  **Chat Engines** - Chat Engines How-To\", \"Using LlamaIndex as as Tool within an Agent Framework  LlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\", \"LangChain  We have deep integrations with LangChain.  LlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!  **Resources** - LangChain integration guide - Building a Chatbot Tutorial (LangChain + LlamaIndex) - OnDemandLoaderTool Tutorial\", \"ChatGPT  LlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).  **Resources** - LlamaIndex ChatGPT Retrieval Plugin\", \"Native OpenAIAgent  With the new OpenAI API that supports function calling, it\\\\u2019s never been easier to build your own agent!  Learn how to write your own OpenAI agent in **under 50 lines of code**, or directly use our super simple `OpenAIAgent` implementation.  ```{toctree} --- maxdepth: 1 --- /examples/agent/openai_agent.ipynb /examples/agent/openai_agent_with_query_engine.ipynb /examples/agent/openai_agent_retrieval.ipynb /examples/agent/openai_agent_query_cookbook.ipynb /examples/agent/openai_agent_query_plan.ipynb /examples/agent/openai_agent_context_retrieval.ipynb ```\", \"A Guide to Building a Full-Stack Web App with LLamaIndex  LlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.  This guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.  All code examples here are available from the llama_index_starter_pack in the flask_react folder.  The main technologies used in this guide are as follows:  - python3.11 - llama_index - flask - typescript - react\", \"Flask Backend  For this guide, our backend will use a Flask API server to communicate with our frontend code. If you prefer, you can also easily translate this to a FastAPI server, or any other python server library of your choice.  Setting up a server using Flask is easy. You import the package, create the app object, and then create your endpoints. Let\\'s create a basic skeleton for the server first:  ```python from flask import Flask  app = Flask(__name__)  @app.route(\\\\\"/\\\\\") def home():     return \\\\\"Hello World!\\\\\"  if __name__ == \\\\\"__main__\\\\\":     app.run(host=\\\\\"0.0.0.0\\\\\", port=5601) ```  _flask_demo.py_  If you run this file (`python flask_demo.py`), it will launch a server on port 5601. If you visit `http://localhost:5601/`, you will see the \\\\\"Hello World!\\\\\" text rendered in your browser. Nice!  The next step is deciding what functions we want to include in our server, and to start using LlamaIndex.  To keep things simple, the most basic operation we can provide is querying an existing index. Using the paul graham essay from LlamaIndex, create a documents folder and download+place the essay text file inside of it.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Agents\", \"Context An \\\\\"agent\\\\\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing that query in order to return the correct result. The key agent components can include, but are not limited to: - Breaking down a complex question into smaller ones - Choosing an external Tool to use + coming up with parameters for calling the Tool - Planning out a set of tasks - Storing previously completed tasks in a memory module  Research developments in LLMs (e.g. ChatGPT Plugins), LLM research (ReAct, Toolformer) and LLM tooling (LangChain, Semantic Kernel) have popularized the concept of agents.\", \"Agents + LlamaIndex  LlamaIndex provides some amazing tools to manage and interact with your data within your LLM application. And it can be a core tool that you use while building an agent-based app. - On one hand, some components within LlamaIndex are \\\\\"agent-like\\\\\" - these make automated decisions to help a particular use case over your data. - On the other hand, LlamaIndex can be used as a core Tool within another agent framework.  In general, LlamaIndex components offer more explicit, constrained behavior for more specific use cases. Agent frameworks such as ReAct (implemented in LangChain) offer agents that are more unconstrained +  capable of general reasoning.   There are tradeoffs for using both - less-capable LLMs typically do better with more constraints. Take a look at our blog post on this for  a more information + a detailed analysis.\", \"\\\\\"Agent-like\\\\\" Components within LlamaIndex  LlamaIndex provides core modules capable of automated reasoning for different use cases over your data. Please check out our use cases doc for more details on high-level use cases that LlamaIndex can help fulfill.  Some of these core modules are shown below along with example tutorials (not comprehensive, please click into the guides/how-tos for more details).  **SubQuestionQueryEngine for Multi-Document Analysis** - Usage - Sub Question Query Engine (Intro) - 10Q Analysis (Uber) - 10K Analysis (Uber and Lyft)   **Query Transformations** - How-To - Multi-Step Query Decomposition (Notebook)  **Routing** - Usage - Router Query Engine Guide (Notebook)  **LLM Reranking** - Second Stage Processing How-To - LLM Reranking Guide (Great Gatsby)  **Chat Engines** - Chat Engines How-To\", \"Using LlamaIndex as as Tool within an Agent Framework  LlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\", \"LangChain  We have deep integrations with LangChain.  LlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!  **Resources** - LangChain integration guide - Building a Chatbot Tutorial (LangChain + LlamaIndex) - OnDemandLoaderTool Tutorial\", \"ChatGPT  LlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).  **Resources** - LlamaIndex ChatGPT Retrieval Plugin\", \"Native OpenAIAgent  With the new OpenAI API that supports function calling, it\\\\u2019s never been easier to build your own agent!  Learn how to write your own OpenAI agent in **under 50 lines of code**, or directly use our super simple `OpenAIAgent` implementation.  ```{toctree} --- maxdepth: 1 --- /examples/agent/openai_agent.ipynb /examples/agent/openai_agent_with_query_engine.ipynb /examples/agent/openai_agent_retrieval.ipynb /examples/agent/openai_agent_query_cookbook.ipynb /examples/agent/openai_agent_query_plan.ipynb /examples/agent/openai_agent_context_retrieval.ipynb ```\", \"A Guide to Building a Full-Stack Web App with LLamaIndex  LlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.  This guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.  All code examples here are available from the llama_index_starter_pack in the flask_react folder.  The main technologies used in this guide are as follows:  - python3.11 - llama_index - flask - typescript - react\", \"Flask Backend  For this guide, our backend will use a Flask API server to communicate with our frontend code. If you prefer, you can also easily translate this to a FastAPI server, or any other python server library of your choice.  Setting up a server using Flask is easy. You import the package, create the app object, and then create your endpoints. Let\\'s create a basic skeleton for the server first:  ```python from flask import Flask  app = Flask(__name__)  @app.route(\\\\\"/\\\\\") def home():     return \\\\\"Hello World!\\\\\"  if __name__ == \\\\\"__main__\\\\\":     app.run(host=\\\\\"0.0.0.0\\\\\", port=5601) ```  _flask_demo.py_  If you run this file (`python flask_demo.py`), it will launch a server on port 5601. If you visit `http://localhost:5601/`, you will see the \\\\\"Hello World!\\\\\" text rendered in your browser. Nice!  The next step is deciding what functions we want to include in our server, and to start using LlamaIndex.  To keep things simple, the most basic operation we can provide is querying an existing index. Using the paul graham essay from LlamaIndex, create a documents folder and download+place the essay text file inside of it.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=47 request_id=5602ae69272e352455255de1d04a6dfb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=47 request_id=5602ae69272e352455255de1d04a6dfb response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Basic Flask - Handling User Index Queries  Now, let\\'s write some code to initialize our index:  ```python import os from llama_index import SimpleDirectoryReader, VectorStoreIndex, StorageContext\", \"NOTE: for local testing only, do NOT deploy with your key hardcoded os.environ[\\'OPENAI_API_KEY\\'] = \\\\\"your key here\\\\\"  index = None  def initialize_index():     global index     storage_context = StorageContext.from_defaults()     if os.path.exists(index_dir):         index = load_index_from_storage(storage_context)     else:         documents = SimpleDirectoryReader(\\\\\"./documents\\\\\").load_data()         index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)         storage_context.persist(index_dir) ```  This function will initialize our index. If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!  Our query endpoint will accept `GET` requests with the query text as a parameter. Here\\'s what the full endpoint function will look like:  ```python from flask import request  @app.route(\\\\\"/query\\\\\", methods=[\\\\\"GET\\\\\"]) def query_index():   global index   query_text = request.args.get(\\\\\"text\\\\\", None)   if query_text is None:     return \\\\\"No text found, please include a ?text=blah parameter in the URL\\\\\", 400   query_engine = index.as_query_engine()   response = query_engine.query(query_text)   return str(response), 200 ```  Now, we\\'ve introduced a few new concepts to our server:  - a new `/query` endpoint, defined by the function decorator - a new import from flask, `request`, which is used to get parameters from the request - if the `text` parameter is missing, then we return an error message and an appropriate HTML response code - otherwise, we query the index, and return the response as a string  A full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \\\\\"%20\\\\\" characters).  Things are looking pretty good! We now have a functional API. Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.\", \"Advanced Flask - Handling User Document Uploads  Things are looking pretty cool, but how can we take this a step further? What if we want to allow users to build their own indexes by uploading their own documents? Have no fear, Flask can handle it all :muscle:.  To let users upload documents, we have to take some extra precautions. Instead of querying an existing index, the index will become **mutable**. If you have many users adding to the same index, we need to think about how to handle concurrency. Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.  One option might be to create an index for each user or group, and store and fetch things from S3. But for this example, we will assume there is one locally stored index that users are interacting with.  To handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks. This sounds scary, but it\\'s not so bad! We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \\\\\"index_server\\\\\", which will be called from our Flask server.  Here\\'s a basic example of what our `index_server.py` will look like after we\\'ve moved our code:  ```python import os from multiprocessing import Lock from multiprocessing.managers import BaseManager from llama_index import SimpleDirectoryReader, VectorStoreIndex, Document\", \"NOTE: for local testing only, do NOT deploy with your key hardcoded os.environ[\\'OPENAI_API_KEY\\'] = \\\\\"your key here\\\\\"  index = None lock = Lock()  def initialize_index():   global index    with lock:     # same as before ...   ...  def query_index(query_text):   global index   query_engine = index.as_query_engine()   response = query_engine.query(query_text)   return str(response)  if __name__ == \\\\\"__main__\\\\\":     # init the global index     print(\\\\\"initializing index...\\\\\")     initialize_index()      # setup server     # NOTE: you might want to handle the password in a less hardcoded way     manager = BaseManager((\\'\\', 5602), b\\'password\\')     manager.register(\\'query_index\\', query_index)     server = manager.get_server()      print(\\\\\"starting server...\\\\\")     server.serve_forever() ```  _index_server.py_  So, we\\'ve moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.  Then, we can adjust our flask code as follows:  ```python from multiprocessing.managers import BaseManager from flask import Flask, request\", \"initialize manager connection manager = BaseManager((\\'\\', 5602), b\\'password\\') manager.register(\\'query_index\\') manager.connect()  @app.route(\\\\\"/query\\\\\", methods=[\\\\\"GET\\\\\"]) def query_index():   global index   query_text = request.args.get(\\\\\"text\\\\\", None)   if query_text is None:     return \\\\\"No text found, please include a ?text=blah parameter in the URL\\\\\", 400   response = manager.query_index(query_text)._getvalue()   return str(response), 200  @app.route(\\\\\"/\\\\\") def home():     return \\\\\"Hello World!\\\\\"if __name__ == \\\\\"__main__\\\\\":     app.run(host=\\\\\"0.0.0.0\\\\\", port=5601)  ```  _flask_demo.py_  The two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.One special thing to note is that `BaseManager` servers don\\'t return objects quite as we expect.To resolve the return value into it\\'s original object, we call the `_getvalue()` function.If we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let\\'s do that first.Then, let\\'s add an endpoint to upload files!First, let\\'s define our Flask endpoint function:  ```python ... manager.register(\\'insert_into_index\\') ...  @app.route(\\\\\"/uploadFile\\\\\", methods=[\\\\\"POST\\\\\"]) def upload_file():     global manager     if \\'file\\' not in request.files:         return \\\\\"Please send a POST request with a file\\\\\", 400      filepath = None     try:         uploaded_file = request.files[\\\\\"file\\\\\"]         filename = secure_filename(uploaded_file.filename)         filepath = os.path.join(\\'documents\\', os.path.basename(filename))         uploaded_file.save(filepath)          if request.form.get(\\\\\"filename_as_doc_id\\\\\", None) is not None:             manager.insert_into_index(filepath, doc_id=filename)         else:             manager.insert_into_index(filepath)     except Exception as e:         # cleanup temp file         if filepath is not None and os.path.exists(filepath):             os.remove(filepath)         return \\\\\"Error: {}\\\\\".format(str(e)), 500      # cleanup temp file     if filepath is not None and os.path.exists(filepath):         os.remove(filepath)      return \\\\\"File inserted!\\\\\", 200 ```  Not too bad!You will notice that we write the file to disk.We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex\\'s `SimpleDirectoryReader` to take care of a bunch of more complex file formats.Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us.This will make more sense once we implement the frontend.With these more complicated requests, I also suggest using a tool like Postman.Examples of using postman to test our endpoints are in the repository for this project.Lastly, you\\'ll notice we added a new function to the manager.\", \"Let\\'s implement that inside `index_server.py`:  ```python def insert_into_index(doc_text, doc_id=None):     global index     document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]     if doc_id is not None:         document.doc_id = doc_id      with lock:         index.insert(document)         index.storage_context.persist()  ... manager.register(\\'insert_into_index\\', insert_into_index) ... ```  Easy!If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!To support some functionality in the frontend, I\\'ve adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn\\'t currently support this in a user-friendly way, but we can augment it ourselves!).Lastly, I had to add CORS support to the server using the `Flask-cors` python package.Check out the complete `flask_demo.py` and `index_server.py` scripts in the repository for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.\", \"React Frontend  Generally, React and Typescript are one of the most popular libraries and languages for writing webapps today. This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.  In the repository, the frontend code is organized inside of the `react_frontend` folder.  The most relevant part of the frontend will be the `src/apis` folder. This is where we make calls to the Flask server, supporting the following queries:  - `/query` -- make a query to the existing index - `/uploadFile` -- upload a file to the flask server for insertion into the index - `/getDocuments` -- list the current document titles and a portion of their texts  Using these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.\", \"fetchDocuments.tsx  This file contains the function to, you guessed it, fetch the list of current documents in the index. The code is as follows:  ```typescript export type Document = {   id: string;   text: string; };  const fetchDocuments = async (): Promise => {   const response = await fetch(\\\\\"http://localhost:5601/getDocuments\\\\\", {     mode: \\\\\"cors\\\\\",   });    if (!response.ok) {     return [];   }    const documentList = (await response.json()) as Document[];   return documentList; }; ```  As you can see, we make a query to the Flask server (here, it assumes running on localhost). Notice that we need to include the `mode: \\'cors\\'` option, as we are making an external request.  Then, we check if the response was ok, and if so, get the response json and return it. Here, the response json is a list of `Document` objects that are defined in the same file.\", \"queryIndex.tsx  This file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.  ```typescript export type ResponseSources = {   text: string;   doc_id: string;   start: number;   end: number;   similarity: number; };  export type QueryResponse = {   text: string;   sources: ResponseSources[]; };  const queryIndex = async (query: string): Promise => {   const queryURL = new URL(\\\\\"http://localhost:5601/query?text=1\\\\\");   queryURL.searchParams.append(\\\\\"text\\\\\", query);    const response = await fetch(queryURL, { mode: \\\\\"cors\\\\\" });   if (!response.ok) {     return { text: \\\\\"Error in query\\\\\", sources: [] };   }    const queryResponse = (await response.json()) as QueryResponse;    return queryResponse; };  export default queryIndex; ```  This is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL. Then, we check if the response is ok and return it with the appropriate typescript type.\", \"insertDocument.tsx  Probably the most complex API call is uploading a document. The function here accepts a file object and constructs a `POST` request using `FormData`.  The actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.  ```typescript const insertDocument = async (file: File) => {   const formData = new FormData();   formData.append(\\\\\"file\\\\\", file);   formData.append(\\\\\"filename_as_doc_id\\\\\", \\\\\"true\\\\\");    const response = await fetch(\\\\\"http://localhost:5601/uploadFile\\\\\", {     mode: \\\\\"cors\\\\\",     method: \\\\\"POST\\\\\",     body: formData,   });    const responseText = response.text();   return responseText; };  export default insertDocument; ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Basic Flask - Handling User Index Queries  Now, let\\'s write some code to initialize our index:  ```python import os from llama_index import SimpleDirectoryReader, VectorStoreIndex, StorageContext\", \"NOTE: for local testing only, do NOT deploy with your key hardcoded os.environ[\\'OPENAI_API_KEY\\'] = \\\\\"your key here\\\\\"  index = None  def initialize_index():     global index     storage_context = StorageContext.from_defaults()     if os.path.exists(index_dir):         index = load_index_from_storage(storage_context)     else:         documents = SimpleDirectoryReader(\\\\\"./documents\\\\\").load_data()         index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)         storage_context.persist(index_dir) ```  This function will initialize our index. If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!  Our query endpoint will accept `GET` requests with the query text as a parameter. Here\\'s what the full endpoint function will look like:  ```python from flask import request  @app.route(\\\\\"/query\\\\\", methods=[\\\\\"GET\\\\\"]) def query_index():   global index   query_text = request.args.get(\\\\\"text\\\\\", None)   if query_text is None:     return \\\\\"No text found, please include a ?text=blah parameter in the URL\\\\\", 400   query_engine = index.as_query_engine()   response = query_engine.query(query_text)   return str(response), 200 ```  Now, we\\'ve introduced a few new concepts to our server:  - a new `/query` endpoint, defined by the function decorator - a new import from flask, `request`, which is used to get parameters from the request - if the `text` parameter is missing, then we return an error message and an appropriate HTML response code - otherwise, we query the index, and return the response as a string  A full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \\\\\"%20\\\\\" characters).  Things are looking pretty good! We now have a functional API. Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.\", \"Advanced Flask - Handling User Document Uploads  Things are looking pretty cool, but how can we take this a step further? What if we want to allow users to build their own indexes by uploading their own documents? Have no fear, Flask can handle it all :muscle:.  To let users upload documents, we have to take some extra precautions. Instead of querying an existing index, the index will become **mutable**. If you have many users adding to the same index, we need to think about how to handle concurrency. Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.  One option might be to create an index for each user or group, and store and fetch things from S3. But for this example, we will assume there is one locally stored index that users are interacting with.  To handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks. This sounds scary, but it\\'s not so bad! We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \\\\\"index_server\\\\\", which will be called from our Flask server.  Here\\'s a basic example of what our `index_server.py` will look like after we\\'ve moved our code:  ```python import os from multiprocessing import Lock from multiprocessing.managers import BaseManager from llama_index import SimpleDirectoryReader, VectorStoreIndex, Document\", \"NOTE: for local testing only, do NOT deploy with your key hardcoded os.environ[\\'OPENAI_API_KEY\\'] = \\\\\"your key here\\\\\"  index = None lock = Lock()  def initialize_index():   global index    with lock:     # same as before ...   ...  def query_index(query_text):   global index   query_engine = index.as_query_engine()   response = query_engine.query(query_text)   return str(response)  if __name__ == \\\\\"__main__\\\\\":     # init the global index     print(\\\\\"initializing index...\\\\\")     initialize_index()      # setup server     # NOTE: you might want to handle the password in a less hardcoded way     manager = BaseManager((\\'\\', 5602), b\\'password\\')     manager.register(\\'query_index\\', query_index)     server = manager.get_server()      print(\\\\\"starting server...\\\\\")     server.serve_forever() ```  _index_server.py_  So, we\\'ve moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.  Then, we can adjust our flask code as follows:  ```python from multiprocessing.managers import BaseManager from flask import Flask, request\", \"initialize manager connection manager = BaseManager((\\'\\', 5602), b\\'password\\') manager.register(\\'query_index\\') manager.connect()  @app.route(\\\\\"/query\\\\\", methods=[\\\\\"GET\\\\\"]) def query_index():   global index   query_text = request.args.get(\\\\\"text\\\\\", None)   if query_text is None:     return \\\\\"No text found, please include a ?text=blah parameter in the URL\\\\\", 400   response = manager.query_index(query_text)._getvalue()   return str(response), 200  @app.route(\\\\\"/\\\\\") def home():     return \\\\\"Hello World!\\\\\"if __name__ == \\\\\"__main__\\\\\":     app.run(host=\\\\\"0.0.0.0\\\\\", port=5601)  ```  _flask_demo.py_  The two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.One special thing to note is that `BaseManager` servers don\\'t return objects quite as we expect.To resolve the return value into it\\'s original object, we call the `_getvalue()` function.If we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let\\'s do that first.Then, let\\'s add an endpoint to upload files!First, let\\'s define our Flask endpoint function:  ```python ... manager.register(\\'insert_into_index\\') ...  @app.route(\\\\\"/uploadFile\\\\\", methods=[\\\\\"POST\\\\\"]) def upload_file():     global manager     if \\'file\\' not in request.files:         return \\\\\"Please send a POST request with a file\\\\\", 400      filepath = None     try:         uploaded_file = request.files[\\\\\"file\\\\\"]         filename = secure_filename(uploaded_file.filename)         filepath = os.path.join(\\'documents\\', os.path.basename(filename))         uploaded_file.save(filepath)          if request.form.get(\\\\\"filename_as_doc_id\\\\\", None) is not None:             manager.insert_into_index(filepath, doc_id=filename)         else:             manager.insert_into_index(filepath)     except Exception as e:         # cleanup temp file         if filepath is not None and os.path.exists(filepath):             os.remove(filepath)         return \\\\\"Error: {}\\\\\".format(str(e)), 500      # cleanup temp file     if filepath is not None and os.path.exists(filepath):         os.remove(filepath)      return \\\\\"File inserted!\\\\\", 200 ```  Not too bad!You will notice that we write the file to disk.We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex\\'s `SimpleDirectoryReader` to take care of a bunch of more complex file formats.Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us.This will make more sense once we implement the frontend.With these more complicated requests, I also suggest using a tool like Postman.Examples of using postman to test our endpoints are in the repository for this project.Lastly, you\\'ll notice we added a new function to the manager.\", \"Let\\'s implement that inside `index_server.py`:  ```python def insert_into_index(doc_text, doc_id=None):     global index     document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]     if doc_id is not None:         document.doc_id = doc_id      with lock:         index.insert(document)         index.storage_context.persist()  ... manager.register(\\'insert_into_index\\', insert_into_index) ... ```  Easy!If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!To support some functionality in the frontend, I\\'ve adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn\\'t currently support this in a user-friendly way, but we can augment it ourselves!).Lastly, I had to add CORS support to the server using the `Flask-cors` python package.Check out the complete `flask_demo.py` and `index_server.py` scripts in the repository for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.\", \"React Frontend  Generally, React and Typescript are one of the most popular libraries and languages for writing webapps today. This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.  In the repository, the frontend code is organized inside of the `react_frontend` folder.  The most relevant part of the frontend will be the `src/apis` folder. This is where we make calls to the Flask server, supporting the following queries:  - `/query` -- make a query to the existing index - `/uploadFile` -- upload a file to the flask server for insertion into the index - `/getDocuments` -- list the current document titles and a portion of their texts  Using these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.\", \"fetchDocuments.tsx  This file contains the function to, you guessed it, fetch the list of current documents in the index. The code is as follows:  ```typescript export type Document = {   id: string;   text: string; };  const fetchDocuments = async (): Promise => {   const response = await fetch(\\\\\"http://localhost:5601/getDocuments\\\\\", {     mode: \\\\\"cors\\\\\",   });    if (!response.ok) {     return [];   }    const documentList = (await response.json()) as Document[];   return documentList; }; ```  As you can see, we make a query to the Flask server (here, it assumes running on localhost). Notice that we need to include the `mode: \\'cors\\'` option, as we are making an external request.  Then, we check if the response was ok, and if so, get the response json and return it. Here, the response json is a list of `Document` objects that are defined in the same file.\", \"queryIndex.tsx  This file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.  ```typescript export type ResponseSources = {   text: string;   doc_id: string;   start: number;   end: number;   similarity: number; };  export type QueryResponse = {   text: string;   sources: ResponseSources[]; };  const queryIndex = async (query: string): Promise => {   const queryURL = new URL(\\\\\"http://localhost:5601/query?text=1\\\\\");   queryURL.searchParams.append(\\\\\"text\\\\\", query);    const response = await fetch(queryURL, { mode: \\\\\"cors\\\\\" });   if (!response.ok) {     return { text: \\\\\"Error in query\\\\\", sources: [] };   }    const queryResponse = (await response.json()) as QueryResponse;    return queryResponse; };  export default queryIndex; ```  This is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL. Then, we check if the response is ok and return it with the appropriate typescript type.\", \"insertDocument.tsx  Probably the most complex API call is uploading a document. The function here accepts a file object and constructs a `POST` request using `FormData`.  The actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.  ```typescript const insertDocument = async (file: File) => {   const formData = new FormData();   formData.append(\\\\\"file\\\\\", file);   formData.append(\\\\\"filename_as_doc_id\\\\\", \\\\\"true\\\\\");    const response = await fetch(\\\\\"http://localhost:5601/uploadFile\\\\\", {     mode: \\\\\"cors\\\\\",     method: \\\\\"POST\\\\\",     body: formData,   });    const responseText = response.text();   return responseText; };  export default insertDocument; ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=131 request_id=86af9340b4f3cf9b66487852a8490d0d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=131 request_id=86af9340b4f3cf9b66487852a8490d0d response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"All the Other Frontend Good-ness  And that pretty much wraps up the frontend portion! The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.  I encourage to read the rest of the codebase and submit any PRs for improvements!\", \"Conclusion  This guide has covered a ton of information. We went from a basic \\\\\"Hello World\\\\\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.  As you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.  You could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.). And when you build an app after reading this, be sure to share the final result in the Discord! Good Luck! :muscle:\", \"A Guide to Building a Full-Stack LlamaIndex Web App with Delphic  This guide seeks to walk you through using LlamaIndex with a production-ready web app starter template called Delphic. All code examples here are available from the Delphic repo\", \"What We\\'re Building  Here\\'s a quick demo of the out-of-the-box functionality of Delphic:  https://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\", \"Architectural Overview  Delphic leverages the LlamaIndex python library to let users to create their own document collections they can then query in a responsive frontend.  We chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python processing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional functionality upon.  The core libraries are:  1. Django 2. Django Channels 3. Django Ninja 4. Redis 5. Celery 6. LlamaIndex 7. Langchain 8. React 9. Docker & Docker Compose  Thanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined developer experience, built-in authentication and user management, asynchronous vector store processing, and web-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based on MUI React for a responsive and modern user interface.\", \"System Requirements  Celery doesn\\'t work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond the scope of this tutorial. For this reason, we recommend you only follow this tutorial if you\\'re running Linux or OSX. You will need Docker and Docker Compose installed to deploy the application. Local development will require node version manager (nvm).\", \"Django Backend\", \"Project Directory Overview  The Delphic application has a structured backend directory organization that follows common Django project conventions. From the repo root, in the `./delphic` subfolder, the main folders are:  1. `contrib`: This directory contains custom modifications or additions to Django\\'s built-in `contrib` apps. 2. `indexes`: This directory contains the core functionality related to document indexing and LLM integration. It    includes:  - `admin.py`: Django admin configuration for the app - `apps.py`: Application configuration - `models.py`: Contains the app\\'s database models - `migrations`: Directory containing database schema migrations for the app - `signals.py`: Defines any signals for the app - `tests.py`: Unit tests for the app  3. `tasks`: This directory contains tasks for asynchronous processing using Celery. The `index_tasks.py` file includes    the tasks for creating vector indexes. 4. `users`: This directory is dedicated to user management, including: 5. `utils`: This directory contains utility modules and functions that are used across the application, such as custom    storage backends, path helpers, and collection-related utilities.\", \"Database Models  The Delphic application has two core models: `Document` and `Collection`. These models represent the central entities the application deals with when indexing and querying documents using LLMs. They\\'re defined in `./delphic/indexes/models.py`.  1. `Collection`:  - `api_key`: A foreign key that links a collection to an API key. This helps associate jobs with the source API key. - `title`: A character field that provides a title for the collection. - `description`: A text field that provides a description of the collection. - `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`   enumeration. - `created`: A datetime field that records when the collection was created. - `modified`: A datetime field that records the last modification time of the collection. - `model`: A file field that stores the model associated with the collection. - `processing`: A boolean field that indicates if the collection is currently being processed.  2. `Document`:  - `collection`: A foreign key that links a document to a collection. This represents the relationship between documents   and collections. - `file`: A file field that stores the uploaded document file. - `description`: A text field that provides a description of the document. - `created`: A datetime field that records when the document was created. - `modified`: A datetime field that records the last modification time of the document.  These models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\", \"Django Ninja API  Django Ninja is a web framework for building APIs with Django and Python 3.7+ type hints.It provides a simple, intuitive, and expressive way of defining API endpoints, leveraging Python\\\\u2019s type hints to automatically generate input validation, serialization, and documentation.In the Delphic repo, the `./config/api/endpoints.py` file contains the API routes and logic for the API endpoints.Now, let\\\\u2019s briefly address the purpose of each endpoint in the `endpoints.py` file:  1.`/heartbeat`: A simple GET endpoint to check if the API is up and running.Returns `True` if the API is accessible.This is helpful for Kubernetes setups that expect to be able to query your container to ensure it\\'s up and running.2.`/collections/create`: A POST endpoint to create a new `Collection`.Accepts form parameters such    as `title`, `description`, and a list of `files`.Creates a new `Collection` and `Document` instances for each file,    and schedules a Celery task to create an index.```python @collections_router.post(\\\\\"/create\\\\\") async def create_collection(request,                             title: str = Form(...),                             description: str = Form(...),                             files: list[UploadedFile] = File(...), ):     key = None if getattr(request, \\\\\"auth\\\\\", None) is None else request.auth     if key is not None:         key = await key      collection_instance = Collection(         api_key=key,         title=title,         description=description,         status=CollectionStatusEnum.QUEUED,     )      await sync_to_async(collection_instance.save)()      for uploaded_file in files:         doc_data = uploaded_file.file.read()         doc_file = ContentFile(doc_data, uploaded_file.name)         document = Document(collection=collection_instance, file=doc_file)         await sync_to_async(document.save)()      create_index.si(collection_instance.id).apply_async()      return await sync_to_async(CollectionModelSchema)(         ...     ) ```  3.`/collections/query` \\\\u2014 a POST endpoint to query a document collection using the LLM.Accepts a JSON payload    containing `collection_id` and `query_str`, and returns a response generated by querying the collection.We don\\'t    actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate    to this REST endpoint to query a specific collection.```python @collections_router.post(\\\\\"/query\\\\\",                          response=CollectionQueryOutput,                          summary=\\\\\"Ask a question of a document collection\\\\\", ) def query_collection_view(request: HttpRequest, query_input: CollectionQueryInput):     collection_id = query_input.collection_id     query_str = query_input.query_str     response = query_collection(collection_id, query_str)     return {\\\\\"response\\\\\": response} ```  4.`/collections/available`: A GET endpoint that returns a list of all collections created with the user\\'s API key.The    output is serialized using the `CollectionModelSchema`.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"All the Other Frontend Good-ness  And that pretty much wraps up the frontend portion! The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.  I encourage to read the rest of the codebase and submit any PRs for improvements!\", \"Conclusion  This guide has covered a ton of information. We went from a basic \\\\\"Hello World\\\\\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.  As you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.  You could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.). And when you build an app after reading this, be sure to share the final result in the Discord! Good Luck! :muscle:\", \"A Guide to Building a Full-Stack LlamaIndex Web App with Delphic  This guide seeks to walk you through using LlamaIndex with a production-ready web app starter template called Delphic. All code examples here are available from the Delphic repo\", \"What We\\'re Building  Here\\'s a quick demo of the out-of-the-box functionality of Delphic:  https://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\", \"Architectural Overview  Delphic leverages the LlamaIndex python library to let users to create their own document collections they can then query in a responsive frontend.  We chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python processing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional functionality upon.  The core libraries are:  1. Django 2. Django Channels 3. Django Ninja 4. Redis 5. Celery 6. LlamaIndex 7. Langchain 8. React 9. Docker & Docker Compose  Thanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined developer experience, built-in authentication and user management, asynchronous vector store processing, and web-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based on MUI React for a responsive and modern user interface.\", \"System Requirements  Celery doesn\\'t work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond the scope of this tutorial. For this reason, we recommend you only follow this tutorial if you\\'re running Linux or OSX. You will need Docker and Docker Compose installed to deploy the application. Local development will require node version manager (nvm).\", \"Django Backend\", \"Project Directory Overview  The Delphic application has a structured backend directory organization that follows common Django project conventions. From the repo root, in the `./delphic` subfolder, the main folders are:  1. `contrib`: This directory contains custom modifications or additions to Django\\'s built-in `contrib` apps. 2. `indexes`: This directory contains the core functionality related to document indexing and LLM integration. It    includes:  - `admin.py`: Django admin configuration for the app - `apps.py`: Application configuration - `models.py`: Contains the app\\'s database models - `migrations`: Directory containing database schema migrations for the app - `signals.py`: Defines any signals for the app - `tests.py`: Unit tests for the app  3. `tasks`: This directory contains tasks for asynchronous processing using Celery. The `index_tasks.py` file includes    the tasks for creating vector indexes. 4. `users`: This directory is dedicated to user management, including: 5. `utils`: This directory contains utility modules and functions that are used across the application, such as custom    storage backends, path helpers, and collection-related utilities.\", \"Database Models  The Delphic application has two core models: `Document` and `Collection`. These models represent the central entities the application deals with when indexing and querying documents using LLMs. They\\'re defined in `./delphic/indexes/models.py`.  1. `Collection`:  - `api_key`: A foreign key that links a collection to an API key. This helps associate jobs with the source API key. - `title`: A character field that provides a title for the collection. - `description`: A text field that provides a description of the collection. - `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`   enumeration. - `created`: A datetime field that records when the collection was created. - `modified`: A datetime field that records the last modification time of the collection. - `model`: A file field that stores the model associated with the collection. - `processing`: A boolean field that indicates if the collection is currently being processed.  2. `Document`:  - `collection`: A foreign key that links a document to a collection. This represents the relationship between documents   and collections. - `file`: A file field that stores the uploaded document file. - `description`: A text field that provides a description of the document. - `created`: A datetime field that records when the document was created. - `modified`: A datetime field that records the last modification time of the document.  These models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\", \"Django Ninja API  Django Ninja is a web framework for building APIs with Django and Python 3.7+ type hints.It provides a simple, intuitive, and expressive way of defining API endpoints, leveraging Python\\\\u2019s type hints to automatically generate input validation, serialization, and documentation.In the Delphic repo, the `./config/api/endpoints.py` file contains the API routes and logic for the API endpoints.Now, let\\\\u2019s briefly address the purpose of each endpoint in the `endpoints.py` file:  1.`/heartbeat`: A simple GET endpoint to check if the API is up and running.Returns `True` if the API is accessible.This is helpful for Kubernetes setups that expect to be able to query your container to ensure it\\'s up and running.2.`/collections/create`: A POST endpoint to create a new `Collection`.Accepts form parameters such    as `title`, `description`, and a list of `files`.Creates a new `Collection` and `Document` instances for each file,    and schedules a Celery task to create an index.```python @collections_router.post(\\\\\"/create\\\\\") async def create_collection(request,                             title: str = Form(...),                             description: str = Form(...),                             files: list[UploadedFile] = File(...), ):     key = None if getattr(request, \\\\\"auth\\\\\", None) is None else request.auth     if key is not None:         key = await key      collection_instance = Collection(         api_key=key,         title=title,         description=description,         status=CollectionStatusEnum.QUEUED,     )      await sync_to_async(collection_instance.save)()      for uploaded_file in files:         doc_data = uploaded_file.file.read()         doc_file = ContentFile(doc_data, uploaded_file.name)         document = Document(collection=collection_instance, file=doc_file)         await sync_to_async(document.save)()      create_index.si(collection_instance.id).apply_async()      return await sync_to_async(CollectionModelSchema)(         ...     ) ```  3.`/collections/query` \\\\u2014 a POST endpoint to query a document collection using the LLM.Accepts a JSON payload    containing `collection_id` and `query_str`, and returns a response generated by querying the collection.We don\\'t    actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate    to this REST endpoint to query a specific collection.```python @collections_router.post(\\\\\"/query\\\\\",                          response=CollectionQueryOutput,                          summary=\\\\\"Ask a question of a document collection\\\\\", ) def query_collection_view(request: HttpRequest, query_input: CollectionQueryInput):     collection_id = query_input.collection_id     query_str = query_input.query_str     response = query_collection(collection_id, query_str)     return {\\\\\"response\\\\\": response} ```  4.`/collections/available`: A GET endpoint that returns a list of all collections created with the user\\'s API key.The    output is serialized using the `CollectionModelSchema`.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=51 request_id=6680a2a7ef690eeb541bfaba8c106b60 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=51 request_id=6680a2a7ef690eeb541bfaba8c106b60 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"```python @collections_router.get(\\\\\"/available\\\\\",                         response=list[CollectionModelSchema],                         summary=\\\\\"Get a list of all of the collections created with my api_key\\\\\", ) async def get_my_collections_view(request: HttpRequest):     key = None if getattr(request, \\\\\"auth\\\\\", None) is None else request.auth     if key is not None:         key = await key      collections = Collection.objects.filter(api_key=key)      return [         {             ...         }         async for collection in collections     ] ```  5.`/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection.Accepts    a `collection_id` path parameter, and form parameters such as `file` and `description`.Adds the file as a `Document`    instance associated with the specified collection.```python @collections_router.post(\\\\\"/{collection_id}/add_file\\\\\", summary=\\\\\"Add a file to a collection\\\\\") async def add_file_to_collection(request,                                  collection_id: int,                                  file: UploadedFile = File(...),                                  description: str = Form(...), ):     collection = await sync_to_async(Collection.objects.get)(id=collection_id ```\", \"Intro to Websockets  WebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a server over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and HTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the connection is established, data can be sent in both directions as \\\\u201cframes\\\\u201d without the need to reestablish the connection each time, unlike traditional HTTP requests.  There are several reasons to use WebSockets, particularly when working with code that takes a long time to load into memory but is quick to run once loaded:  1. **Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each    request, reducing latency. 2. **Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more    efficient use of resources and better responsiveness. 3. **Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications    that require high concurrency.  In the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory. By establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be processed quickly without the need to reload the model each time.  The ASGI configuration file `./config/asgi.py` defines how the application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections based on their protocol type. In this case, we have two protocol types: \\\\\"http\\\\\" and \\\\\"websocket\\\\\".  The \\\\u201chttp\\\\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \\\\u201cwebsocket\\\\u201d protocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections. The `URLRouter` within the `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling WebSocket connections related to querying document collections.  ```python application = ProtocolTypeRouter(     {         \\\\\"http\\\\\": get_asgi_application(),         \\\\\"websocket\\\\\": TokenAuthMiddleware(             URLRouter(                 [                     re_path(                         r\\\\\"ws/collections/(?P\\\\\\\\w+)/query/$\\\\\",                         CollectionQueryConsumer.as_asgi(),                     ),                 ]             )         ),     } ) ```  This configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query document collections using the LLMs, without the need to reload the models for each request.\", \"Websocket Handler  The `CollectionQueryConsumer` class in `config/api/websockets/queries.py` is responsible for handling WebSocket connections related to querying document collections. It inherits from the `AsyncWebsocketConsumer` class provided by Django Channels.  The `CollectionQueryConsumer` class has three main methods:  1. `connect`: Called when a WebSocket is handshaking as part of the connection process. 2. `disconnect`: Called when a WebSocket closes for any reason. 3. `receive`: Called when the server receives a message from the WebSocket.\", \"Websocket connect listener  The `connect` method is responsible for establishing the connection, extracting the collection ID from the connection path, loading the collection model, and accepting the connection.  ```python async def connect(self):     try:         self.collection_id = extract_connection_id(self.scope[\\\\\"path\\\\\"])         self.index = await load_collection_model(self.collection_id)         await self.accept()  except ValueError as e: await self.accept() await self.close(code=4000) except Exception as e: pass ```\", \"Websocket disconnect listener  The `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is closed.\", \"Websocket receive listener  The `receive` method is responsible for processing incoming messages from the WebSocket.It takes the incoming message, decodes it, and then queries the loaded collection model using the provided query.The response is then formatted as a markdown string and sent back to the client over the WebSocket connection.```python async def receive(self, text_data):     text_data_json = json.loads(text_data)      if self.index is not None:         query_str = text_data_json[\\\\\"query\\\\\"]         modified_query_str = f\\\\\"Please return a nicely formatted markdown string to this request:\\\\\\\\n\\\\\\\\n{query_str}\\\\\"         query_engine = self.index.as_query_engine()         response = query_engine.query(modified_query_str)          markdown_response = f\\\\\"## Response\\\\\\\\n\\\\\\\\n{response}\\\\\\\\n\\\\\\\\n\\\\\"         if response.source_nodes:             markdown_sources = f\\\\\"## Sources\\\\\\\\n\\\\\\\\n{response.get_formatted_sources()}\\\\\"         else:             markdown_sources = \\\\\"\\\\\"          formatted_response = f\\\\\"{markdown_response}{markdown_sources}\\\\\"          await self.send(json.dumps({\\\\\"response\\\\\": formatted_response}, indent=4))     else:         await self.send(json.dumps({\\\\\"error\\\\\": \\\\\"No index loaded for this connection.\\\\\"}, indent=4)) ```  To load the collection model, the `load_collection_model` function is used, which can be found in `delphic/utils/collections.py`.This function retrieves the collection object with the given collection ID, checks if a JSON file for the collection model exists, and if not, creates one.Then, it sets up the `LLMPredictor` and `ServiceContext` before loading the `VectorStoreIndex` using the cache file.```python async def load_collection_model(collection_id: str | int) -> VectorStoreIndex:     \\\\\"\\\\\"\\\\\"     Load the Collection model from cache or the database, and return the index.Args:         collection_id (Union[str, int]): The ID of the Collection model instance.Returns:         VectorStoreIndex: The loaded index.This function performs the following steps:     1.Retrieve the Collection object with the given collection_id.2.Check if a JSON file with the name \\'/cache/model_{collection_id}.json\\' exists.3.If the JSON file doesn\\'t exist, load the JSON from the Collection.model FileField and save it to        \\'/cache/model_{collection_id}.json\\'.4.Call VectorStoreIndex.load_from_disk with the cache_file_path.     \\\\\"\\\\\"\\\\\"\", \"# Retrieve the Collection object     collection = await Collection.objects.aget(id=collection_id)     logger.info(f\\\\\"load_collection_model() - loaded collection {collection_id}\\\\\")      # Make sure there\\'s a model     if collection.model.name:         logger.info(\\\\\"load_collection_model() - Setup local json index file\\\\\")          # Check if the JSON file exists         cache_dir = Path(settings.BASE_DIR) / \\\\\"cache\\\\\"         cache_file_path = cache_dir / f\\\\\"model_{collection_id}.json\\\\\"         if not cache_file_path.exists():             cache_dir.mkdir(parents=True, exist_ok=True)             with collection.model.open(\\\\\"rb\\\\\") as model_file:                 with cache_file_path.open(\\\\\"w+\\\\\", encoding=\\\\\"utf-8\\\\\") as cache_file:                     cache_file.write(model_file.read().decode(\\\\\"utf-8\\\\\"))          # define LLM         logger.info(             f\\\\\"load_collection_model() - Setup service context with tokens {settings.MAX_TOKENS} and \\\\\"             f\\\\\"model {settings.MODEL_NAME}\\\\\"         )         llm = OpenAI(temperature=0, model=\\\\\"text-davinci-003\\\\\", max_tokens=512)         service_context = ServiceContext.from_defaults(llm=llm)          # Call VectorStoreIndex.load_from_disk         logger.info(\\\\\"load_collection_model() - Load llama index\\\\\")         index = VectorStoreIndex.load_from_disk(             cache_file_path, service_context=service_context         )         logger.info(             \\\\\"load_collection_model() - Llamaindex loaded and ready for query...\\\\\"         )      else:         logger.error(             f\\\\\"load_collection_model() - collection {collection_id} has no model!\\\\\")         raise ValueError(\\\\\"No model exists for this collection!\\\\\")return index ```\", \"React Frontend\", \"Overview  We chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\\\\u2019s frontend for a couple reasons. First, as the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this project accessible to a huge community of developers. Second, React is, at this point, a stable and generally well-liked framework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in our opinion, pretty easy to learn, again making it accessible.\", \"Frontend Project Structure  The frontend can be found in the `/frontend` directory of the repo, with the React-related components being in `/frontend/src` . You\\\\u2019ll notice there is a DockerFile in the `frontend` directory and several folders and files related to configuring our frontend web server \\\\u2014 nginx.  The `/frontend/src/App.tsx` file serves as the entry point of the application. It defines the main components, such as the login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based on whether the user is logged in and has an authentication token.  The DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file. This component manages the layout of the application and provides the navigation and main content areas.  Since the application is relatively simple, we can get away with not using a complex state management solution like Redux and just use React\\\\u2019s useState hooks.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"```python @collections_router.get(\\\\\"/available\\\\\",                         response=list[CollectionModelSchema],                         summary=\\\\\"Get a list of all of the collections created with my api_key\\\\\", ) async def get_my_collections_view(request: HttpRequest):     key = None if getattr(request, \\\\\"auth\\\\\", None) is None else request.auth     if key is not None:         key = await key      collections = Collection.objects.filter(api_key=key)      return [         {             ...         }         async for collection in collections     ] ```  5.`/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection.Accepts    a `collection_id` path parameter, and form parameters such as `file` and `description`.Adds the file as a `Document`    instance associated with the specified collection.```python @collections_router.post(\\\\\"/{collection_id}/add_file\\\\\", summary=\\\\\"Add a file to a collection\\\\\") async def add_file_to_collection(request,                                  collection_id: int,                                  file: UploadedFile = File(...),                                  description: str = Form(...), ):     collection = await sync_to_async(Collection.objects.get)(id=collection_id ```\", \"Intro to Websockets  WebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a server over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and HTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the connection is established, data can be sent in both directions as \\\\u201cframes\\\\u201d without the need to reestablish the connection each time, unlike traditional HTTP requests.  There are several reasons to use WebSockets, particularly when working with code that takes a long time to load into memory but is quick to run once loaded:  1. **Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each    request, reducing latency. 2. **Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more    efficient use of resources and better responsiveness. 3. **Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications    that require high concurrency.  In the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory. By establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be processed quickly without the need to reload the model each time.  The ASGI configuration file `./config/asgi.py` defines how the application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections based on their protocol type. In this case, we have two protocol types: \\\\\"http\\\\\" and \\\\\"websocket\\\\\".  The \\\\u201chttp\\\\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \\\\u201cwebsocket\\\\u201d protocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections. The `URLRouter` within the `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling WebSocket connections related to querying document collections.  ```python application = ProtocolTypeRouter(     {         \\\\\"http\\\\\": get_asgi_application(),         \\\\\"websocket\\\\\": TokenAuthMiddleware(             URLRouter(                 [                     re_path(                         r\\\\\"ws/collections/(?P\\\\\\\\w+)/query/$\\\\\",                         CollectionQueryConsumer.as_asgi(),                     ),                 ]             )         ),     } ) ```  This configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query document collections using the LLMs, without the need to reload the models for each request.\", \"Websocket Handler  The `CollectionQueryConsumer` class in `config/api/websockets/queries.py` is responsible for handling WebSocket connections related to querying document collections. It inherits from the `AsyncWebsocketConsumer` class provided by Django Channels.  The `CollectionQueryConsumer` class has three main methods:  1. `connect`: Called when a WebSocket is handshaking as part of the connection process. 2. `disconnect`: Called when a WebSocket closes for any reason. 3. `receive`: Called when the server receives a message from the WebSocket.\", \"Websocket connect listener  The `connect` method is responsible for establishing the connection, extracting the collection ID from the connection path, loading the collection model, and accepting the connection.  ```python async def connect(self):     try:         self.collection_id = extract_connection_id(self.scope[\\\\\"path\\\\\"])         self.index = await load_collection_model(self.collection_id)         await self.accept()  except ValueError as e: await self.accept() await self.close(code=4000) except Exception as e: pass ```\", \"Websocket disconnect listener  The `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is closed.\", \"Websocket receive listener  The `receive` method is responsible for processing incoming messages from the WebSocket.It takes the incoming message, decodes it, and then queries the loaded collection model using the provided query.The response is then formatted as a markdown string and sent back to the client over the WebSocket connection.```python async def receive(self, text_data):     text_data_json = json.loads(text_data)      if self.index is not None:         query_str = text_data_json[\\\\\"query\\\\\"]         modified_query_str = f\\\\\"Please return a nicely formatted markdown string to this request:\\\\\\\\n\\\\\\\\n{query_str}\\\\\"         query_engine = self.index.as_query_engine()         response = query_engine.query(modified_query_str)          markdown_response = f\\\\\"## Response\\\\\\\\n\\\\\\\\n{response}\\\\\\\\n\\\\\\\\n\\\\\"         if response.source_nodes:             markdown_sources = f\\\\\"## Sources\\\\\\\\n\\\\\\\\n{response.get_formatted_sources()}\\\\\"         else:             markdown_sources = \\\\\"\\\\\"          formatted_response = f\\\\\"{markdown_response}{markdown_sources}\\\\\"          await self.send(json.dumps({\\\\\"response\\\\\": formatted_response}, indent=4))     else:         await self.send(json.dumps({\\\\\"error\\\\\": \\\\\"No index loaded for this connection.\\\\\"}, indent=4)) ```  To load the collection model, the `load_collection_model` function is used, which can be found in `delphic/utils/collections.py`.This function retrieves the collection object with the given collection ID, checks if a JSON file for the collection model exists, and if not, creates one.Then, it sets up the `LLMPredictor` and `ServiceContext` before loading the `VectorStoreIndex` using the cache file.```python async def load_collection_model(collection_id: str | int) -> VectorStoreIndex:     \\\\\"\\\\\"\\\\\"     Load the Collection model from cache or the database, and return the index.Args:         collection_id (Union[str, int]): The ID of the Collection model instance.Returns:         VectorStoreIndex: The loaded index.This function performs the following steps:     1.Retrieve the Collection object with the given collection_id.2.Check if a JSON file with the name \\'/cache/model_{collection_id}.json\\' exists.3.If the JSON file doesn\\'t exist, load the JSON from the Collection.model FileField and save it to        \\'/cache/model_{collection_id}.json\\'.4.Call VectorStoreIndex.load_from_disk with the cache_file_path.     \\\\\"\\\\\"\\\\\"\", \"# Retrieve the Collection object     collection = await Collection.objects.aget(id=collection_id)     logger.info(f\\\\\"load_collection_model() - loaded collection {collection_id}\\\\\")      # Make sure there\\'s a model     if collection.model.name:         logger.info(\\\\\"load_collection_model() - Setup local json index file\\\\\")          # Check if the JSON file exists         cache_dir = Path(settings.BASE_DIR) / \\\\\"cache\\\\\"         cache_file_path = cache_dir / f\\\\\"model_{collection_id}.json\\\\\"         if not cache_file_path.exists():             cache_dir.mkdir(parents=True, exist_ok=True)             with collection.model.open(\\\\\"rb\\\\\") as model_file:                 with cache_file_path.open(\\\\\"w+\\\\\", encoding=\\\\\"utf-8\\\\\") as cache_file:                     cache_file.write(model_file.read().decode(\\\\\"utf-8\\\\\"))          # define LLM         logger.info(             f\\\\\"load_collection_model() - Setup service context with tokens {settings.MAX_TOKENS} and \\\\\"             f\\\\\"model {settings.MODEL_NAME}\\\\\"         )         llm = OpenAI(temperature=0, model=\\\\\"text-davinci-003\\\\\", max_tokens=512)         service_context = ServiceContext.from_defaults(llm=llm)          # Call VectorStoreIndex.load_from_disk         logger.info(\\\\\"load_collection_model() - Load llama index\\\\\")         index = VectorStoreIndex.load_from_disk(             cache_file_path, service_context=service_context         )         logger.info(             \\\\\"load_collection_model() - Llamaindex loaded and ready for query...\\\\\"         )      else:         logger.error(             f\\\\\"load_collection_model() - collection {collection_id} has no model!\\\\\")         raise ValueError(\\\\\"No model exists for this collection!\\\\\")return index ```\", \"React Frontend\", \"Overview  We chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\\\\u2019s frontend for a couple reasons. First, as the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this project accessible to a huge community of developers. Second, React is, at this point, a stable and generally well-liked framework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in our opinion, pretty easy to learn, again making it accessible.\", \"Frontend Project Structure  The frontend can be found in the `/frontend` directory of the repo, with the React-related components being in `/frontend/src` . You\\\\u2019ll notice there is a DockerFile in the `frontend` directory and several folders and files related to configuring our frontend web server \\\\u2014 nginx.  The `/frontend/src/App.tsx` file serves as the entry point of the application. It defines the main components, such as the login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based on whether the user is logged in and has an authentication token.  The DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file. This component manages the layout of the application and provides the navigation and main content areas.  Since the application is relatively simple, we can get away with not using a complex state management solution like Redux and just use React\\\\u2019s useState hooks.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=f2457ac28e403ce4e5bbc7add612e753 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=50 request_id=f2457ac28e403ce4e5bbc7add612e753 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Grabbing Collections from the Backend  The collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process can be broken down into the following steps:  1. Initializing state variables:  ```tsx const[collections, setCollections] = useState  ([]); const[loading, setLoading] = useState(true); ```  Here, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether the collections are being fetched.  2. Collections are fetched for the logged-in user with the `fetchCollections()` function:  ```tsx const fetchCollections = async () = > { try { const accessToken = localStorage.getItem(\\\\\"accessToken\\\\\"); if (accessToken) { const response = await getMyCollections(accessToken); setCollections(response.data); } } catch (error) { console.error(error); } finally { setLoading(false); } }; ```  The `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API function with the user\\'s access token. It then updates the `collections` state with the retrieved data and sets the `loading` state to `false` to indicate that fetching is complete.\", \"Displaying Collections  The latest collectios are displayed in the drawer like this:  ```tsx  {collections.map((collection) = > (               < ListItemButton     disabled={     collection.status != = CollectionStatus.COMPLETE | |     !collection.has_model     }     onClick={() = > handleCollectionClick(collection)} selected = {     selectedCollection & &     selectedCollection.id == = collection.id } > < ListItemText primary = {collection.title} / >           {collection.status == = CollectionStatus.RUNNING ? (     < CircularProgress     size={24}     style={{position: \\\\\"absolute\\\\\", right: 16}}     / > ): null}                ))}  ```  You\\\\u2019ll notice that the `disabled` property of a collection\\\\u2019s `ListItemButton` is set based on whether the collection\\'s status is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`). If either of these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less collection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.  In a separate `useEffect` hook, we check if any collection in the `collections` state has a status of `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`. If so, we set up an interval to repeatedly call the `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the application periodically checks for completed collections, and the UI is updated accordingly when the processing is done.  ```tsx useEffect(() = > {     let interval: NodeJS.Timeout; if (     collections.some(         (collection) = > collection.status == = CollectionStatus.RUNNING | | collection.status == = CollectionStatus.QUEUED ) ) {     interval = setInterval(() = > {     fetchCollections(); }, 15000); } return () = > clearInterval(interval); }, [collections]); ```\", \"Chat View Component  The `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface for a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time with the server, sending and receiving messages.  Key features of the `ChatView` component include:  1. Establishing and managing the WebSocket connection with the server. 2. Displaying messages from the user and the server in a chat-like format. 3. Handling user input to send messages to the server. 4. Updating the messages state and UI based on received messages from the server. 5. Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors    while loading a collection.  Together, all of this allows users to interact with their selected collection with a very smooth, low-latency experience.\", \"Chat Websocket Client  The WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and the server.The WebSocket connection is set up and managed in the `ChatView` component as follows:  First, we want to initialize the the WebSocket reference:  const websocket = useRef(null);  A `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for communication.`useRef` is a hook in React that allows you to create a mutable reference object that persists across renders.It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket connection, without causing unnecessary re-renders.In the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of the component, and it should not trigger a re-render when the connection state changes.By using `useRef`, you ensure that the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state changes, such as updating messages or displaying errors.The `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to handle different WebSocket events.Overall, the setupWebsocket function looks like this:  ```tsx const setupWebsocket = () => {     setConnecting(true);     // Here, a new WebSocket object is created using the specified URL, which includes the      // selected collection\\'s ID and the user\\'s authentication token.websocket.current = new WebSocket(       `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`     );        websocket.current.onopen = (event) => {       //...     };        websocket.current.onmessage = (event) => {       //...     };        websocket.current.onclose = (event) => {       //...     };        websocket.current.onerror = (event) => {       //...     };        return () => {       websocket.current?.close();     };   }; ```  Notice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.When the component first opens and we try to establish a connection, the `onopen` listener is triggered.In the callback, the component updates the states to reflect that the connection is established, any previous errors are cleared, and no messages are awaiting responses:  ```tsx websocket.current.onopen = (event) => {     setError(false);     setConnecting(false);     setAwaitingMessage(false);        console.log(\\\\\"WebSocket connected:\\\\\", event);   }; ```  `onmessage`is triggered when a new message is received from the server through the WebSocket connection.In the callback, the received data is parsed and the `messages` state is updated with the new message from the server:  ``` websocket.current.onmessage = (event) => {     const data = JSON.parse(event.data);     console.log(\\\\\"WebSocket message received:\\\\\", data);     setAwaitingMessage(false);        if (data.response) {       // Update the messages state with the new message from the server       setMessages((prevMessages) => [         ...prevMessages,         {           sender_id: \\\\\"server\\\\\",           message: data.response,           timestamp: new Date().toLocaleTimeString(),         },       ]);     }   }; ```  `onclose`is triggered when the WebSocket connection is closed.\", \"In the callback, the component checks for a specific close code (`4000`) to display a warning toast and update the component states accordingly.It also logs the close event:  ```tsx websocket.current.onclose = (event) => {     if (event.code === 4000) {       toast.warning(         \\\\\"Selected collection\\'s model is unavailable.Was it created properly?\\\\\");       setError(true);       setConnecting(false);       setAwaitingMessage(false);     }     console.log(\\\\\"WebSocket closed:\\\\\", event);   }; ```  Finally, `onerror` is triggered when an error occurs with the WebSocket connection.In the callback, the component updates the states to reflect the error and logs the error event:  ```tsx     websocket.current.onerror = (event) => {       setError(true);       setConnecting(false);       setAwaitingMessage(false);        console.error(\\\\\"WebSocket error:\\\\\", event);     };   ```\", \"Rendering our Chat Messages  In the `ChatView` component, the layout is determined using CSS styling and Material-UI components. The main layout consists of a container with a `flex` display and a column-oriented `flexDirection`. This ensures that the content within the container is arranged vertically.  There are three primary sections within the layout:  1. The chat messages area: This section takes up most of the available space and displays a list of messages exchanged    between the user and the server. It has an overflow-y set to \\\\u2018auto\\\\u2019, which allows scrolling when the content    overflows the available space. The messages are rendered using the `ChatMessage` component for each message and    a `ChatMessageLoading` component to show the loading state while waiting for a server response. 2. The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,    creating a clear visual distinction between the two sections. 3. The input area: This section is located at the bottom and allows the user to type and send messages. It contains    a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input    area also includes a `Button` component to send the message. The user can either click the \\\\\"Send\\\\\" button or press \\\\\"    Enter\\\\\" on their keyboard to send the message.  The user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`. The component processes these text inputs and sends them to the server through the WebSocket connection.\", \"Deployment\", \"Prerequisites  To deploy the app, you\\'re going to need Docker and Docker Compose installed. If you\\'re on Ubuntu or another, common Linux distribution, DigitalOcean has a great Docker tutorial and another great tutorial for Docker Compose you can follow. If those don\\'t work for you, try the official docker documentation.\", \"Build and Deploy  The project is based on django-cookiecutter, and it\\\\u2019s pretty easy to get it deployed on a VM and configured to serve HTTPs traffic for a specific domain. The configuration is somewhat involved, however \\\\u2014 not because of this project, but it\\\\u2019s just a fairly involved topic to configure your certificates, DNS, etc.  For the purposes of this guide, let\\\\u2019s just get running locally. Perhaps we\\\\u2019ll release a guide on production deployment. In the meantime, check out the Django Cookiecutter project docs for starters.  This guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you won\\\\u2019t want to launch the compose stack with the \\\\u2014 profiles fullstack flag and will instead want to launch the react frontend using the node development server.  To deploy, first clone the repo:  ```commandline git clone https://github.com/yourusername/delphic.git ```  Change into the project directory:  ```commandline cd delphic ```  Copy the sample environment files:  ```commandline mkdir -p ./.envs/.local/   cp -a ./docs/sample_envs/local/.frontend ./frontend   cp -a ./docs/sample_envs/local/.django ./.envs/.local   cp -a ./docs/sample_envs/local/.postgres ./.envs/.local ```  Edit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your database user. You can also set the response token limit in the .django file or switch which OpenAI model you want to use. GPT4 is supported, assuming you\\\\u2019re authorized to access it.  Build the docker compose stack with the `--profiles fullstack` flag:  ```commandline sudo docker-compose --profiles fullstack -f local.yml build ```  The fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched along with all of the needed, backend containers. It takes a long time to build a production React container, however, so we don\\\\u2019t recommend you develop this way. Follow the instructions in the project readme.md for development environment setup instructions.  Finally, bring up the application:  ```commandline sudo docker-compose -f local.yml up ```  Now, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.\", \"Using the Application\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Grabbing Collections from the Backend  The collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process can be broken down into the following steps:  1. Initializing state variables:  ```tsx const[collections, setCollections] = useState  ([]); const[loading, setLoading] = useState(true); ```  Here, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether the collections are being fetched.  2. Collections are fetched for the logged-in user with the `fetchCollections()` function:  ```tsx const fetchCollections = async () = > { try { const accessToken = localStorage.getItem(\\\\\"accessToken\\\\\"); if (accessToken) { const response = await getMyCollections(accessToken); setCollections(response.data); } } catch (error) { console.error(error); } finally { setLoading(false); } }; ```  The `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API function with the user\\'s access token. It then updates the `collections` state with the retrieved data and sets the `loading` state to `false` to indicate that fetching is complete.\", \"Displaying Collections  The latest collectios are displayed in the drawer like this:  ```tsx  {collections.map((collection) = > (               < ListItemButton     disabled={     collection.status != = CollectionStatus.COMPLETE | |     !collection.has_model     }     onClick={() = > handleCollectionClick(collection)} selected = {     selectedCollection & &     selectedCollection.id == = collection.id } > < ListItemText primary = {collection.title} / >           {collection.status == = CollectionStatus.RUNNING ? (     < CircularProgress     size={24}     style={{position: \\\\\"absolute\\\\\", right: 16}}     / > ): null}                ))}  ```  You\\\\u2019ll notice that the `disabled` property of a collection\\\\u2019s `ListItemButton` is set based on whether the collection\\'s status is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`). If either of these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less collection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.  In a separate `useEffect` hook, we check if any collection in the `collections` state has a status of `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`. If so, we set up an interval to repeatedly call the `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the application periodically checks for completed collections, and the UI is updated accordingly when the processing is done.  ```tsx useEffect(() = > {     let interval: NodeJS.Timeout; if (     collections.some(         (collection) = > collection.status == = CollectionStatus.RUNNING | | collection.status == = CollectionStatus.QUEUED ) ) {     interval = setInterval(() = > {     fetchCollections(); }, 15000); } return () = > clearInterval(interval); }, [collections]); ```\", \"Chat View Component  The `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface for a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time with the server, sending and receiving messages.  Key features of the `ChatView` component include:  1. Establishing and managing the WebSocket connection with the server. 2. Displaying messages from the user and the server in a chat-like format. 3. Handling user input to send messages to the server. 4. Updating the messages state and UI based on received messages from the server. 5. Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors    while loading a collection.  Together, all of this allows users to interact with their selected collection with a very smooth, low-latency experience.\", \"Chat Websocket Client  The WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and the server.The WebSocket connection is set up and managed in the `ChatView` component as follows:  First, we want to initialize the the WebSocket reference:  const websocket = useRef(null);  A `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for communication.`useRef` is a hook in React that allows you to create a mutable reference object that persists across renders.It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket connection, without causing unnecessary re-renders.In the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of the component, and it should not trigger a re-render when the connection state changes.By using `useRef`, you ensure that the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state changes, such as updating messages or displaying errors.The `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to handle different WebSocket events.Overall, the setupWebsocket function looks like this:  ```tsx const setupWebsocket = () => {     setConnecting(true);     // Here, a new WebSocket object is created using the specified URL, which includes the      // selected collection\\'s ID and the user\\'s authentication token.websocket.current = new WebSocket(       `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`     );        websocket.current.onopen = (event) => {       //...     };        websocket.current.onmessage = (event) => {       //...     };        websocket.current.onclose = (event) => {       //...     };        websocket.current.onerror = (event) => {       //...     };        return () => {       websocket.current?.close();     };   }; ```  Notice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.When the component first opens and we try to establish a connection, the `onopen` listener is triggered.In the callback, the component updates the states to reflect that the connection is established, any previous errors are cleared, and no messages are awaiting responses:  ```tsx websocket.current.onopen = (event) => {     setError(false);     setConnecting(false);     setAwaitingMessage(false);        console.log(\\\\\"WebSocket connected:\\\\\", event);   }; ```  `onmessage`is triggered when a new message is received from the server through the WebSocket connection.In the callback, the received data is parsed and the `messages` state is updated with the new message from the server:  ``` websocket.current.onmessage = (event) => {     const data = JSON.parse(event.data);     console.log(\\\\\"WebSocket message received:\\\\\", data);     setAwaitingMessage(false);        if (data.response) {       // Update the messages state with the new message from the server       setMessages((prevMessages) => [         ...prevMessages,         {           sender_id: \\\\\"server\\\\\",           message: data.response,           timestamp: new Date().toLocaleTimeString(),         },       ]);     }   }; ```  `onclose`is triggered when the WebSocket connection is closed.\", \"In the callback, the component checks for a specific close code (`4000`) to display a warning toast and update the component states accordingly.It also logs the close event:  ```tsx websocket.current.onclose = (event) => {     if (event.code === 4000) {       toast.warning(         \\\\\"Selected collection\\'s model is unavailable.Was it created properly?\\\\\");       setError(true);       setConnecting(false);       setAwaitingMessage(false);     }     console.log(\\\\\"WebSocket closed:\\\\\", event);   }; ```  Finally, `onerror` is triggered when an error occurs with the WebSocket connection.In the callback, the component updates the states to reflect the error and logs the error event:  ```tsx     websocket.current.onerror = (event) => {       setError(true);       setConnecting(false);       setAwaitingMessage(false);        console.error(\\\\\"WebSocket error:\\\\\", event);     };   ```\", \"Rendering our Chat Messages  In the `ChatView` component, the layout is determined using CSS styling and Material-UI components. The main layout consists of a container with a `flex` display and a column-oriented `flexDirection`. This ensures that the content within the container is arranged vertically.  There are three primary sections within the layout:  1. The chat messages area: This section takes up most of the available space and displays a list of messages exchanged    between the user and the server. It has an overflow-y set to \\\\u2018auto\\\\u2019, which allows scrolling when the content    overflows the available space. The messages are rendered using the `ChatMessage` component for each message and    a `ChatMessageLoading` component to show the loading state while waiting for a server response. 2. The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,    creating a clear visual distinction between the two sections. 3. The input area: This section is located at the bottom and allows the user to type and send messages. It contains    a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input    area also includes a `Button` component to send the message. The user can either click the \\\\\"Send\\\\\" button or press \\\\\"    Enter\\\\\" on their keyboard to send the message.  The user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`. The component processes these text inputs and sends them to the server through the WebSocket connection.\", \"Deployment\", \"Prerequisites  To deploy the app, you\\'re going to need Docker and Docker Compose installed. If you\\'re on Ubuntu or another, common Linux distribution, DigitalOcean has a great Docker tutorial and another great tutorial for Docker Compose you can follow. If those don\\'t work for you, try the official docker documentation.\", \"Build and Deploy  The project is based on django-cookiecutter, and it\\\\u2019s pretty easy to get it deployed on a VM and configured to serve HTTPs traffic for a specific domain. The configuration is somewhat involved, however \\\\u2014 not because of this project, but it\\\\u2019s just a fairly involved topic to configure your certificates, DNS, etc.  For the purposes of this guide, let\\\\u2019s just get running locally. Perhaps we\\\\u2019ll release a guide on production deployment. In the meantime, check out the Django Cookiecutter project docs for starters.  This guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you won\\\\u2019t want to launch the compose stack with the \\\\u2014 profiles fullstack flag and will instead want to launch the react frontend using the node development server.  To deploy, first clone the repo:  ```commandline git clone https://github.com/yourusername/delphic.git ```  Change into the project directory:  ```commandline cd delphic ```  Copy the sample environment files:  ```commandline mkdir -p ./.envs/.local/   cp -a ./docs/sample_envs/local/.frontend ./frontend   cp -a ./docs/sample_envs/local/.django ./.envs/.local   cp -a ./docs/sample_envs/local/.postgres ./.envs/.local ```  Edit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your database user. You can also set the response token limit in the .django file or switch which OpenAI model you want to use. GPT4 is supported, assuming you\\\\u2019re authorized to access it.  Build the docker compose stack with the `--profiles fullstack` flag:  ```commandline sudo docker-compose --profiles fullstack -f local.yml build ```  The fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched along with all of the needed, backend containers. It takes a long time to build a production React container, however, so we don\\\\u2019t recommend you develop this way. Follow the instructions in the project readme.md for development environment setup instructions.  Finally, bring up the application:  ```commandline sudo docker-compose -f local.yml up ```  Now, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.\", \"Using the Application\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=61 request_id=7acc143dea2d6ce6a4c76cc00d3e3013 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=61 request_id=7acc143dea2d6ce6a4c76cc00d3e3013 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Setup Users  In order to actually use the application (at the moment, we intend to make it possible to share certain models with unauthenticated users), you need a login. You can use either a superuser or non-superuser. In either case, someone needs to first create a superuser using the console:  **Why set up a Django superuser?** A Django superuser has all the permissions in the application and can manage all aspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a superuser allows you to fully control and manage the application.  **How to create a Django superuser:**  1 Run the following command to create a superuser:  sudo docker-compose -f local.yml run django python manage.py createsuperuser  2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required information.  **How to create additional users using Django admin:**  1. Start your Delphic application locally following the deployment instructions. 2. Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser. 3. Log in with the superuser credentials you created earlier. 4. Click on \\\\u201cUsers\\\\u201d under the \\\\u201cAuthentication and Authorization\\\\u201d section. 5. Click on the \\\\u201cAdd user +\\\\u201d button in the top right corner. 6. Enter the required information for the new user, such as username and password. Click \\\\u201cSave\\\\u201d to create the user. 7. To grant the new user additional permissions or make them a superuser, click on their username in the user list,    scroll down to the \\\\u201cPermissions\\\\u201d section, and configure their permissions accordingly. Save your changes.\", \"Full-Stack Web Application  LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.  We provide tutorials and resources to help you get started in this area.  Relevant Resources: - Fullstack Application Guide - Fullstack Application with Delphic - A Guide to Extracting Terms and Definitions - LlamaIndex Starter Pack\", \"\\\\ud83d\\\\udcac\\\\ud83e\\\\udd16 How to Build a Chatbot  LlamaIndex is an interface between your data and LLM\\'s; it offers the toolkit for you to setup a query interface around your data for any downstream task, whether it\\'s question-answering, summarization, or more.  In this tutorial, we show you how to build a context augmented chatbot. We use Langchain for the underlying Agent/Chatbot abstractions, and we use LlamaIndex for the data retrieval/lookup/querying! The result is a chatbot agent that has access to a rich set of \\\\\"data interface\\\\\" Tools that LlamaIndex provides to answer queries over your data.  **Note**: This is a continuation of some initial work building a query interface over SEC 10-K filings - check it out here.\", \"Context  In this tutorial, we build an \\\\\"10-K Chatbot\\\\\" by downloading the raw UBER 10-K HTML filings from Dropbox. The user can choose to ask questions regarding the 10-K filings.\", \"Ingest Data  Let\\'s first download the raw 10-k files, from 2019-2022.  ```python\", \"NOTE: the code examples assume you\\'re operating within a Jupyter notebook. !mkdir data !wget \\\\\"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\\\\\" -O data/UBER.zip !unzip data/UBER.zip -d data  ```  We use the Unstructured library to parse the HTML files into formatted text. We have a direct integration with Unstructured through LlamaHub - this allows us to convert any text into a Document format that LlamaIndex can ingest.  ```python  from llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage from pathlib import Path  years = [2022, 2021, 2020, 2019] UnstructuredReader = download_loader(\\\\\"UnstructuredReader\\\\\", refresh_cache=True)  loader = UnstructuredReader() doc_set = {} all_docs = [] for year in years:     year_docs = loader.load_data(file=Path(f\\'./data/UBER/UBER_{year}.html\\'), split_documents=False)     # insert year metadata into each year     for d in year_docs:         d.metadata = {\\\\\"year\\\\\": year}     doc_set[year] = year_docs     all_docs.extend(year_docs) ```\", \"Setting up Vector Indices for each year  We first setup a vector index for each year. Each vector index allows us  to ask questions about the 10-K filing of a given year.  We build each index and save it to disk.  ```python\", \"initialize simple vector indices + global vector index service_context = ServiceContext.from_defaults(chunk_size=512) index_set = {} for year in years:     storage_context = StorageContext.from_defaults()     cur_index = VectorStoreIndex.from_documents(         doc_set[year],          service_context=service_context,         storage_context=storage_context,     )     index_set[year] = cur_index     storage_context.persist(persist_dir=f\\'./storage/{year}\\')  ```  To load an index from disk, do the following ```python\", \"Load indices from disk index_set = {} for year in years:     storage_context = StorageContext.from_defaults(persist_dir=f\\'./storage/{year}\\')     cur_index = load_index_from_storage(storage_context=storage_context)     index_set[year] = cur_index ```\", \"Composing a Graph to Synthesize Answers Across 10-K Filings  Since we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.   To address this, we compose a \\\\\"graph\\\\\" which consists of a list index defined over the 4 vector indices. Querying this graph would first retrieve information from each vector index, and combine information together via the list index.  ```python from llama_index import ListIndex, LLMPredictor, ServiceContext, load_graph_from_storage from llama_index.llms import OpenAI from llama_index.indices.composability import ComposableGraph\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Setup Users  In order to actually use the application (at the moment, we intend to make it possible to share certain models with unauthenticated users), you need a login. You can use either a superuser or non-superuser. In either case, someone needs to first create a superuser using the console:  **Why set up a Django superuser?** A Django superuser has all the permissions in the application and can manage all aspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a superuser allows you to fully control and manage the application.  **How to create a Django superuser:**  1 Run the following command to create a superuser:  sudo docker-compose -f local.yml run django python manage.py createsuperuser  2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required information.  **How to create additional users using Django admin:**  1. Start your Delphic application locally following the deployment instructions. 2. Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser. 3. Log in with the superuser credentials you created earlier. 4. Click on \\\\u201cUsers\\\\u201d under the \\\\u201cAuthentication and Authorization\\\\u201d section. 5. Click on the \\\\u201cAdd user +\\\\u201d button in the top right corner. 6. Enter the required information for the new user, such as username and password. Click \\\\u201cSave\\\\u201d to create the user. 7. To grant the new user additional permissions or make them a superuser, click on their username in the user list,    scroll down to the \\\\u201cPermissions\\\\u201d section, and configure their permissions accordingly. Save your changes.\", \"Full-Stack Web Application  LlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.  We provide tutorials and resources to help you get started in this area.  Relevant Resources: - Fullstack Application Guide - Fullstack Application with Delphic - A Guide to Extracting Terms and Definitions - LlamaIndex Starter Pack\", \"\\\\ud83d\\\\udcac\\\\ud83e\\\\udd16 How to Build a Chatbot  LlamaIndex is an interface between your data and LLM\\'s; it offers the toolkit for you to setup a query interface around your data for any downstream task, whether it\\'s question-answering, summarization, or more.  In this tutorial, we show you how to build a context augmented chatbot. We use Langchain for the underlying Agent/Chatbot abstractions, and we use LlamaIndex for the data retrieval/lookup/querying! The result is a chatbot agent that has access to a rich set of \\\\\"data interface\\\\\" Tools that LlamaIndex provides to answer queries over your data.  **Note**: This is a continuation of some initial work building a query interface over SEC 10-K filings - check it out here.\", \"Context  In this tutorial, we build an \\\\\"10-K Chatbot\\\\\" by downloading the raw UBER 10-K HTML filings from Dropbox. The user can choose to ask questions regarding the 10-K filings.\", \"Ingest Data  Let\\'s first download the raw 10-k files, from 2019-2022.  ```python\", \"NOTE: the code examples assume you\\'re operating within a Jupyter notebook. !mkdir data !wget \\\\\"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\\\\\" -O data/UBER.zip !unzip data/UBER.zip -d data  ```  We use the Unstructured library to parse the HTML files into formatted text. We have a direct integration with Unstructured through LlamaHub - this allows us to convert any text into a Document format that LlamaIndex can ingest.  ```python  from llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage from pathlib import Path  years = [2022, 2021, 2020, 2019] UnstructuredReader = download_loader(\\\\\"UnstructuredReader\\\\\", refresh_cache=True)  loader = UnstructuredReader() doc_set = {} all_docs = [] for year in years:     year_docs = loader.load_data(file=Path(f\\'./data/UBER/UBER_{year}.html\\'), split_documents=False)     # insert year metadata into each year     for d in year_docs:         d.metadata = {\\\\\"year\\\\\": year}     doc_set[year] = year_docs     all_docs.extend(year_docs) ```\", \"Setting up Vector Indices for each year  We first setup a vector index for each year. Each vector index allows us  to ask questions about the 10-K filing of a given year.  We build each index and save it to disk.  ```python\", \"initialize simple vector indices + global vector index service_context = ServiceContext.from_defaults(chunk_size=512) index_set = {} for year in years:     storage_context = StorageContext.from_defaults()     cur_index = VectorStoreIndex.from_documents(         doc_set[year],          service_context=service_context,         storage_context=storage_context,     )     index_set[year] = cur_index     storage_context.persist(persist_dir=f\\'./storage/{year}\\')  ```  To load an index from disk, do the following ```python\", \"Load indices from disk index_set = {} for year in years:     storage_context = StorageContext.from_defaults(persist_dir=f\\'./storage/{year}\\')     cur_index = load_index_from_storage(storage_context=storage_context)     index_set[year] = cur_index ```\", \"Composing a Graph to Synthesize Answers Across 10-K Filings  Since we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.   To address this, we compose a \\\\\"graph\\\\\" which consists of a list index defined over the 4 vector indices. Querying this graph would first retrieve information from each vector index, and combine information together via the list index.  ```python from llama_index import ListIndex, LLMPredictor, ServiceContext, load_graph_from_storage from llama_index.llms import OpenAI from llama_index.indices.composability import ComposableGraph\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=064ea23043f3ad091ca8f3a359da160b response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=064ea23043f3ad091ca8f3a359da160b response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"describe each index to help traversal of composed graph index_summaries = [f\\\\\"UBER 10-k Filing for {year} fiscal year\\\\\" for year in years]\", \"define an LLMPredictor set number of output tokens llm = OpenAI(temperature=0, max_tokens=512, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(llm=llm) storage_context = StorageContext.from_defaults()\", \"define a list index over the vector indices graph = ComposableGraph.from_indices(     ListIndex,     [index_set[y] for y in years],      index_summaries=index_summaries,     service_context=service_context,     storage_context = storage_context, ) root_id = graph.root_id\", \"[optional] save to disk storage_context.persist(persist_dir=f\\'./storage/root\\')\", \"[optional] load from disk, so you don\\'t need to build graph from scratch graph = load_graph_from_storage(     root_id=root_id,      service_context=service_context,     storage_context=storage_context, )  ```\", \"Setting up the Tools + Langchain Chatbot Agent  We use Langchain to setup the outer chatbot agent, which has access to a set of Tools. LlamaIndex provides some wrappers around indices and graphs so that they can be easily used within a Tool interface.  ```python\", \"do imports from langchain.chains.conversation.memory import ConversationBufferMemory from langchain.agents import initialize_agent  from llama_index import LLMPredictor from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig ```  We want to define a separate Tool for each index (corresponding to a given year), as well  as the graph. We can define all tools under a central `LlamaToolkit` interface.  Below, we define a `IndexToolConfig` for our graph. Note that we also import a `DecomposeQueryTransform` module for use within each vector index within the graph - this allows us to \\\\\"decompose\\\\\" the overall query into a query that can be answered from each subindex. (see example below).  ```python\", \"define a decompose transform from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     LLMPredictor(llm=llm), verbose=True )\", \"define custom retrievers from llama_index.query_engine.transform_query_engine import TransformQueryEngine  custom_query_engines = {} for index in index_set.values():     query_engine = index.as_query_engine()     query_engine = TransformQueryEngine(         query_engine,         query_transform=decompose_transform,         transform_extra_info={\\'index_summary\\': index.index_struct.summary},     )     custom_query_engines[index.index_id] = query_engine custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(     response_mode=\\'tree_summarize\\',     verbose=True, )\", \"construct query engine graph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"describe each index to help traversal of composed graph index_summaries = [f\\\\\"UBER 10-k Filing for {year} fiscal year\\\\\" for year in years]\", \"define an LLMPredictor set number of output tokens llm = OpenAI(temperature=0, max_tokens=512, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(llm=llm) storage_context = StorageContext.from_defaults()\", \"define a list index over the vector indices graph = ComposableGraph.from_indices(     ListIndex,     [index_set[y] for y in years],      index_summaries=index_summaries,     service_context=service_context,     storage_context = storage_context, ) root_id = graph.root_id\", \"[optional] save to disk storage_context.persist(persist_dir=f\\'./storage/root\\')\", \"[optional] load from disk, so you don\\'t need to build graph from scratch graph = load_graph_from_storage(     root_id=root_id,      service_context=service_context,     storage_context=storage_context, )  ```\", \"Setting up the Tools + Langchain Chatbot Agent  We use Langchain to setup the outer chatbot agent, which has access to a set of Tools. LlamaIndex provides some wrappers around indices and graphs so that they can be easily used within a Tool interface.  ```python\", \"do imports from langchain.chains.conversation.memory import ConversationBufferMemory from langchain.agents import initialize_agent  from llama_index import LLMPredictor from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig ```  We want to define a separate Tool for each index (corresponding to a given year), as well  as the graph. We can define all tools under a central `LlamaToolkit` interface.  Below, we define a `IndexToolConfig` for our graph. Note that we also import a `DecomposeQueryTransform` module for use within each vector index within the graph - this allows us to \\\\\"decompose\\\\\" the overall query into a query that can be answered from each subindex. (see example below).  ```python\", \"define a decompose transform from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     LLMPredictor(llm=llm), verbose=True )\", \"define custom retrievers from llama_index.query_engine.transform_query_engine import TransformQueryEngine  custom_query_engines = {} for index in index_set.values():     query_engine = index.as_query_engine()     query_engine = TransformQueryEngine(         query_engine,         query_transform=decompose_transform,         transform_extra_info={\\'index_summary\\': index.index_struct.summary},     )     custom_query_engines[index.index_id] = query_engine custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(     response_mode=\\'tree_summarize\\',     verbose=True, )\", \"construct query engine graph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=252 request_id=c3f3813e8c0a50fdb0a7a558a525b6bc response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=252 request_id=c3f3813e8c0a50fdb0a7a558a525b6bc response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"tool config graph_config = IndexToolConfig(     query_engine=graph_query_engine,     name=f\\\\\"Graph Index\\\\\",     description=\\\\\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.\\\\\",     tool_kwargs={\\\\\"return_direct\\\\\": True} ) ```  Besides the `IndexToolConfig` object for the graph, we also define an `IndexToolConfig` corresponding to each index:  ```python\", \"define toolkit index_configs = [] for y in range(2019, 2023):     query_engine = index_set[y].as_query_engine(         similarity_top_k=3,     )     tool_config = IndexToolConfig(         query_engine=query_engine,          name=f\\\\\"Vector Index {y}\\\\\",         description=f\\\\\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\\\\\",         tool_kwargs={\\\\\"return_direct\\\\\": True}     )     index_configs.append(tool_config) ```  Finally, we combine these configs with our `LlamaToolkit`:   ```python toolkit = LlamaToolkit(     index_configs=index_configs + [graph_config], ) ```   Finally, we call `create_llama_chat_agent` to create our Langchain chatbot agent, which has access to the 5 Tools we defined above:  ```python memory = ConversationBufferMemory(memory_key=\\\\\"chat_history\\\\\") llm=OpenAI(temperature=0) agent_chain = create_llama_chat_agent(     toolkit,     llm,     memory=memory,     verbose=True ) ```\", \"Testing the Agent  We can now test the agent with various queries.If we test it with a simple \\\\\"hello\\\\\" query, the agent does not use any Tools.```python agent_chain.run(input=\\\\\"hi, i am bob\\\\\") ```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?No AI: Hi Bob, nice to meet you!How can I help you today?> Finished chain.\\'Hi Bob, nice to meet you!How can I help you today?\\'```  If we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.```python agent_chain.run(input=\\\\\"What were some of the biggest risk factors in 2020 for Uber?\\\\\")```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes Action: Vector Index 2020 Action Input: Risk Factors ...Observation:   Risk Factors  The COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business, financial condition, and results of operations.... \\'\\\\\\\\n\\\\\\\\nRisk Factors\\\\\\\\n\\\\\\\\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business,  ```  Finally, if we test it with a query to compare/contrast risk factors across years, the agent will use the graph index Tool.```python cross_query_str = (     \\\\\"Compare/contrast the risk factors described in the Uber 10-K across years.Give answer in bullet points.\\\\\") agent_chain.run(input=cross_query_str) ```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes Action: Graph Index Action Input: Compare/contrast the risk factors described in the Uber 10-K across years.> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 964 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  The risk factors described in the Uber 10-K for the 2022 fiscal year include: the potential for changes in the classification of Drivers, the potential for increased competition, the potential for... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 590 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  1.The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business.2.Our business would be adversely ... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?\", \"INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 516 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  The risk factors described in the Uber 10-K for the 2020 fiscal year include: the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental ... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1020 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens INFO:llama_index.indices.common.tree.base:> Building index from nodes: 0 chunks > Got response:  Risk factors described in the Uber 10-K for the 2019 fiscal year include: competition from other transportation providers; the impact of government regulations; the impact of litigation; the impac... INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7039 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 72 tokens  Observation:  In 2020, the risk factors included the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental authorities, the further impact on the business of Drivers  ...  ```\", \"Setting up the Chatbot Loop  Now that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to converse with our SEC-augmented chatbot!   ```python while True:     text_input = input(\\\\\"User: \\\\\")     response = agent_chain.run(input=text_input)     print(f\\'Agent: {response}\\')      ```  Here\\'s an example of the loop in action: ``` User:  What were some of the legal proceedings against Uber in 2022? Agent:   In 2022, legal proceedings against Uber include a motion to compel arbitration, an appeal of a ruling that Proposition 22 is unconstitutional, a complaint alleging that drivers are employees and entitled to protections under the wage and labor laws, a summary judgment motion, allegations of misclassification of drivers and related employment violations in New York, fraud related to certain deductions, class actions in Australia alleging that Uber entities conspired to injure the group members during the period 2014 to 2017 by either directly breaching transport legislation or commissioning offenses against transport legislation by UberX Drivers in Australia, and claims of lost income and decreased value of certain taxi. Additionally, Uber is facing a challenge in California Superior Court alleging that Proposition 22 is unconstitutional, and a preliminary injunction order prohibiting Uber from classifying Drivers as independent contractors and from violating various wage and hour laws.  User:   ```\", \"Notebook  Take a look at our corresponding notebook.\", \"Chatbots  Chatbots are an incredibly popular use case for LLM\\'s. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.  Relevant Resources: - Building a Chatbot - Using with a LangChain Agent\", \"Component Wise Evaluation To do more in-depth evaluation of your pipeline, it helps to break it down into an evaluation of individual components.   For instance, a particular failure case may be due to a combination of not retrieving the right documents and also the LLM misunderstanding the context and hallucinating an incorrect result. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\", \"Utilizing public benchmarks When doing initial model selection, it helps to look at how well the model is performing on a standardized, diverse set of domains or tasks.  A useful benchmark for embeddings is the MTEB Leaderboard.\", \"Evaluating Retrieval  BEIR is useful for benchmarking if a particular retrieval model generalize well to niche domains in a zero-shot setting.  Since most publically-available embedding and retrieval models are already benchmarked against BEIR (e.g. through the MTEB benchmark), utilizing BEIR is more helpful when you have a unique model that you want to evaluate.   For instance, after fine-tuning an embedding model on your dataset, it may be helpful to view whether and by how much its performance degrades on a diverse set of domains. This can be an indication of how much data drift may affect your retrieval accuracy, such as if you add documents to your RAG system outside of your fine-tuning training distribution.  Here is a notebook showing how the BEIR dataset can be used with your retrieval pipeline.   ```{toctree} --- maxdepth: 1 --- /examples/evaluation/BeirEvaluation.ipynb ```  We will be adding more methods to evaluate retrieval soon. This includes evaluating retrieval on your own dataset.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"tool config graph_config = IndexToolConfig(     query_engine=graph_query_engine,     name=f\\\\\"Graph Index\\\\\",     description=\\\\\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.\\\\\",     tool_kwargs={\\\\\"return_direct\\\\\": True} ) ```  Besides the `IndexToolConfig` object for the graph, we also define an `IndexToolConfig` corresponding to each index:  ```python\", \"define toolkit index_configs = [] for y in range(2019, 2023):     query_engine = index_set[y].as_query_engine(         similarity_top_k=3,     )     tool_config = IndexToolConfig(         query_engine=query_engine,          name=f\\\\\"Vector Index {y}\\\\\",         description=f\\\\\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\\\\\",         tool_kwargs={\\\\\"return_direct\\\\\": True}     )     index_configs.append(tool_config) ```  Finally, we combine these configs with our `LlamaToolkit`:   ```python toolkit = LlamaToolkit(     index_configs=index_configs + [graph_config], ) ```   Finally, we call `create_llama_chat_agent` to create our Langchain chatbot agent, which has access to the 5 Tools we defined above:  ```python memory = ConversationBufferMemory(memory_key=\\\\\"chat_history\\\\\") llm=OpenAI(temperature=0) agent_chain = create_llama_chat_agent(     toolkit,     llm,     memory=memory,     verbose=True ) ```\", \"Testing the Agent  We can now test the agent with various queries.If we test it with a simple \\\\\"hello\\\\\" query, the agent does not use any Tools.```python agent_chain.run(input=\\\\\"hi, i am bob\\\\\") ```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?No AI: Hi Bob, nice to meet you!How can I help you today?> Finished chain.\\'Hi Bob, nice to meet you!How can I help you today?\\'```  If we test it with a query regarding the 10-k of a given year, the agent will use the relevant vector index Tool.```python agent_chain.run(input=\\\\\"What were some of the biggest risk factors in 2020 for Uber?\\\\\")```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes Action: Vector Index 2020 Action Input: Risk Factors ...Observation:   Risk Factors  The COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business, financial condition, and results of operations.... \\'\\\\\\\\n\\\\\\\\nRisk Factors\\\\\\\\n\\\\\\\\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business,  ```  Finally, if we test it with a query to compare/contrast risk factors across years, the agent will use the graph index Tool.```python cross_query_str = (     \\\\\"Compare/contrast the risk factors described in the Uber 10-K across years.Give answer in bullet points.\\\\\") agent_chain.run(input=cross_query_str) ```  ``` > Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes Action: Graph Index Action Input: Compare/contrast the risk factors described in the Uber 10-K across years.> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 964 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  The risk factors described in the Uber 10-K for the 2022 fiscal year include: the potential for changes in the classification of Drivers, the potential for increased competition, the potential for... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 590 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  1.The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business.2.Our business would be adversely ... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?\", \"INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 516 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens > Got response:  The risk factors described in the Uber 10-K for the 2020 fiscal year include: the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental ... > Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1020 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens INFO:llama_index.indices.common.tree.base:> Building index from nodes: 0 chunks > Got response:  Risk factors described in the Uber 10-K for the 2019 fiscal year include: competition from other transportation providers; the impact of government regulations; the impact of litigation; the impac... INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7039 tokens INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 72 tokens  Observation:  In 2020, the risk factors included the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental authorities, the further impact on the business of Drivers  ...  ```\", \"Setting up the Chatbot Loop  Now that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to converse with our SEC-augmented chatbot!   ```python while True:     text_input = input(\\\\\"User: \\\\\")     response = agent_chain.run(input=text_input)     print(f\\'Agent: {response}\\')      ```  Here\\'s an example of the loop in action: ``` User:  What were some of the legal proceedings against Uber in 2022? Agent:   In 2022, legal proceedings against Uber include a motion to compel arbitration, an appeal of a ruling that Proposition 22 is unconstitutional, a complaint alleging that drivers are employees and entitled to protections under the wage and labor laws, a summary judgment motion, allegations of misclassification of drivers and related employment violations in New York, fraud related to certain deductions, class actions in Australia alleging that Uber entities conspired to injure the group members during the period 2014 to 2017 by either directly breaching transport legislation or commissioning offenses against transport legislation by UberX Drivers in Australia, and claims of lost income and decreased value of certain taxi. Additionally, Uber is facing a challenge in California Superior Court alleging that Proposition 22 is unconstitutional, and a preliminary injunction order prohibiting Uber from classifying Drivers as independent contractors and from violating various wage and hour laws.  User:   ```\", \"Notebook  Take a look at our corresponding notebook.\", \"Chatbots  Chatbots are an incredibly popular use case for LLM\\'s. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.  Relevant Resources: - Building a Chatbot - Using with a LangChain Agent\", \"Component Wise Evaluation To do more in-depth evaluation of your pipeline, it helps to break it down into an evaluation of individual components.   For instance, a particular failure case may be due to a combination of not retrieving the right documents and also the LLM misunderstanding the context and hallucinating an incorrect result. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\", \"Utilizing public benchmarks When doing initial model selection, it helps to look at how well the model is performing on a standardized, diverse set of domains or tasks.  A useful benchmark for embeddings is the MTEB Leaderboard.\", \"Evaluating Retrieval  BEIR is useful for benchmarking if a particular retrieval model generalize well to niche domains in a zero-shot setting.  Since most publically-available embedding and retrieval models are already benchmarked against BEIR (e.g. through the MTEB benchmark), utilizing BEIR is more helpful when you have a unique model that you want to evaluate.   For instance, after fine-tuning an embedding model on your dataset, it may be helpful to view whether and by how much its performance degrades on a diverse set of domains. This can be an indication of how much data drift may affect your retrieval accuracy, such as if you add documents to your RAG system outside of your fine-tuning training distribution.  Here is a notebook showing how the BEIR dataset can be used with your retrieval pipeline.   ```{toctree} --- maxdepth: 1 --- /examples/evaluation/BeirEvaluation.ipynb ```  We will be adding more methods to evaluate retrieval soon. This includes evaluating retrieval on your own dataset.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=62 request_id=eee552e0d2864b0c5545b931b21ff12c response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=62 request_id=eee552e0d2864b0c5545b931b21ff12c response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Evaluating the Query Engine Components (e.g. Without Retrieval)  In this case, we may want to evaluate how specific components of a query engine (one which may generate sub-questions or follow-up questions) may perform on a standard benchmark. It can help give an indication of how far behind or ahead your retrieval pipeline is compared to alternate pipelines or models.\", \"HotpotQA Dataset  The HotpotQA dataset is useful for evaluating queries that require multiple retrieval steps.  Example: ```{toctree} --- maxdepth: 1 --- /examples/evaluation/HotpotQADistractor.ipynb ```  Limitations:  1. HotpotQA is evaluated on a Wikipedia corpus. LLMs, especially GPT4, tend to have memorized information from Wikipedia relatively well. Hence, the benchmark is not particularly good for evaluating retrieval + rerank systems with knowledgeable models like GPT4.\", \"The Development Pathway  In your journey to developing an LLM application, it helps to start with a discovery phase of understanding your data and doing some identification of issues and corner cases as you interact with the system.   Over time, you would try to formalize processes and evaluation methodology, setting up tools for observability, debugging and experiment tracking, and eventually production monitoring.  Below, we provide some additional guidance on considerations and hurdles you may face when developing your application.\", \"The Challenges of Building a Production-Ready LLM Application Many who are interested in the LLM application space are not machine learning engineers but rather software developers or  even non-technical folk.   One of the biggest strides forward that LLMs and foundation models have made to the AI/ML application landscape is that it makes it really easy to go from idea to prototype without facing all of the hurdles and uncertainty of a traditional machine learning project.  This would have involved collecting, exploring and cleaning data, keeping up with latest research and exploring different methods, training models, adjusting hyperparameters, and dealing with unexpected issues in model quality.   The huge infrastructure burden, long development cycle, and high risk to reward ratio have been blockers to successful applications.  At the same time, despite the fact that getting a prototype working quickly through frameworks like LlamaIndex has become a lot more accessible, deploying a machine learning product in the real world is still rife with uncertainty and challenges.\", \"Quality and User Interaction On the tamer side, one may face quality issues, and in the worse case, one may be liable to losing user trust if the application proves itself to be unreliable.   We\\'ve already seen a bit of this with ChatGPT - despite its life-likeness and seeming ability to understand our conversations and requests, it often makes things up (\\\\\"hallucinates\\\\\"). It\\'s not connected to the real world, data, or other digital applications.  It is important to be able to monitor, track and improve against quality issues.\", \"Tradeoffs in LLM Application Development There are a few tradeoffs in LLM application development: 1. **Cost** - more powerful models may be more expensive 2. **Latency** - more powerful models may be slower 3. **Simplicity** (one size fits all) - how powerful and flexible is the model / pipeline? 4. **Reliability / Useability** - is my application working at least in the general case? Is it ready for unstructured user interaction? Have I covered the major usage patterns?  LLM infra improvements are progressing quickly and we expect cost and latency to go down over time.    Here are some additional concerns: 1. **Evaluation** - Once I start diving deeper into improving quality, how can I evaluate individual components? How can I keep track of issues and track whether / how they are being improved over time as I change my application? 2. **Data-Driven** - How can I automate more of my evaluation and iteration process? How do I start small and add useful data points over time? How can I organize different datasets and metrics which serving different purposes? How can I manage the complexity while keeping track of my guiding light of providing the best user experience?  3. **Customization / Complexity Tradeoff** - How do I improve each stage of the pipeline - preprocessing and feature extraction, retrieval, generation? Does this involve adding additional structure or processing? How can I break down this goal into more measurable and trackable sub-goals?  Differences between **Evaluation** and being **Data-Driven**: 1. **Evaluation** does not necessarily have to be rigorous or fully data-driven process - especially at the initial stages. It is more concerned with the initial *development* phase of the application - validating that the overall pipeline works in the general case and starting to define possible signals and metrics which may be carried forward into production. 2. Being **Data-Driven** is closely tied to *automation*. After we\\'ve chosen our basic application structure, how can we improve the system over time? How can we ensure quality in a systematic way? How can we reduce the cost of monitoring, and what are the pathways to adding and curating data points? How can we leverage ML systems (including but not limited to LLMs) to make this process easier?  Additional considerations: 1. **Privacy** - how can I ensure that my data is not leaked if I am feeding it into these models? What infrastructure am I using and what is the security guarantee / how is the access control structured?\", \"Development Hurdles  Here are some potential problems you may encounter when developing your LLM application which may lead to unsatisfactory results.\", \"Retrieval  1. **Out of Domain:** If your data is extremely specific (medical, legal, scientific, financial, or other documents with technical lingo), it may be worth:     - trying out alternate embeddings        - Check the MTEB Leaderboard       - You may configure a local embedding model with the steps here     - testing out fine-tuning of embeddings         - Tools: setfit         - Anecdotally, we have seen retrieval accuracy improve by ~12% by curating a small annotated dataset from production data         - Even synthetic data generation without human labels has been shown to improve retrieval metrics across similar documents in train / val sets.         - More detailed guides and case studies will come soon.     - testing out sparse retrieval methods (see ColBERT, SPLADE)         - these methods have been shown to generalize well to out of domain data         - that are starting to be available in some enterprise systems (e.g. Elastic Search\\'s ELSeR)     - checking out our evaluation principles guide on how you might evaluate the above changes\", \"End-to-End Evaluation End-to-End evaluation should be the guiding signal for your RAG application - will my pipeline generate the right responses given the data sources and a set of queries?  While it helps initially to individually inspect queries and responses, as you deal with more failure and corner cases, it may stop being feasible to look at each query individually, and rather it may help instead to define a set of summary metrics or automated evaluation, and gain an intuition for what they might be telling you and where you might dive deeper.\", \"Setting up an Evaluation Set  It is helpful to start off with a small but diverse set of queries, and build up more examples as one discovers problematic queries or interactions.  We\\'ve created some tools that automatically generate a dataset for you given a set of documents to query. (See example below).   ```{toctree} --- maxdepth: 1 --- /examples/evaluation/QuestionGeneration.ipynb ```  In the future, we will also be able to create datasets automatically against tools.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Evaluating the Query Engine Components (e.g. Without Retrieval)  In this case, we may want to evaluate how specific components of a query engine (one which may generate sub-questions or follow-up questions) may perform on a standard benchmark. It can help give an indication of how far behind or ahead your retrieval pipeline is compared to alternate pipelines or models.\", \"HotpotQA Dataset  The HotpotQA dataset is useful for evaluating queries that require multiple retrieval steps.  Example: ```{toctree} --- maxdepth: 1 --- /examples/evaluation/HotpotQADistractor.ipynb ```  Limitations:  1. HotpotQA is evaluated on a Wikipedia corpus. LLMs, especially GPT4, tend to have memorized information from Wikipedia relatively well. Hence, the benchmark is not particularly good for evaluating retrieval + rerank systems with knowledgeable models like GPT4.\", \"The Development Pathway  In your journey to developing an LLM application, it helps to start with a discovery phase of understanding your data and doing some identification of issues and corner cases as you interact with the system.   Over time, you would try to formalize processes and evaluation methodology, setting up tools for observability, debugging and experiment tracking, and eventually production monitoring.  Below, we provide some additional guidance on considerations and hurdles you may face when developing your application.\", \"The Challenges of Building a Production-Ready LLM Application Many who are interested in the LLM application space are not machine learning engineers but rather software developers or  even non-technical folk.   One of the biggest strides forward that LLMs and foundation models have made to the AI/ML application landscape is that it makes it really easy to go from idea to prototype without facing all of the hurdles and uncertainty of a traditional machine learning project.  This would have involved collecting, exploring and cleaning data, keeping up with latest research and exploring different methods, training models, adjusting hyperparameters, and dealing with unexpected issues in model quality.   The huge infrastructure burden, long development cycle, and high risk to reward ratio have been blockers to successful applications.  At the same time, despite the fact that getting a prototype working quickly through frameworks like LlamaIndex has become a lot more accessible, deploying a machine learning product in the real world is still rife with uncertainty and challenges.\", \"Quality and User Interaction On the tamer side, one may face quality issues, and in the worse case, one may be liable to losing user trust if the application proves itself to be unreliable.   We\\'ve already seen a bit of this with ChatGPT - despite its life-likeness and seeming ability to understand our conversations and requests, it often makes things up (\\\\\"hallucinates\\\\\"). It\\'s not connected to the real world, data, or other digital applications.  It is important to be able to monitor, track and improve against quality issues.\", \"Tradeoffs in LLM Application Development There are a few tradeoffs in LLM application development: 1. **Cost** - more powerful models may be more expensive 2. **Latency** - more powerful models may be slower 3. **Simplicity** (one size fits all) - how powerful and flexible is the model / pipeline? 4. **Reliability / Useability** - is my application working at least in the general case? Is it ready for unstructured user interaction? Have I covered the major usage patterns?  LLM infra improvements are progressing quickly and we expect cost and latency to go down over time.    Here are some additional concerns: 1. **Evaluation** - Once I start diving deeper into improving quality, how can I evaluate individual components? How can I keep track of issues and track whether / how they are being improved over time as I change my application? 2. **Data-Driven** - How can I automate more of my evaluation and iteration process? How do I start small and add useful data points over time? How can I organize different datasets and metrics which serving different purposes? How can I manage the complexity while keeping track of my guiding light of providing the best user experience?  3. **Customization / Complexity Tradeoff** - How do I improve each stage of the pipeline - preprocessing and feature extraction, retrieval, generation? Does this involve adding additional structure or processing? How can I break down this goal into more measurable and trackable sub-goals?  Differences between **Evaluation** and being **Data-Driven**: 1. **Evaluation** does not necessarily have to be rigorous or fully data-driven process - especially at the initial stages. It is more concerned with the initial *development* phase of the application - validating that the overall pipeline works in the general case and starting to define possible signals and metrics which may be carried forward into production. 2. Being **Data-Driven** is closely tied to *automation*. After we\\'ve chosen our basic application structure, how can we improve the system over time? How can we ensure quality in a systematic way? How can we reduce the cost of monitoring, and what are the pathways to adding and curating data points? How can we leverage ML systems (including but not limited to LLMs) to make this process easier?  Additional considerations: 1. **Privacy** - how can I ensure that my data is not leaked if I am feeding it into these models? What infrastructure am I using and what is the security guarantee / how is the access control structured?\", \"Development Hurdles  Here are some potential problems you may encounter when developing your LLM application which may lead to unsatisfactory results.\", \"Retrieval  1. **Out of Domain:** If your data is extremely specific (medical, legal, scientific, financial, or other documents with technical lingo), it may be worth:     - trying out alternate embeddings        - Check the MTEB Leaderboard       - You may configure a local embedding model with the steps here     - testing out fine-tuning of embeddings         - Tools: setfit         - Anecdotally, we have seen retrieval accuracy improve by ~12% by curating a small annotated dataset from production data         - Even synthetic data generation without human labels has been shown to improve retrieval metrics across similar documents in train / val sets.         - More detailed guides and case studies will come soon.     - testing out sparse retrieval methods (see ColBERT, SPLADE)         - these methods have been shown to generalize well to out of domain data         - that are starting to be available in some enterprise systems (e.g. Elastic Search\\'s ELSeR)     - checking out our evaluation principles guide on how you might evaluate the above changes\", \"End-to-End Evaluation End-to-End evaluation should be the guiding signal for your RAG application - will my pipeline generate the right responses given the data sources and a set of queries?  While it helps initially to individually inspect queries and responses, as you deal with more failure and corner cases, it may stop being feasible to look at each query individually, and rather it may help instead to define a set of summary metrics or automated evaluation, and gain an intuition for what they might be telling you and where you might dive deeper.\", \"Setting up an Evaluation Set  It is helpful to start off with a small but diverse set of queries, and build up more examples as one discovers problematic queries or interactions.  We\\'ve created some tools that automatically generate a dataset for you given a set of documents to query. (See example below).   ```{toctree} --- maxdepth: 1 --- /examples/evaluation/QuestionGeneration.ipynb ```  In the future, we will also be able to create datasets automatically against tools.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=133 request_id=9a57d799137936909253b1ddcae9317d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=133 request_id=9a57d799137936909253b1ddcae9317d response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"The Spectrum of Evaluation Options  Quantitative eval is more useful when evaluating applications where there is a correct answer - for instance, validating that the choice of tools and their inputs are correct given the plan, or retrieving specific pieces of information, or attempting to produce intermediate output of a certain schema (e.g. JSON fields).  Qualitative eval is more useful when generating long-form responses that are meant to be *helpful* but not necessarily completely accurate.  There is a spectrum of evaluation options ranging from metrics, cheaper models, more expensive models (GPT4), and human evaluation.  Below is some example usage of the evaluation modules:  ```{toctree} --- maxdepth: 1 --- /examples/evaluation/TestNYC-Evaluation-Query.ipynb /examples/evaluation/TestNYC-Evaluation.ipynb ```\", \"Discovery - Sensitivity Testing  With a complex pipeline, it may be unclear which parts of the pipeline are affecting your results.  Sensitivity testing can be a good inroad into choosing which components to individually test or tweak more thoroughly, or which parts of your dataset (e.g. queries) may be producing problematic results.  More details on how to discover issues automatically with methods such as sensitivity testing will come soon.  Examples of this in the more traditional ML domain include Giskard.\", \"Metrics Ensembling  It may be expensive to use GPT-4 to carry out evaluation especially as your dev set grows large.  Metrics ensembling uses an ensemble of weaker signals (exact match, F1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output of a more expensive evaluation methods that are closer to the gold labels (human-labelled/GPT-4).  It is intenteded for two purposes:  1. Evaluating changes cheaply and quickly across a large dataset during the development stage. 2. Flagging outliers for further evaluation (GPT-4 / human alerting) during the production monitoring stage.  We also want the metrics ensembling to be interpretable - the correlation and weighting scores should give an indication of which metrics best capture the evaluation criteria.  We will discuss more about the methodology in future updates.\", \"Evaluation\", \"Setting the Stage  LlamaIndex is meant to connect your data to your LLM applications.  Sometimes, even after diagnosing and fixing bugs by looking at traces, more fine-grained evaluation is required to systematically diagnose issues.  LlamaIndex aims to provide those tools to make identifying issues and receiving useful diagnostic signals easy.  Closely tied to evaluation are the concepts of experimentation and experiment tracking.\", \"General Strategy  When developing your LLM application, it could help to first define an end-to-end evaluation workflow, and then once you\\'ve started collecting failure or corner cases and getting an intuition for what is or isn\\'t going well, you may dive deeper into evaluating and improving specific components.   The analogy with software testing is integration tests and unit tests. You should probably start writing unit tests once you start fiddling with individual components. Equally, your gold standard on whether things are working will together are integration tests. Both are equally important.   ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/e2e_evaluation.md /end_to_end_tutorials/dev_practices/component_wise_evaluation.md ```  Here is an overview of the existing modules for evaluation. We will be adding more modules and support over time.  ```{toctree} --- maxdepth: 1 --- /core_modules/supporting_modules/evaluation/root.md ```\", \"E2E or Component-Wise - Which Do I Start With? If you want to get an overall idea of how your system is doing as you iterate upon it, it makes sense to start with centering your core development loop around the e2e eval - as an overall sanity/vibe check.  If you have an idea of what you\\'re doing and want to iterate step by step on each component, building it up as things go - you may want to start with a component-wise eval. However this may run the risk of premature optimization - making model selection or parameter choices without assessing the overall application needs. You may have to revisit these choices when creating your final application.\", \"Diving Deeper into Evaluation Evaluation is a controversial topic, and as the field of NLP has evolved, so have the methods of evaluation.  In a world where powerful foundation models are now performing annotation tasks better than human annotators, the best practices around evaluation are constantly changing. Previous methods of evaluation which were used to bootstrap and evaluate today\\'s models such as BLEU or F1 have been shown to have poor correlation with human judgements, and need to be applied prudently.  Typically, generation-heavy, open-ended tasks and requiring judgement or opinion and harder to evaluate automatically than factual questions due to their subjective nature. We will aim to provide more guides and case-studies for which methods are appropriate in a given scenario.\", \"Standard Metrics  Against annotated datasets, whether your own data or an academic benchmark, there are a number of standard metrics that it helps to be aware of:  1. **Exact Match (EM):** The percentage of queries that are answered exactly correctly. 2. **F1:** The percentage of queries that are answered exactly correctly or with a small edit distance (e.g. 1-2 words). 3. **Recall:** The percentage of queries that are answered correctly, regardless of the number of answers returned. 4. **Precision:** The percentage of queries that are answered correctly, divided by the number of answers returned.  This towardsdatascience article covers more technical metrics like NDCG, MAP and MRR in greater depth.\", \"Case Studies and Resources 1. (Course) Data-Centric AI (MIT), 2023 2. Scale\\'s Approach to LLM Testing and Evaluation 3. LLM Patterns by Eugene Yan\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"The Spectrum of Evaluation Options  Quantitative eval is more useful when evaluating applications where there is a correct answer - for instance, validating that the choice of tools and their inputs are correct given the plan, or retrieving specific pieces of information, or attempting to produce intermediate output of a certain schema (e.g. JSON fields).  Qualitative eval is more useful when generating long-form responses that are meant to be *helpful* but not necessarily completely accurate.  There is a spectrum of evaluation options ranging from metrics, cheaper models, more expensive models (GPT4), and human evaluation.  Below is some example usage of the evaluation modules:  ```{toctree} --- maxdepth: 1 --- /examples/evaluation/TestNYC-Evaluation-Query.ipynb /examples/evaluation/TestNYC-Evaluation.ipynb ```\", \"Discovery - Sensitivity Testing  With a complex pipeline, it may be unclear which parts of the pipeline are affecting your results.  Sensitivity testing can be a good inroad into choosing which components to individually test or tweak more thoroughly, or which parts of your dataset (e.g. queries) may be producing problematic results.  More details on how to discover issues automatically with methods such as sensitivity testing will come soon.  Examples of this in the more traditional ML domain include Giskard.\", \"Metrics Ensembling  It may be expensive to use GPT-4 to carry out evaluation especially as your dev set grows large.  Metrics ensembling uses an ensemble of weaker signals (exact match, F1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output of a more expensive evaluation methods that are closer to the gold labels (human-labelled/GPT-4).  It is intenteded for two purposes:  1. Evaluating changes cheaply and quickly across a large dataset during the development stage. 2. Flagging outliers for further evaluation (GPT-4 / human alerting) during the production monitoring stage.  We also want the metrics ensembling to be interpretable - the correlation and weighting scores should give an indication of which metrics best capture the evaluation criteria.  We will discuss more about the methodology in future updates.\", \"Evaluation\", \"Setting the Stage  LlamaIndex is meant to connect your data to your LLM applications.  Sometimes, even after diagnosing and fixing bugs by looking at traces, more fine-grained evaluation is required to systematically diagnose issues.  LlamaIndex aims to provide those tools to make identifying issues and receiving useful diagnostic signals easy.  Closely tied to evaluation are the concepts of experimentation and experiment tracking.\", \"General Strategy  When developing your LLM application, it could help to first define an end-to-end evaluation workflow, and then once you\\'ve started collecting failure or corner cases and getting an intuition for what is or isn\\'t going well, you may dive deeper into evaluating and improving specific components.   The analogy with software testing is integration tests and unit tests. You should probably start writing unit tests once you start fiddling with individual components. Equally, your gold standard on whether things are working will together are integration tests. Both are equally important.   ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/e2e_evaluation.md /end_to_end_tutorials/dev_practices/component_wise_evaluation.md ```  Here is an overview of the existing modules for evaluation. We will be adding more modules and support over time.  ```{toctree} --- maxdepth: 1 --- /core_modules/supporting_modules/evaluation/root.md ```\", \"E2E or Component-Wise - Which Do I Start With? If you want to get an overall idea of how your system is doing as you iterate upon it, it makes sense to start with centering your core development loop around the e2e eval - as an overall sanity/vibe check.  If you have an idea of what you\\'re doing and want to iterate step by step on each component, building it up as things go - you may want to start with a component-wise eval. However this may run the risk of premature optimization - making model selection or parameter choices without assessing the overall application needs. You may have to revisit these choices when creating your final application.\", \"Diving Deeper into Evaluation Evaluation is a controversial topic, and as the field of NLP has evolved, so have the methods of evaluation.  In a world where powerful foundation models are now performing annotation tasks better than human annotators, the best practices around evaluation are constantly changing. Previous methods of evaluation which were used to bootstrap and evaluate today\\'s models such as BLEU or F1 have been shown to have poor correlation with human judgements, and need to be applied prudently.  Typically, generation-heavy, open-ended tasks and requiring judgement or opinion and harder to evaluate automatically than factual questions due to their subjective nature. We will aim to provide more guides and case-studies for which methods are appropriate in a given scenario.\", \"Standard Metrics  Against annotated datasets, whether your own data or an academic benchmark, there are a number of standard metrics that it helps to be aware of:  1. **Exact Match (EM):** The percentage of queries that are answered exactly correctly. 2. **F1:** The percentage of queries that are answered exactly correctly or with a small edit distance (e.g. 1-2 words). 3. **Recall:** The percentage of queries that are answered correctly, regardless of the number of answers returned. 4. **Precision:** The percentage of queries that are answered correctly, divided by the number of answers returned.  This towardsdatascience article covers more technical metrics like NDCG, MAP and MRR in greater depth.\", \"Case Studies and Resources 1. (Course) Data-Centric AI (MIT), 2023 2. Scale\\'s Approach to LLM Testing and Evaluation 3. LLM Patterns by Eugene Yan\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=192 request_id=aa1e31577b8197df5034bb7c1ccc5dff response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=192 request_id=aa1e31577b8197df5034bb7c1ccc5dff response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Monitoring When developing your LLM application, it can be helpful to keep track of production data such as: -   Pipeline performance (latency/token count/throughput of various stages) -   Resource usage (LLM/Embedding Inference Cost, CPU/GPU utilization) -   Evaluation metrics (accuracy, precision, recall, qualitative eval and drift) -   Pipeline versioning (which versions of sub-components e.g. LLM/embedding &  artifacts e.g. prompts were used in the pipeline at a given time)  We will share more on how to set up monitoring in the future.\", \"Observability  In a complex LLM application with many moving parts, as with traditional software engineering, it helps to be able to inspect the artifacts and execution traces of the application.\", \"How to Set Up Observability  You may refer to our \\\\\"One-Click Observability\\\\\" guide to set up observability with your preferred observability provider.\", \"Discover LlamaIndex Video Series  This page contains links to videos + associated notebooks for our ongoing video tutorial series \\\\\"Discover LlamaIndex\\\\\".\", \"SubQuestionQueryEngine + 10K Analysis  This video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.  Youtube  Notebook\", \"Discord Document Management  This video covers managing documents from a source that is consantly updating (i.e Discord) and how you can avoid document duplication and save embedding tokens.  Youtube  Notebook + Supplimentary Material  Reference Docs\", \"Joint Text to SQL and Semantic Search  This video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.  Youtube  Notebook\", \"Finetuning\", \"Overview  Finetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.  The core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.  While finetuning can be also used to \\\\\"augment\\\\\" a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\", \"Embedding Finetuning Benefits - Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data --> leads to better retrieval performance.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Monitoring When developing your LLM application, it can be helpful to keep track of production data such as: -   Pipeline performance (latency/token count/throughput of various stages) -   Resource usage (LLM/Embedding Inference Cost, CPU/GPU utilization) -   Evaluation metrics (accuracy, precision, recall, qualitative eval and drift) -   Pipeline versioning (which versions of sub-components e.g. LLM/embedding &  artifacts e.g. prompts were used in the pipeline at a given time)  We will share more on how to set up monitoring in the future.\", \"Observability  In a complex LLM application with many moving parts, as with traditional software engineering, it helps to be able to inspect the artifacts and execution traces of the application.\", \"How to Set Up Observability  You may refer to our \\\\\"One-Click Observability\\\\\" guide to set up observability with your preferred observability provider.\", \"Discover LlamaIndex Video Series  This page contains links to videos + associated notebooks for our ongoing video tutorial series \\\\\"Discover LlamaIndex\\\\\".\", \"SubQuestionQueryEngine + 10K Analysis  This video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.  Youtube  Notebook\", \"Discord Document Management  This video covers managing documents from a source that is consantly updating (i.e Discord) and how you can avoid document duplication and save embedding tokens.  Youtube  Notebook + Supplimentary Material  Reference Docs\", \"Joint Text to SQL and Semantic Search  This video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.  Youtube  Notebook\", \"Finetuning\", \"Overview  Finetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.  The core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.  While finetuning can be also used to \\\\\"augment\\\\\" a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\", \"Embedding Finetuning Benefits - Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data --> leads to better retrieval performance.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=dfc63a7d5cae4553bdf55808c36429e5 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=56 request_id=dfc63a7d5cae4553bdf55808c36429e5 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"LLM Finetuning Benefits - Allow it to learn a style over a given dataset - Allow it to learn a DSL that might be less represented in the training data (e.g. SQL)  - Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering - Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\", \"Integrations with LlamaIndex  This is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details! - Finetuning embeddings for better retrieval performance - Finetuning Llama 2 for better text-to-SQL - Finetuning gpt-3.5-turbo to distill gpt-4\", \"Finetuning Embeddings for Better Retrieval Performance   We created a comprehensive repo/guide showing you how to finetune an open-source embedding model (in this case, `bge`) over an unstructured text corpus. It consists of the following steps: 1. Generating a synthetic question/answer dataset using LlamaIndex over any unstructed context. 2. Finetuning the model 3. Evaluating the model.  Finetuning gives you a 5-10% increase in retrieval evaluation metrics. You can then plug this fine-tuned model into your RAG application with LlamaIndex.   ```{toctree} --- maxdepth: 1 --- Embedding Fine-tuning Repo  Embedding Fine-tuning Blog  ```\", \"Finetuning GPT-3.5 to distill GPT-4  We released a guide showing how to use OpenAI\\'s finetuning endpoints to fine-tune gpt-3.5-turbo to output GPT-4 responses for a full RAG pipeline.  We use GPT-4 to automatically generate questions from any unstructured context, and use a GPT-4 query engine pipeline to generate \\\\\"ground-truth\\\\\" answers. Our `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.   We then launch a finetuning job, and get back a distilled model. We can evaluate this model with Ragas to benchmark against a naive GPT-3.5 pipeline.  ```{toctree} --- maxdepth: 1 --- GPT-3.5 Fine-tuning Notebook (Colab)  GPT-3.5 Fine-tuning Notebook  ```  **Old**  ```{toctree} --- maxdepth: 1 --- GPT-3.5 Fine-tuning Notebook (Colab)  GPT-3.5 Fine-tuning Notebook (in Repo)  ```\", \"Finetuning Llama 2 for Better Text-to-SQL  In this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using LlamaIndex abstractions.  The stack includes `sql-create-context` as the training dataset, OpenLLaMa as the base model, PEFT for finetuning, Modal for cloud compute, LlamaIndex for inference abstractions.  ```{toctree} --- maxdepth: 1 --- Llama 2 Text-to-SQL Fine-tuning (Repo)  Llama 2 Text-to-SQL Fine-tuning (Notebook)  ```\", \"Knowledge Graphs  LlamaIndex contains some fantastic guides for building with knowledge graphs.  Check out the end-to-end tutorials/workshops below. Also check out our knowledge graph query engine guides here.   ```{toctree} --- maxdepth: 1 --- LlamaIndex Workshop: Building RAG with Knowledge Graphs  REBEL + Knowledge Graph Index  ```\", \"One-Click Observability  LlamaIndex provides **one-click observability**  \\\\ud83d\\\\udd2d to allow you to build principled LLM applications in a production setting.  A key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe, debug, and evaluate your system - both as a whole and for each component.  This feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you\\'ll be able to do things like the following: - View LLM/prompt inputs/outputs - That that the outputs of any component (LLMs, embeddings) are performing as expected - View call traces for both indexing and querying  Each provider has similarities and differences. Take a look below for the full set of guides for each one!\", \"Usage Pattern  To toggle, you will generally just need to do the following:  ```python  from llama_index import set_global_handler\", \"general usage set_global_handler(\\\\\"\\\\\", **kwargs)\", \"W&B example  ```  Note that all `kwargs` to `set_global_handler` are passed to the underlying callback handler.  And that\\'s it! Executions will get seamlessly piped to downstream service (e.g. W&B Prompts) and you\\'ll be able to access features such as viewing execution traces of your application.  **NOTE**: TruLens (by TruEra) uses a different \\\\\"one-click\\\\\" experience. See below for details.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"LLM Finetuning Benefits - Allow it to learn a style over a given dataset - Allow it to learn a DSL that might be less represented in the training data (e.g. SQL)  - Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering - Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\", \"Integrations with LlamaIndex  This is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details! - Finetuning embeddings for better retrieval performance - Finetuning Llama 2 for better text-to-SQL - Finetuning gpt-3.5-turbo to distill gpt-4\", \"Finetuning Embeddings for Better Retrieval Performance   We created a comprehensive repo/guide showing you how to finetune an open-source embedding model (in this case, `bge`) over an unstructured text corpus. It consists of the following steps: 1. Generating a synthetic question/answer dataset using LlamaIndex over any unstructed context. 2. Finetuning the model 3. Evaluating the model.  Finetuning gives you a 5-10% increase in retrieval evaluation metrics. You can then plug this fine-tuned model into your RAG application with LlamaIndex.   ```{toctree} --- maxdepth: 1 --- Embedding Fine-tuning Repo  Embedding Fine-tuning Blog  ```\", \"Finetuning GPT-3.5 to distill GPT-4  We released a guide showing how to use OpenAI\\'s finetuning endpoints to fine-tune gpt-3.5-turbo to output GPT-4 responses for a full RAG pipeline.  We use GPT-4 to automatically generate questions from any unstructured context, and use a GPT-4 query engine pipeline to generate \\\\\"ground-truth\\\\\" answers. Our `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.   We then launch a finetuning job, and get back a distilled model. We can evaluate this model with Ragas to benchmark against a naive GPT-3.5 pipeline.  ```{toctree} --- maxdepth: 1 --- GPT-3.5 Fine-tuning Notebook (Colab)  GPT-3.5 Fine-tuning Notebook  ```  **Old**  ```{toctree} --- maxdepth: 1 --- GPT-3.5 Fine-tuning Notebook (Colab)  GPT-3.5 Fine-tuning Notebook (in Repo)  ```\", \"Finetuning Llama 2 for Better Text-to-SQL  In this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using LlamaIndex abstractions.  The stack includes `sql-create-context` as the training dataset, OpenLLaMa as the base model, PEFT for finetuning, Modal for cloud compute, LlamaIndex for inference abstractions.  ```{toctree} --- maxdepth: 1 --- Llama 2 Text-to-SQL Fine-tuning (Repo)  Llama 2 Text-to-SQL Fine-tuning (Notebook)  ```\", \"Knowledge Graphs  LlamaIndex contains some fantastic guides for building with knowledge graphs.  Check out the end-to-end tutorials/workshops below. Also check out our knowledge graph query engine guides here.   ```{toctree} --- maxdepth: 1 --- LlamaIndex Workshop: Building RAG with Knowledge Graphs  REBEL + Knowledge Graph Index  ```\", \"One-Click Observability  LlamaIndex provides **one-click observability**  \\\\ud83d\\\\udd2d to allow you to build principled LLM applications in a production setting.  A key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe, debug, and evaluate your system - both as a whole and for each component.  This feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you\\'ll be able to do things like the following: - View LLM/prompt inputs/outputs - That that the outputs of any component (LLMs, embeddings) are performing as expected - View call traces for both indexing and querying  Each provider has similarities and differences. Take a look below for the full set of guides for each one!\", \"Usage Pattern  To toggle, you will generally just need to do the following:  ```python  from llama_index import set_global_handler\", \"general usage set_global_handler(\\\\\"\\\\\", **kwargs)\", \"W&B example  ```  Note that all `kwargs` to `set_global_handler` are passed to the underlying callback handler.  And that\\'s it! Executions will get seamlessly piped to downstream service (e.g. W&B Prompts) and you\\'ll be able to access features such as viewing execution traces of your application.  **NOTE**: TruLens (by TruEra) uses a different \\\\\"one-click\\\\\" experience. See below for details.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=67 request_id=e26743f0820b8da16acd26b5bc3250d2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=67 request_id=e26743f0820b8da16acd26b5bc3250d2 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Partners  We offer a rich set of integrations with our partners. A short description + usage pattern, and guide is provided for each partner.\", \"Weights and Biases Prompts  Prompts allows users to log/trace/inspect the execution flow of LlamaIndex during index construction and querying. It also allows users to version-control their indices.\", \"Usage Pattern  ```python from llama_index import set_global_handler set_global_handler(\\\\\"wandb\\\\\", run_args={\\\\\"project\\\\\": \\\\\"llamaindex\\\\\"})\", \"NOTE: No need to do the following\", \"access additional methods on handler to persist index + load index import llama_index\", \"persist index llama_index.global_handler.persist_index(graph, index_name=\\\\\"composable_graph\\\\\")\", \"load storage context storage_context = llama_index.global_handler.load_storage_context(     artifact_url=\\\\\"ayut/llamaindex/composable_graph:v0\\\\\" )  ```  !\", \"Guides ```{toctree} --- maxdepth: 1 --- /examples/callbacks/WandbCallbackHandler.ipynb ```\", \"OpenInference  OpenInference is an open standard for capturing and storing AI model inferences. It enables experimentation, visualization, and evaluation of LLM applications using LLM observability solutions such as Phoenix.\", \"Usage Pattern  ```python import llama_index  llama_index.set_global_handler(\\\\\"openinference\\\\\")\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Partners  We offer a rich set of integrations with our partners. A short description + usage pattern, and guide is provided for each partner.\", \"Weights and Biases Prompts  Prompts allows users to log/trace/inspect the execution flow of LlamaIndex during index construction and querying. It also allows users to version-control their indices.\", \"Usage Pattern  ```python from llama_index import set_global_handler set_global_handler(\\\\\"wandb\\\\\", run_args={\\\\\"project\\\\\": \\\\\"llamaindex\\\\\"})\", \"NOTE: No need to do the following\", \"access additional methods on handler to persist index + load index import llama_index\", \"persist index llama_index.global_handler.persist_index(graph, index_name=\\\\\"composable_graph\\\\\")\", \"load storage context storage_context = llama_index.global_handler.load_storage_context(     artifact_url=\\\\\"ayut/llamaindex/composable_graph:v0\\\\\" )  ```  !\", \"Guides ```{toctree} --- maxdepth: 1 --- /examples/callbacks/WandbCallbackHandler.ipynb ```\", \"OpenInference  OpenInference is an open standard for capturing and storing AI model inferences. It enables experimentation, visualization, and evaluation of LLM applications using LLM observability solutions such as Phoenix.\", \"Usage Pattern  ```python import llama_index  llama_index.set_global_handler(\\\\\"openinference\\\\\")\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1006 request_id=9727042c019f777518d7c457f7a68936 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1006 request_id=9727042c019f777518d7c457f7a68936 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"NOTE: No need to do the following\", \"Run your LlamaIndex application here... for query in queries:     query_engine.query(query)\", \"View your LLM app data as a dataframe in OpenInference format. from llama_index.callbacks.open_inference_callback import as_dataframe  query_data_buffer = llama_index.global_handler.flush_query_data_buffer() query_dataframe = as_dataframe(query_data_buffer) ```  **NOTE**: To unlock capabilities of Phoenix, you will need to define additional steps to feed in query/ context dataframes. See below!\", \"Guides ```{toctree} --- maxdepth: 1 --- /examples/callbacks/OpenInferenceCallback.ipynb Evaluating and Improving a LlamaIndex Search and Retrieval Application  ```\", \"TruEra TruLens  TruLens allows users to instrument/evaluate LlamaIndex applications, through features such as feedback functions and tracing.\", \"Usage Pattern + Guides  ```python\", \"use trulens from trulens_eval import TruLlama tru_query_engine = TruLlama(query_engine)\", \"query tru_query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ``` !\", \"Guides  ```{toctree} --- maxdepth: 1 --- /community/integrations/trulens.md Quickstart Guide with LlamaIndex + TruLens  Colab  ```\", \"Principled Development Practices  In order to develop your application, it can help to implement some principled development practices.  Here we provide some general guidance to help you better anticipate the challenges and concerns you may encounter as you develop your LLM application.  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/development_pathway.md  ```  Here are the main pillars of principled development of LLM and RAG applications. - The first pillar is **observability**: setting up initial tools to observe, debug your system and evaluate it on ad-hoc examples. - The next pillar is **evaluation**: being able to evaluate different components of your system so that you can experiment and improve it in a more systematic fashion. - The last pillar is **monitoring**: after the application is deployed, we want to continuously monitor and test that it is performing well in production.   % TODO: also add UX patterns doc  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/observability.md /end_to_end_tutorials/dev_practices/evaluation.md /end_to_end_tutorials/dev_practices/monitoring.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"NOTE: No need to do the following\", \"Run your LlamaIndex application here... for query in queries:     query_engine.query(query)\", \"View your LLM app data as a dataframe in OpenInference format. from llama_index.callbacks.open_inference_callback import as_dataframe  query_data_buffer = llama_index.global_handler.flush_query_data_buffer() query_dataframe = as_dataframe(query_data_buffer) ```  **NOTE**: To unlock capabilities of Phoenix, you will need to define additional steps to feed in query/ context dataframes. See below!\", \"Guides ```{toctree} --- maxdepth: 1 --- /examples/callbacks/OpenInferenceCallback.ipynb Evaluating and Improving a LlamaIndex Search and Retrieval Application  ```\", \"TruEra TruLens  TruLens allows users to instrument/evaluate LlamaIndex applications, through features such as feedback functions and tracing.\", \"Usage Pattern + Guides  ```python\", \"use trulens from trulens_eval import TruLlama tru_query_engine = TruLlama(query_engine)\", \"query tru_query_engine.query(\\\\\"What did the author do growing up?\\\\\")  ``` !\", \"Guides  ```{toctree} --- maxdepth: 1 --- /community/integrations/trulens.md Quickstart Guide with LlamaIndex + TruLens  Colab  ```\", \"Principled Development Practices  In order to develop your application, it can help to implement some principled development practices.  Here we provide some general guidance to help you better anticipate the challenges and concerns you may encounter as you develop your LLM application.  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/development_pathway.md  ```  Here are the main pillars of principled development of LLM and RAG applications. - The first pillar is **observability**: setting up initial tools to observe, debug your system and evaluate it on ad-hoc examples. - The next pillar is **evaluation**: being able to evaluate different components of your system so that you can experiment and improve it in a more systematic fashion. - The last pillar is **monitoring**: after the application is deployed, we want to continuously monitor and test that it is performing well in production.   % TODO: also add UX patterns doc  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/dev_practices/observability.md /end_to_end_tutorials/dev_practices/evaluation.md /end_to_end_tutorials/dev_practices/monitoring.md ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=110 request_id=5baccc5a1b380cfaa061b6af93a5c8ba response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=110 request_id=5baccc5a1b380cfaa061b6af93a5c8ba response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Contribute Your Insights! If you have thoughts on sections to add or how to improve, please make a contribution (link)\", \"Private Setup  Relevant Resources: - Using LlamaIndex with Local Models\", \"A Guide to Extracting Terms and Definitions  Llama Index has many use cases (semantic search, summarization, etc.) that are well documented. However, this doesn\\'t mean we can\\'t apply Llama Index to very specific use cases!  In this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using Streamlit, we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.  This tutorial assumes you have Python3.9+ and the following packages installed:  - llama-index - streamlit  At the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.  The final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\", \"Uploading Text  Step one is giving users a way to upload documents. Let\\\\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.  ```python import streamlit as st  st.title(\\\\\"\\\\ud83e\\\\udd99 Llama Index Term Extractor \\\\ud83e\\\\udd99\\\\\")  document_text = st.text_area(\\\\\"Or enter raw text\\\\\") if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:     with st.spinner(\\\\\"Extracting...\\\\\"):         extracted_terms = document text  # this is a placeholder!     st.write(extracted_terms) ```  Super simple right! But you\\'ll notice that the app doesn\\'t do anything useful yet. To use llama_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what\\'s best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\", \"LLM Settings  This next step introduces some tabs to our app, to separate it into different panes that provide different features. Let\\'s create a tab for LLM settings and for uploading text:  ```python import os import streamlit as st  DEFAULT_TERM_STR = (     \\\\\"Make a list of terms and definitions that are defined in the context, \\\\\"     \\\\\"with one pair on each line. \\\\\"     \\\\\"If a term is missing it\\'s definition, use your best judgment. \\\\\"     \\\\\"Write each line as as follows:\\\\\\\\nTerm:  Definition: \\\\\" )  st.title(\\\\\"\\\\ud83e\\\\udd99 Llama Index Term Extractor \\\\ud83e\\\\udd99\\\\\")  setup_tab, upload_tab = st.tabs([\\\\\"Setup\\\\\", \\\\\"Upload/Extract Terms\\\\\"])  with setup_tab:     st.subheader(\\\\\"LLM Setup\\\\\")     api_key = st.text_input(\\\\\"Enter your OpenAI API key here\\\\\", type=\\\\\"password\\\\\")     llm_name = st.selectbox(\\'Which LLM?\\', [\\\\\"text-davinci-003\\\\\", \\\\\"gpt-3.5-turbo\\\\\", \\\\\"gpt-4\\\\\"])     model_temperature = st.slider(\\\\\"LLM Temperature\\\\\", min_value=0.0, max_value=1.0, step=0.1)     term_extract_str = st.text_area(\\\\\"The query to extract terms and definitions with.\\\\\", value=DEFAULT_TERM_STR)  with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     document_text = st.text_area(\\\\\"Or enter raw text\\\\\")     if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:         with st.spinner(\\\\\"Extracting...\\\\\"):             extracted_terms = document text  # this is a placeholder!         st.write(extracted_terms) ```  Now our app has two tabs, which really helps with the organization. You\\'ll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it\\'s just the prompt I arrived at after experimenting a bit.  Speaking of extracting terms, it\\'s time to add some functions to do just that!\", \"Extracting and Storing Terms  Now that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!We can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.```python from llama_index import Document, ListIndex, LLMPredictor, ServiceContext, load_index_from_storage from llama_index.llms import OpenAI  def get_llm(llm_name, model_temperature, api_key, max_tokens=256):     os.environ[\\'OPENAI_API_KEY\\'] = api_key     return OpenAI(temperature=model_temperature, model=llm_name, max_tokens=max_tokens)  def extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):     llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)      service_context = ServiceContext.from_defaults(llm=llm,                                                    chunk_size=1024)      temp_index = ListIndex.from_documents(documents, service_context=service_context)     query_engine = temp_index.as_query_engine(response_mode=\\\\\"tree_summarize\\\\\")     terms_definitions = str(query_engine.query(term_extract_str))     terms_definitions = [x for x in terms_definitions.split(\\\\\"\\\\\\\\n\\\\\") if x and \\'Term:\\' in x and \\'Definition:\\' in x]     # parse the text into a dict     terms_to_definition = {x.split(\\\\\"Definition:\\\\\")[0].split(\\\\\"Term:\\\\\")[-1].strip(): x.split(\\\\\"Definition:\\\\\")[-1].strip() for x in terms_definitions}     return terms_to_definition ```  Now, using the new functions, we can finally extract our terms!```python ... with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     document_text = st.text_area(\\\\\"Or enter raw text\\\\\")     if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:         with st.spinner(\\\\\"Extracting...\\\\\"):             extracted_terms = extract_terms([Document(text=document_text)],                                             term_extract_str, llm_name,                                             model_temperature, api_key)         st.write(extracted_terms) ```  There\\'s a lot going on now, let\\'s take a moment to go over what is happening.`get_llm()` is instantiating the LLM based on the user configuration from the setup tab.Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).`extract_terms()` is where all the good stuff happens.First, we call `get_llm()` with `max_tokens=1024`, since we don\\'t want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set).Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output.When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.Next, we create a temporary list index and pass in our service context.\", \"A list index will read every single piece of text in our index, which is perfect for extracting terms.Finally, we use our pre-defined query text to extract terms, using `response_mode=\\\\\"tree_summarize`.This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children.Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.Lastly, we do some minor post processing.We assume the model followed instructions and put a term/definition pair on each line.If a line is missing the `Term:` or `Definition:` labels, we skip it.Then, we convert this to a dictionary for easy storage!\", \"Saving Extracted Terms  Now that we can extract terms, we need to put them somewhere so that we can query for them later.A `VectorStoreIndex` should be a perfect choice for now!But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later.Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!First things first though, let\\'s add a feature to initialize a global vector index and another function to insert the extracted terms.```python ... if \\'all_terms\\' not in st.session_state:     st.session_state[\\'all_terms\\'] = DEFAULT_TERMS ...  def insert_terms(terms_to_definition):     for term, definition in terms_to_definition.items():         doc = Document(text=f\\\\\"Term: {term}\\\\\\\\nDefinition: {definition}\\\\\")         st.session_state[\\'llama_index\\'].insert(doc)  @st.cache_resource def initialize_index(llm_name, model_temperature, api_key):     \\\\\"\\\\\"\\\\\"Create the VectorStoreIndex object.\\\\\"\\\\\"\\\\\"llm = get_llm(llm_name, model_temperature, api_key)      service_context = ServiceContext.from_defaults(llm=llm)      index = VectorStoreIndex([], service_context=service_context)      return index  ...  with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\"):         st.session_state[\\'llama_index\\'] = initialize_index(llm_name, model_temperature, api_key)         st.session_state[\\'all_terms\\'] = {}      if \\\\\"llama_index\\\\\" in st.session_state:         st.markdown(\\\\\"Either upload an image/screenshot of a document, or enter the text manually.\\\\\")document_text = st.text_area(\\\\\"Or enter raw text\\\\\")         if st.button(\\\\\"Extract Terms and Definitions\\\\\") and (uploaded_file or document_text):             st.session_state[\\'terms\\'] = {}             terms_docs = {}             with st.spinner(\\\\\"Extracting...\\\\\"):                 terms_docs.update(extract_terms([Document(text=document_text)], term_extract_str, llm_name, model_temperature, api_key))             st.session_state[\\'terms\\'].update(terms_docs)          if \\\\\"terms\\\\\" in st.session_state and st.session_state[\\\\\"terms\\\\\"]::             st.markdown(\\\\\"Extracted terms\\\\\")             st.json(st.session_state[\\'terms\\'])              if st.button(\\\\\"Insert terms?\\\\\"):                 with st.spinner(\\\\\"Inserting terms\\\\\"):                     insert_terms(st.session_state[\\'terms\\'])                 st.session_state[\\'all_terms\\'].update(st.session_state[\\'terms\\'])                 st.session_state[\\'terms\\'] = {}                 st.experimental_rerun() ```  Now you are really starting to leverage the power of streamlit!Let\\'s start with the code under the upload tab.We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms.\", \"Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting.If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.\", \"Querying for Extracted Terms/Definitions  With the terms and definitions extracted and saved, how can we use them? And how will the user even remember what\\'s previously been saved?? We can simply add some more tabs to the app to handle these features.  ```python ... setup_tab, terms_tab, upload_tab, query_tab = st.tabs(     [\\\\\"Setup\\\\\", \\\\\"All Terms\\\\\", \\\\\"Upload/Extract Terms\\\\\", \\\\\"Query Terms\\\\\"] ) ... with terms_tab:     with terms_tab:     st.subheader(\\\\\"Current Extracted Terms and Definitions\\\\\")     st.json(st.session_state[\\\\\"all_terms\\\\\"]) ... with query_tab:     st.subheader(\\\\\"Query for Terms/Definitions!\\\\\")     st.markdown(         (             \\\\\"The LLM will attempt to answer your query, and augment it\\'s answers using the terms/definitions you\\'ve inserted. \\\\\"             \\\\\"If a term is not in the index, it will answer using it\\'s internal knowledge.\\\\\"         )     )     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\", key=\\\\\"init_index_2\\\\\"):         st.session_state[\\\\\"llama_index\\\\\"] = initialize_index(             llm_name, model_temperature, api_key         )         st.session_state[\\\\\"all_terms\\\\\"] = {}      if \\\\\"llama_index\\\\\" in st.session_state:         query_text = st.text_input(\\\\\"Ask about a term or definition:\\\\\")         if query_text:             query_text = query_text + \\\\\"\\\\\\\\nIf you can\\'t find the answer, answer the query with the best of your knowledge.\\\\\"             with st.spinner(\\\\\"Generating answer...\\\\\"):                 response = st.session_state[\\\\\"llama_index\\\\\"].query(                     query_text, similarity_top_k=5, response_mode=\\\\\"compact\\\\\"                 )             st.markdown(str(response)) ```  While this is mostly basic, some important things to note:  - Our initialize button has the same text as our other button. Streamlit will complain about this, so we provide a unique key instead. - Some additional text has been added to the query! This is to try and compensate for times when the index does not have the answer. - In our index query, we\\'ve specified two options:   - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.   - `response_mode=\\\\\"compact\\\\\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call. Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Contribute Your Insights! If you have thoughts on sections to add or how to improve, please make a contribution (link)\", \"Private Setup  Relevant Resources: - Using LlamaIndex with Local Models\", \"A Guide to Extracting Terms and Definitions  Llama Index has many use cases (semantic search, summarization, etc.) that are well documented. However, this doesn\\'t mean we can\\'t apply Llama Index to very specific use cases!  In this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using Streamlit, we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.  This tutorial assumes you have Python3.9+ and the following packages installed:  - llama-index - streamlit  At the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.  The final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\", \"Uploading Text  Step one is giving users a way to upload documents. Let\\\\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.  ```python import streamlit as st  st.title(\\\\\"\\\\ud83e\\\\udd99 Llama Index Term Extractor \\\\ud83e\\\\udd99\\\\\")  document_text = st.text_area(\\\\\"Or enter raw text\\\\\") if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:     with st.spinner(\\\\\"Extracting...\\\\\"):         extracted_terms = document text  # this is a placeholder!     st.write(extracted_terms) ```  Super simple right! But you\\'ll notice that the app doesn\\'t do anything useful yet. To use llama_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what\\'s best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\", \"LLM Settings  This next step introduces some tabs to our app, to separate it into different panes that provide different features. Let\\'s create a tab for LLM settings and for uploading text:  ```python import os import streamlit as st  DEFAULT_TERM_STR = (     \\\\\"Make a list of terms and definitions that are defined in the context, \\\\\"     \\\\\"with one pair on each line. \\\\\"     \\\\\"If a term is missing it\\'s definition, use your best judgment. \\\\\"     \\\\\"Write each line as as follows:\\\\\\\\nTerm:  Definition: \\\\\" )  st.title(\\\\\"\\\\ud83e\\\\udd99 Llama Index Term Extractor \\\\ud83e\\\\udd99\\\\\")  setup_tab, upload_tab = st.tabs([\\\\\"Setup\\\\\", \\\\\"Upload/Extract Terms\\\\\"])  with setup_tab:     st.subheader(\\\\\"LLM Setup\\\\\")     api_key = st.text_input(\\\\\"Enter your OpenAI API key here\\\\\", type=\\\\\"password\\\\\")     llm_name = st.selectbox(\\'Which LLM?\\', [\\\\\"text-davinci-003\\\\\", \\\\\"gpt-3.5-turbo\\\\\", \\\\\"gpt-4\\\\\"])     model_temperature = st.slider(\\\\\"LLM Temperature\\\\\", min_value=0.0, max_value=1.0, step=0.1)     term_extract_str = st.text_area(\\\\\"The query to extract terms and definitions with.\\\\\", value=DEFAULT_TERM_STR)  with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     document_text = st.text_area(\\\\\"Or enter raw text\\\\\")     if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:         with st.spinner(\\\\\"Extracting...\\\\\"):             extracted_terms = document text  # this is a placeholder!         st.write(extracted_terms) ```  Now our app has two tabs, which really helps with the organization. You\\'ll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it\\'s just the prompt I arrived at after experimenting a bit.  Speaking of extracting terms, it\\'s time to add some functions to do just that!\", \"Extracting and Storing Terms  Now that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!We can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.```python from llama_index import Document, ListIndex, LLMPredictor, ServiceContext, load_index_from_storage from llama_index.llms import OpenAI  def get_llm(llm_name, model_temperature, api_key, max_tokens=256):     os.environ[\\'OPENAI_API_KEY\\'] = api_key     return OpenAI(temperature=model_temperature, model=llm_name, max_tokens=max_tokens)  def extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):     llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)      service_context = ServiceContext.from_defaults(llm=llm,                                                    chunk_size=1024)      temp_index = ListIndex.from_documents(documents, service_context=service_context)     query_engine = temp_index.as_query_engine(response_mode=\\\\\"tree_summarize\\\\\")     terms_definitions = str(query_engine.query(term_extract_str))     terms_definitions = [x for x in terms_definitions.split(\\\\\"\\\\\\\\n\\\\\") if x and \\'Term:\\' in x and \\'Definition:\\' in x]     # parse the text into a dict     terms_to_definition = {x.split(\\\\\"Definition:\\\\\")[0].split(\\\\\"Term:\\\\\")[-1].strip(): x.split(\\\\\"Definition:\\\\\")[-1].strip() for x in terms_definitions}     return terms_to_definition ```  Now, using the new functions, we can finally extract our terms!```python ... with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     document_text = st.text_area(\\\\\"Or enter raw text\\\\\")     if st.button(\\\\\"Extract Terms and Definitions\\\\\") and document_text:         with st.spinner(\\\\\"Extracting...\\\\\"):             extracted_terms = extract_terms([Document(text=document_text)],                                             term_extract_str, llm_name,                                             model_temperature, api_key)         st.write(extracted_terms) ```  There\\'s a lot going on now, let\\'s take a moment to go over what is happening.`get_llm()` is instantiating the LLM based on the user configuration from the setup tab.Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).`extract_terms()` is where all the good stuff happens.First, we call `get_llm()` with `max_tokens=1024`, since we don\\'t want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set).Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output.When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.Next, we create a temporary list index and pass in our service context.\", \"A list index will read every single piece of text in our index, which is perfect for extracting terms.Finally, we use our pre-defined query text to extract terms, using `response_mode=\\\\\"tree_summarize`.This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children.Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.Lastly, we do some minor post processing.We assume the model followed instructions and put a term/definition pair on each line.If a line is missing the `Term:` or `Definition:` labels, we skip it.Then, we convert this to a dictionary for easy storage!\", \"Saving Extracted Terms  Now that we can extract terms, we need to put them somewhere so that we can query for them later.A `VectorStoreIndex` should be a perfect choice for now!But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later.Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!First things first though, let\\'s add a feature to initialize a global vector index and another function to insert the extracted terms.```python ... if \\'all_terms\\' not in st.session_state:     st.session_state[\\'all_terms\\'] = DEFAULT_TERMS ...  def insert_terms(terms_to_definition):     for term, definition in terms_to_definition.items():         doc = Document(text=f\\\\\"Term: {term}\\\\\\\\nDefinition: {definition}\\\\\")         st.session_state[\\'llama_index\\'].insert(doc)  @st.cache_resource def initialize_index(llm_name, model_temperature, api_key):     \\\\\"\\\\\"\\\\\"Create the VectorStoreIndex object.\\\\\"\\\\\"\\\\\"llm = get_llm(llm_name, model_temperature, api_key)      service_context = ServiceContext.from_defaults(llm=llm)      index = VectorStoreIndex([], service_context=service_context)      return index  ...  with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\"):         st.session_state[\\'llama_index\\'] = initialize_index(llm_name, model_temperature, api_key)         st.session_state[\\'all_terms\\'] = {}      if \\\\\"llama_index\\\\\" in st.session_state:         st.markdown(\\\\\"Either upload an image/screenshot of a document, or enter the text manually.\\\\\")document_text = st.text_area(\\\\\"Or enter raw text\\\\\")         if st.button(\\\\\"Extract Terms and Definitions\\\\\") and (uploaded_file or document_text):             st.session_state[\\'terms\\'] = {}             terms_docs = {}             with st.spinner(\\\\\"Extracting...\\\\\"):                 terms_docs.update(extract_terms([Document(text=document_text)], term_extract_str, llm_name, model_temperature, api_key))             st.session_state[\\'terms\\'].update(terms_docs)          if \\\\\"terms\\\\\" in st.session_state and st.session_state[\\\\\"terms\\\\\"]::             st.markdown(\\\\\"Extracted terms\\\\\")             st.json(st.session_state[\\'terms\\'])              if st.button(\\\\\"Insert terms?\\\\\"):                 with st.spinner(\\\\\"Inserting terms\\\\\"):                     insert_terms(st.session_state[\\'terms\\'])                 st.session_state[\\'all_terms\\'].update(st.session_state[\\'terms\\'])                 st.session_state[\\'terms\\'] = {}                 st.experimental_rerun() ```  Now you are really starting to leverage the power of streamlit!Let\\'s start with the code under the upload tab.We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms.\", \"Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting.If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.\", \"Querying for Extracted Terms/Definitions  With the terms and definitions extracted and saved, how can we use them? And how will the user even remember what\\'s previously been saved?? We can simply add some more tabs to the app to handle these features.  ```python ... setup_tab, terms_tab, upload_tab, query_tab = st.tabs(     [\\\\\"Setup\\\\\", \\\\\"All Terms\\\\\", \\\\\"Upload/Extract Terms\\\\\", \\\\\"Query Terms\\\\\"] ) ... with terms_tab:     with terms_tab:     st.subheader(\\\\\"Current Extracted Terms and Definitions\\\\\")     st.json(st.session_state[\\\\\"all_terms\\\\\"]) ... with query_tab:     st.subheader(\\\\\"Query for Terms/Definitions!\\\\\")     st.markdown(         (             \\\\\"The LLM will attempt to answer your query, and augment it\\'s answers using the terms/definitions you\\'ve inserted. \\\\\"             \\\\\"If a term is not in the index, it will answer using it\\'s internal knowledge.\\\\\"         )     )     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\", key=\\\\\"init_index_2\\\\\"):         st.session_state[\\\\\"llama_index\\\\\"] = initialize_index(             llm_name, model_temperature, api_key         )         st.session_state[\\\\\"all_terms\\\\\"] = {}      if \\\\\"llama_index\\\\\" in st.session_state:         query_text = st.text_input(\\\\\"Ask about a term or definition:\\\\\")         if query_text:             query_text = query_text + \\\\\"\\\\\\\\nIf you can\\'t find the answer, answer the query with the best of your knowledge.\\\\\"             with st.spinner(\\\\\"Generating answer...\\\\\"):                 response = st.session_state[\\\\\"llama_index\\\\\"].query(                     query_text, similarity_top_k=5, response_mode=\\\\\"compact\\\\\"                 )             st.markdown(str(response)) ```  While this is mostly basic, some important things to note:  - Our initialize button has the same text as our other button. Streamlit will complain about this, so we provide a unique key instead. - Some additional text has been added to the query! This is to try and compensate for times when the index does not have the answer. - In our index query, we\\'ve specified two options:   - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.   - `response_mode=\\\\\"compact\\\\\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call. Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=79 request_id=535715054d7053e7b10e39eff3c5563e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=79 request_id=535715054d7053e7b10e39eff3c5563e response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Dry Run Test  Well, actually I hope you\\'ve been testing as we went. But now, let\\'s try one complete test.  1. Refresh the app 2. Enter your LLM settings 3. Head over to the query tab 4. Ask the following: `What is a bunnyhug?` 5. The app should give some nonsense response. If you didn\\'t know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies! 6. Let\\'s add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.` 7. Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it! 8. If we open the terms tab, the term and definition we just extracted should be displayed 9. Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\", \"Improvement 1 - Create a Starting Index  With our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app\\'s query capabilities? We can do just that! First, let\\'s make a small change to our app so that we save the index to disk after every upload:  ```python def insert_terms(terms_to_definition):     for term, definition in terms_to_definition.items():         doc = Document(text=f\\\\\"Term: {term}\\\\\\\\nDefinition: {definition}\\\\\")         st.session_state[\\'llama_index\\'].insert(doc)     # TEMPORARY - save to disk     st.session_state[\\'llama_index\\'].storage_context.persist() ```  Now, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text here.  If you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.  After inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:  ```python @st.cache_resource def initialize_index(llm_name, model_temperature, api_key):     \\\\\"\\\\\"\\\\\"Load the Index object.\\\\\"\\\\\"\\\\\"     llm = get_llm(llm_name, model_temperature, api_key)      service_context = ServiceContext.from_defaults(llm=llm)      index = load_index_from_storage(service_context=service_context)      return index ```  Did you remember to save that giant list of extracted terms in a notepad? Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:  ```python ... if \\\\\"all_terms\\\\\" not in st.session_state:     st.session_state[\\\\\"all_terms\\\\\"] = DEFAULT_TERMS ... ```  Repeat the above anywhere where we were previously resetting the `all_terms` values.\", \"Improvement 2 - (Refining) Better Prompts  If you play around with the app a bit now, you might notice that it stopped following our prompt! Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.  This is due to the concept of \\\\\"refining\\\\\" answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.  So, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let\\'s create those now, using the default prompts and chat specific prompts as a guide. Using a new file `constants.py`, let\\'s create some new query templates:  ```python from llama_index.prompts import PromptTemplate, SelectorPromptTemplate, ChatPromptTemplate from llama_index.prompts.utils import is_chat_model from llama_index.llms.base import ChatMessage, MessageRole\", \"Text QA templates DEFAULT_TEXT_QA_PROMPT_TMPL = (     \\\\\"Context information is below. \\\\\\\\n\\\\\"     \\\\\"---------------------\\\\\\\\n\\\\\"     \\\\\"{context_str}\\\\\"     \\\\\"\\\\\\\\n---------------------\\\\\\\\n\\\\\"     \\\\\"Given the context information answer the following question \\\\\"     \\\\\"(if you don\\'t know the answer, use the best of your knowledge): {query_str}\\\\\\\\n\\\\\" ) TEXT_QA_TEMPLATE = PromptTemplate(DEFAULT_TEXT_QA_PROMPT_TMPL)\", \"Refine templates DEFAULT_REFINE_PROMPT_TMPL = (     \\\\\"The original question is as follows: {query_str}\\\\\\\\n\\\\\"     \\\\\"We have provided an existing answer: {existing_answer}\\\\\\\\n\\\\\"     \\\\\"We have the opportunity to refine the existing answer \\\\\"     \\\\\"(only if needed) with some more context below.\\\\\\\\n\\\\\"     \\\\\"------------\\\\\\\\n\\\\\"     \\\\\"{context_msg}\\\\\\\\n\\\\\"     \\\\\"------------\\\\\\\\n\\\\\"     \\\\\"Given the new context and using the best of your knowledge, improve the existing answer. \\\\\"     \\\\\"If you can\\'t improve the existing answer, just repeat it again.\\\\\" ) DEFAULT_REFINE_PROMPT = PromptTemplate(DEFAULT_REFINE_PROMPT_TMPL)  CHAT_REFINE_PROMPT_TMPL_MSGS = [     ChatMessage(content=\\\\\"{query_str}\\\\\", role=MessageRole.USER),     ChatMessage(content=\\\\\"{existing_answer}\\\\\", role=MessageRole.ASSISTANT),     ChatMessage(         content=\\\\\"We have the opportunity to refine the above answer \\\\\"         \\\\\"(only if needed) with some more context below.\\\\\\\\n\\\\\"         \\\\\"------------\\\\\\\\n\\\\\"         \\\\\"{context_msg}\\\\\\\\n\\\\\"         \\\\\"------------\\\\\\\\n\\\\\"         \\\\\"Given the new context and using the best of your knowledge, improve the existing answer. \\\\\"         \\\\\"If you can\\'t improve the existing answer, just repeat it again.\\\\\",         role=MessageRole.USER,     ), ]  CHAT_REFINE_PROMPT = ChatPromptTemplate(CHAT_REFINE_PROMPT_TMPL_MSGS)\", \"refine prompt selector REFINE_TEMPLATE = SelectorPromptTemplate(     default_template=DEFAULT_REFINE_PROMPT,     conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)], ) ```  That seems like a lot of code, but it\\'s not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.  Another thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \\\\\"human\\\\\" message.  So, now we can import these prompts into our app and use them during the query.  ```python from constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE ...     if \\\\\"llama_index\\\\\" in st.session_state:         query_text = st.text_input(\\\\\"Ask about a term or definition:\\\\\")         if query_text:             query_text = query_text  # Notice we removed the old instructions             with st.spinner(\\\\\"Generating answer...\\\\\"):                 response = st.session_state[\\\\\"llama_index\\\\\"].query(                     query_text, similarity_top_k=5, response_mode=\\\\\"compact\\\\\",                     text_qa_template=TEXT_QA_TEMPLATE, refine_template=REFINE_TEMPLATE                 )             st.markdown(str(response)) ... ```  If you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!\", \"Improvement 3 - Image Support  Llama index also supports images!Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text.We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.If you get an import error about PIL, install it using `pip install Pillow` first.```python from PIL import Image from llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser  @st.cache_resource def get_file_extractor():     image_parser = ImageParser(keep_image=True, parse_text=True)     file_extractor = DEFAULT_FILE_EXTRACTOR     file_extractor.update(         {             \\\\\".jpg\\\\\": image_parser,             \\\\\".png\\\\\": image_parser,             \\\\\".jpeg\\\\\": image_parser,         }     )      return file_extractor  file_extractor = get_file_extractor() ... with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\", key=\\\\\"init_index_1\\\\\"):         st.session_state[\\\\\"llama_index\\\\\"] = initialize_index(             llm_name, model_temperature, api_key         )         st.session_state[\\\\\"all_terms\\\\\"] = DEFAULT_TERMS      if \\\\\"llama_index\\\\\" in st.session_state:         st.markdown(             \\\\\"Either upload an image/screenshot of a document, or enter the text manually.\\\\\")         uploaded_file = st.file_uploader(             \\\\\"Upload an image/screenshot of a document:\\\\\", type=[\\\\\"png\\\\\", \\\\\"jpg\\\\\", \\\\\"jpeg\\\\\"]         )         document_text = st.text_area(\\\\\"Or enter raw text\\\\\")         if st.button(\\\\\"Extract Terms and Definitions\\\\\") and (             uploaded_file or document_text         ):             st.session_state[\\\\\"terms\\\\\"] = {}             terms_docs = {}             with st.spinner(\\\\\"Extracting (images may be slow).\\\\\"):                 if document_text:                     terms_docs.update(                         extract_terms(                             [Document(text=document_text)],                             term_extract_str,                             llm_name,                             model_temperature,                             api_key,\", \")                     )                 if uploaded_file:                     Image.open(uploaded_file).convert(\\\\\"RGB\\\\\").save(\\\\\"temp.png\\\\\")                     img_reader = SimpleDirectoryReader(                         input_files=[\\\\\"temp.png\\\\\"], file_extractor=file_extractor                     )                     img_docs = img_reader.load_data()                     os.remove(\\\\\"temp.png\\\\\")                     terms_docs.update(                         extract_terms(                             img_docs,                             term_extract_str,                             llm_name,                             model_temperature,                             api_key,                         )                     )             st.session_state[\\\\\"terms\\\\\"].update(terms_docs)          if \\\\\"terms\\\\\" in st.session_state and st.session_state[\\\\\"terms\\\\\"]:             st.markdown(\\\\\"Extracted terms\\\\\")             st.json(st.session_state[\\\\\"terms\\\\\"])              if st.button(\\\\\"Insert terms?\\\\\"):                 with st.spinner(\\\\\"Inserting terms\\\\\"):                     insert_terms(st.session_state[\\\\\"terms\\\\\"])                 st.session_state[\\\\\"all_terms\\\\\"].update(st.session_state[\\\\\"terms\\\\\"])                 st.session_state[\\\\\"terms\\\\\"] = {}                 st.experimental_rerun() ```  Here, we added the option to upload a file using Streamlit.Then the image is opened and saved to disk (this seems hacky but it keeps things simple).Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.Now that we have the documents, we can call `extract_terms()` the same as before.\", \"Conclusion/TLDR  In this tutorial, we covered a ton of information, while solving some common issues and problems along the way:  - Using different indexes for different use cases (List vs. Vector index) - Storing global state values with Streamlit\\'s `session_state` concept - Customizing internal prompts with Llama Index - Reading text from images with Llama Index  The final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\", \"A Guide to Creating a Unified Query Framework over your Indexes  LlamaIndex offers a variety of different use cases.  For simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `ListIndex` for summarization.  For more complex queries, we may want to use a composable graph.  But how do we integrate indexes and graphs into our LLM application? Different indexes and graphs may be better suited for different types of queries that you may want to run.  In this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Dry Run Test  Well, actually I hope you\\'ve been testing as we went. But now, let\\'s try one complete test.  1. Refresh the app 2. Enter your LLM settings 3. Head over to the query tab 4. Ask the following: `What is a bunnyhug?` 5. The app should give some nonsense response. If you didn\\'t know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies! 6. Let\\'s add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.` 7. Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it! 8. If we open the terms tab, the term and definition we just extracted should be displayed 9. Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\", \"Improvement 1 - Create a Starting Index  With our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app\\'s query capabilities? We can do just that! First, let\\'s make a small change to our app so that we save the index to disk after every upload:  ```python def insert_terms(terms_to_definition):     for term, definition in terms_to_definition.items():         doc = Document(text=f\\\\\"Term: {term}\\\\\\\\nDefinition: {definition}\\\\\")         st.session_state[\\'llama_index\\'].insert(doc)     # TEMPORARY - save to disk     st.session_state[\\'llama_index\\'].storage_context.persist() ```  Now, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text here.  If you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.  After inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:  ```python @st.cache_resource def initialize_index(llm_name, model_temperature, api_key):     \\\\\"\\\\\"\\\\\"Load the Index object.\\\\\"\\\\\"\\\\\"     llm = get_llm(llm_name, model_temperature, api_key)      service_context = ServiceContext.from_defaults(llm=llm)      index = load_index_from_storage(service_context=service_context)      return index ```  Did you remember to save that giant list of extracted terms in a notepad? Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:  ```python ... if \\\\\"all_terms\\\\\" not in st.session_state:     st.session_state[\\\\\"all_terms\\\\\"] = DEFAULT_TERMS ... ```  Repeat the above anywhere where we were previously resetting the `all_terms` values.\", \"Improvement 2 - (Refining) Better Prompts  If you play around with the app a bit now, you might notice that it stopped following our prompt! Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.  This is due to the concept of \\\\\"refining\\\\\" answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.  So, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let\\'s create those now, using the default prompts and chat specific prompts as a guide. Using a new file `constants.py`, let\\'s create some new query templates:  ```python from llama_index.prompts import PromptTemplate, SelectorPromptTemplate, ChatPromptTemplate from llama_index.prompts.utils import is_chat_model from llama_index.llms.base import ChatMessage, MessageRole\", \"Text QA templates DEFAULT_TEXT_QA_PROMPT_TMPL = (     \\\\\"Context information is below. \\\\\\\\n\\\\\"     \\\\\"---------------------\\\\\\\\n\\\\\"     \\\\\"{context_str}\\\\\"     \\\\\"\\\\\\\\n---------------------\\\\\\\\n\\\\\"     \\\\\"Given the context information answer the following question \\\\\"     \\\\\"(if you don\\'t know the answer, use the best of your knowledge): {query_str}\\\\\\\\n\\\\\" ) TEXT_QA_TEMPLATE = PromptTemplate(DEFAULT_TEXT_QA_PROMPT_TMPL)\", \"Refine templates DEFAULT_REFINE_PROMPT_TMPL = (     \\\\\"The original question is as follows: {query_str}\\\\\\\\n\\\\\"     \\\\\"We have provided an existing answer: {existing_answer}\\\\\\\\n\\\\\"     \\\\\"We have the opportunity to refine the existing answer \\\\\"     \\\\\"(only if needed) with some more context below.\\\\\\\\n\\\\\"     \\\\\"------------\\\\\\\\n\\\\\"     \\\\\"{context_msg}\\\\\\\\n\\\\\"     \\\\\"------------\\\\\\\\n\\\\\"     \\\\\"Given the new context and using the best of your knowledge, improve the existing answer. \\\\\"     \\\\\"If you can\\'t improve the existing answer, just repeat it again.\\\\\" ) DEFAULT_REFINE_PROMPT = PromptTemplate(DEFAULT_REFINE_PROMPT_TMPL)  CHAT_REFINE_PROMPT_TMPL_MSGS = [     ChatMessage(content=\\\\\"{query_str}\\\\\", role=MessageRole.USER),     ChatMessage(content=\\\\\"{existing_answer}\\\\\", role=MessageRole.ASSISTANT),     ChatMessage(         content=\\\\\"We have the opportunity to refine the above answer \\\\\"         \\\\\"(only if needed) with some more context below.\\\\\\\\n\\\\\"         \\\\\"------------\\\\\\\\n\\\\\"         \\\\\"{context_msg}\\\\\\\\n\\\\\"         \\\\\"------------\\\\\\\\n\\\\\"         \\\\\"Given the new context and using the best of your knowledge, improve the existing answer. \\\\\"         \\\\\"If you can\\'t improve the existing answer, just repeat it again.\\\\\",         role=MessageRole.USER,     ), ]  CHAT_REFINE_PROMPT = ChatPromptTemplate(CHAT_REFINE_PROMPT_TMPL_MSGS)\", \"refine prompt selector REFINE_TEMPLATE = SelectorPromptTemplate(     default_template=DEFAULT_REFINE_PROMPT,     conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)], ) ```  That seems like a lot of code, but it\\'s not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.  Another thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \\\\\"human\\\\\" message.  So, now we can import these prompts into our app and use them during the query.  ```python from constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE ...     if \\\\\"llama_index\\\\\" in st.session_state:         query_text = st.text_input(\\\\\"Ask about a term or definition:\\\\\")         if query_text:             query_text = query_text  # Notice we removed the old instructions             with st.spinner(\\\\\"Generating answer...\\\\\"):                 response = st.session_state[\\\\\"llama_index\\\\\"].query(                     query_text, similarity_top_k=5, response_mode=\\\\\"compact\\\\\",                     text_qa_template=TEXT_QA_TEMPLATE, refine_template=REFINE_TEMPLATE                 )             st.markdown(str(response)) ... ```  If you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!\", \"Improvement 3 - Image Support  Llama index also supports images!Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text.We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.If you get an import error about PIL, install it using `pip install Pillow` first.```python from PIL import Image from llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser  @st.cache_resource def get_file_extractor():     image_parser = ImageParser(keep_image=True, parse_text=True)     file_extractor = DEFAULT_FILE_EXTRACTOR     file_extractor.update(         {             \\\\\".jpg\\\\\": image_parser,             \\\\\".png\\\\\": image_parser,             \\\\\".jpeg\\\\\": image_parser,         }     )      return file_extractor  file_extractor = get_file_extractor() ... with upload_tab:     st.subheader(\\\\\"Extract and Query Definitions\\\\\")     if st.button(\\\\\"Initialize Index and Reset Terms\\\\\", key=\\\\\"init_index_1\\\\\"):         st.session_state[\\\\\"llama_index\\\\\"] = initialize_index(             llm_name, model_temperature, api_key         )         st.session_state[\\\\\"all_terms\\\\\"] = DEFAULT_TERMS      if \\\\\"llama_index\\\\\" in st.session_state:         st.markdown(             \\\\\"Either upload an image/screenshot of a document, or enter the text manually.\\\\\")         uploaded_file = st.file_uploader(             \\\\\"Upload an image/screenshot of a document:\\\\\", type=[\\\\\"png\\\\\", \\\\\"jpg\\\\\", \\\\\"jpeg\\\\\"]         )         document_text = st.text_area(\\\\\"Or enter raw text\\\\\")         if st.button(\\\\\"Extract Terms and Definitions\\\\\") and (             uploaded_file or document_text         ):             st.session_state[\\\\\"terms\\\\\"] = {}             terms_docs = {}             with st.spinner(\\\\\"Extracting (images may be slow).\\\\\"):                 if document_text:                     terms_docs.update(                         extract_terms(                             [Document(text=document_text)],                             term_extract_str,                             llm_name,                             model_temperature,                             api_key,\", \")                     )                 if uploaded_file:                     Image.open(uploaded_file).convert(\\\\\"RGB\\\\\").save(\\\\\"temp.png\\\\\")                     img_reader = SimpleDirectoryReader(                         input_files=[\\\\\"temp.png\\\\\"], file_extractor=file_extractor                     )                     img_docs = img_reader.load_data()                     os.remove(\\\\\"temp.png\\\\\")                     terms_docs.update(                         extract_terms(                             img_docs,                             term_extract_str,                             llm_name,                             model_temperature,                             api_key,                         )                     )             st.session_state[\\\\\"terms\\\\\"].update(terms_docs)          if \\\\\"terms\\\\\" in st.session_state and st.session_state[\\\\\"terms\\\\\"]:             st.markdown(\\\\\"Extracted terms\\\\\")             st.json(st.session_state[\\\\\"terms\\\\\"])              if st.button(\\\\\"Insert terms?\\\\\"):                 with st.spinner(\\\\\"Inserting terms\\\\\"):                     insert_terms(st.session_state[\\\\\"terms\\\\\"])                 st.session_state[\\\\\"all_terms\\\\\"].update(st.session_state[\\\\\"terms\\\\\"])                 st.session_state[\\\\\"terms\\\\\"] = {}                 st.experimental_rerun() ```  Here, we added the option to upload a file using Streamlit.Then the image is opened and saved to disk (this seems hacky but it keeps things simple).Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.Now that we have the documents, we can call `extract_terms()` the same as before.\", \"Conclusion/TLDR  In this tutorial, we covered a ton of information, while solving some common issues and problems along the way:  - Using different indexes for different use cases (List vs. Vector index) - Storing global state values with Streamlit\\'s `session_state` concept - Customizing internal prompts with Llama Index - Reading text from images with Llama Index  The final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\", \"A Guide to Creating a Unified Query Framework over your Indexes  LlamaIndex offers a variety of different use cases.  For simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `ListIndex` for summarization.  For more complex queries, we may want to use a composable graph.  But how do we integrate indexes and graphs into our LLM application? Different indexes and graphs may be better suited for different types of queries that you may want to run.  In this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=153 request_id=9abcbe192bce42b92234686dd9c1d28a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=153 request_id=9abcbe192bce42b92234686dd9c1d28a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Setup  In this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.  The below code snippet downloads the relevant data into files.  ```python  from pathlib import Path import requests  wiki_titles = [\\\\\"Toronto\\\\\", \\\\\"Seattle\\\\\", \\\\\"Chicago\\\\\", \\\\\"Boston\\\\\", \\\\\"Houston\\\\\"]  for title in wiki_titles:     response = requests.get(         \\'https://en.wikipedia.org/w/api.php\\',         params={             \\'action\\': \\'query\\',             \\'format\\': \\'json\\',             \\'titles\\': title,             \\'prop\\': \\'extracts\\',             # \\'exintro\\': True,             \\'explaintext\\': True,         }     ).json()     page = next(iter(response[\\'query\\'][\\'pages\\'].values()))     wiki_text = page[\\'extract\\']      data_path = Path(\\'data\\')     if not data_path.exists():         Path.mkdir(data_path)      with open(data_path / f\\\\\"{title}.txt\\\\\", \\'w\\') as fp:         fp.write(wiki_text)  ```  The next snippet loads all files into Document objects.  ```python\", \"Load all wiki documents city_docs = {} for wiki_title in wiki_titles:     city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\\\\\"data/{wiki_title}.txt\\\\\"]).load_data()  ```\", \"Defining the Set of Indexes  We will now define a set of indexes and graphs over our data. You can think of each index/graph as a lightweight structure that solves a distinct use case.  We will first define a vector index over the documents of each city.  ```python from llama_index import VectorStoreIndex, ServiceContext, StorageContext from llama_index.llms import OpenAI\", \"set service context llm_gpt4 = OpenAI(temperature=0, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(     llm=llm_gpt4, chunk_size=1024 )\", \"Build city document index vector_indices = {} for wiki_title in wiki_titles:     storage_context = StorageContext.from_defaults()     # build vector index     vector_indices[wiki_title] = VectorStoreIndex.from_documents(         city_docs[wiki_title],         service_context=service_context,         storage_context=storage_context,     )     # set id for vector index     vector_indices[wiki_title].index_struct.index_id = wiki_title     # persist to disk     storage_context.persist(persist_dir=f\\'./storage/{wiki_title}\\') ```  Querying a vector index lets us easily perform semantic search over a given city\\'s documents.  ```python response = vector_indices[\\\\\"Toronto\\\\\"].as_query_engine().query(\\\\\"What are the sports teams in Toronto?\\\\\") print(str(response))  ```  Example response:  ```text The sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL). ```\", \"Defining a Graph for Compare/Contrast Queries  We will now define a composed graph in order to run **compare/contrast** queries (see use cases doc). This graph contains a keyword table composed on top of existing vector indexes.  To do this, we first want to set the \\\\\"summary text\\\\\" for each vector index.  ```python index_summaries = {} for wiki_title in wiki_titles:     # set summary text for city     index_summaries[wiki_title] = (         f\\\\\"This content contains Wikipedia articles about {wiki_title}. \\\\\"         f\\\\\"Use this index if you need to lookup specific facts about {wiki_title}.\\\\\\\\n\\\\\"         \\\\\"Do not use this index if you want to analyze multiple cities.\\\\\"     ) ```  Next, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.  ```python from llama_index.indices.composability import ComposableGraph  graph = ComposableGraph.from_indices(     SimpleKeywordTableIndex,     [index for _, index in vector_indices.items()],     [summary for _, summary in index_summaries.items()],     max_keywords_per_chunk=50 )\", \"get root index root_index = graph.get_index(graph.index_struct.root_id, SimpleKeywordTableIndex)\", \"set id of root index root_index.set_index_id(\\\\\"compare_contrast\\\\\") root_summary = (     \\\\\"This index contains Wikipedia articles about multiple cities. \\\\\"     \\\\\"Use this index if you want to compare multiple cities. \\\\\" )  ```  Querying this graph (with a query transform module), allows us to easily compare/contrast between different cities. An example is shown below.  ```python\", \"define decompose_transform from llama_index import LLMPredictor from llama_index.indices.query.query_transform.base import DecomposeQueryTransform  decompose_transform = DecomposeQueryTransform(     LLMPredictor(llm=llm_gpt4), verbose=True )\", \"define custom query engines from llama_index.query_engine.transform_query_engine import TransformQueryEngine custom_query_engines = {} for index in vector_indices.values():     query_engine = index.as_query_engine(service_context=service_context)     query_engine = TransformQueryEngine(         query_engine,         query_transform=decompose_transform,         transform_extra_info={\\'index_summary\\': index.index_struct.summary},     )     custom_query_engines[index.index_id] = query_engine custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(     retriever_mode=\\'simple\\',     response_mode=\\'tree_summarize\\',     service_context=service_context, )\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Setup  In this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.  The below code snippet downloads the relevant data into files.  ```python  from pathlib import Path import requests  wiki_titles = [\\\\\"Toronto\\\\\", \\\\\"Seattle\\\\\", \\\\\"Chicago\\\\\", \\\\\"Boston\\\\\", \\\\\"Houston\\\\\"]  for title in wiki_titles:     response = requests.get(         \\'https://en.wikipedia.org/w/api.php\\',         params={             \\'action\\': \\'query\\',             \\'format\\': \\'json\\',             \\'titles\\': title,             \\'prop\\': \\'extracts\\',             # \\'exintro\\': True,             \\'explaintext\\': True,         }     ).json()     page = next(iter(response[\\'query\\'][\\'pages\\'].values()))     wiki_text = page[\\'extract\\']      data_path = Path(\\'data\\')     if not data_path.exists():         Path.mkdir(data_path)      with open(data_path / f\\\\\"{title}.txt\\\\\", \\'w\\') as fp:         fp.write(wiki_text)  ```  The next snippet loads all files into Document objects.  ```python\", \"Load all wiki documents city_docs = {} for wiki_title in wiki_titles:     city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\\\\\"data/{wiki_title}.txt\\\\\"]).load_data()  ```\", \"Defining the Set of Indexes  We will now define a set of indexes and graphs over our data. You can think of each index/graph as a lightweight structure that solves a distinct use case.  We will first define a vector index over the documents of each city.  ```python from llama_index import VectorStoreIndex, ServiceContext, StorageContext from llama_index.llms import OpenAI\", \"set service context llm_gpt4 = OpenAI(temperature=0, model=\\\\\"gpt-4\\\\\") service_context = ServiceContext.from_defaults(     llm=llm_gpt4, chunk_size=1024 )\", \"Build city document index vector_indices = {} for wiki_title in wiki_titles:     storage_context = StorageContext.from_defaults()     # build vector index     vector_indices[wiki_title] = VectorStoreIndex.from_documents(         city_docs[wiki_title],         service_context=service_context,         storage_context=storage_context,     )     # set id for vector index     vector_indices[wiki_title].index_struct.index_id = wiki_title     # persist to disk     storage_context.persist(persist_dir=f\\'./storage/{wiki_title}\\') ```  Querying a vector index lets us easily perform semantic search over a given city\\'s documents.  ```python response = vector_indices[\\\\\"Toronto\\\\\"].as_query_engine().query(\\\\\"What are the sports teams in Toronto?\\\\\") print(str(response))  ```  Example response:  ```text The sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL). ```\", \"Defining a Graph for Compare/Contrast Queries  We will now define a composed graph in order to run **compare/contrast** queries (see use cases doc). This graph contains a keyword table composed on top of existing vector indexes.  To do this, we first want to set the \\\\\"summary text\\\\\" for each vector index.  ```python index_summaries = {} for wiki_title in wiki_titles:     # set summary text for city     index_summaries[wiki_title] = (         f\\\\\"This content contains Wikipedia articles about {wiki_title}. \\\\\"         f\\\\\"Use this index if you need to lookup specific facts about {wiki_title}.\\\\\\\\n\\\\\"         \\\\\"Do not use this index if you want to analyze multiple cities.\\\\\"     ) ```  Next, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.  ```python from llama_index.indices.composability import ComposableGraph  graph = ComposableGraph.from_indices(     SimpleKeywordTableIndex,     [index for _, index in vector_indices.items()],     [summary for _, summary in index_summaries.items()],     max_keywords_per_chunk=50 )\", \"get root index root_index = graph.get_index(graph.index_struct.root_id, SimpleKeywordTableIndex)\", \"set id of root index root_index.set_index_id(\\\\\"compare_contrast\\\\\") root_summary = (     \\\\\"This index contains Wikipedia articles about multiple cities. \\\\\"     \\\\\"Use this index if you want to compare multiple cities. \\\\\" )  ```  Querying this graph (with a query transform module), allows us to easily compare/contrast between different cities. An example is shown below.  ```python\", \"define decompose_transform from llama_index import LLMPredictor from llama_index.indices.query.query_transform.base import DecomposeQueryTransform  decompose_transform = DecomposeQueryTransform(     LLMPredictor(llm=llm_gpt4), verbose=True )\", \"define custom query engines from llama_index.query_engine.transform_query_engine import TransformQueryEngine custom_query_engines = {} for index in vector_indices.values():     query_engine = index.as_query_engine(service_context=service_context)     query_engine = TransformQueryEngine(         query_engine,         query_transform=decompose_transform,         transform_extra_info={\\'index_summary\\': index.index_struct.summary},     )     custom_query_engines[index.index_id] = query_engine custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(     retriever_mode=\\'simple\\',     response_mode=\\'tree_summarize\\',     service_context=service_context, )\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=102 request_id=dfd0c57920003889847830f2950e9c36 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=102 request_id=dfd0c57920003889847830f2950e9c36 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"define query engine query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\", \"query the graph query_str = (     \\\\\"Compare and contrast the arts and culture of Houston and Boston. \\\\\" ) response_chatgpt = query_engine.query(query_str) ```\", \"Defining the Unified Query Interface  Now that we\\'ve defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface to our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph will be used for the job.  There are a few ways to do this, both within our framework as well as outside of it!  - Build a **router query engine** on top of your existing indexes/graphs - Define each index/graph as a Tool within an agent framework (e.g. LangChain).  For the purposes of this tutorial, we follow the former approach. If you want to take a look at how the latter approach works, take a look at our example tutorial here.  Let\\'s take a look at an example of building a router query engine to automatically \\\\\"route\\\\\" any query to the set of indexes/graphs that you have define under the hood.  First, we define the query engines for the set of indexes/graph that we want to route our query to. We also give each a description (about what data it holds and what it\\'s useful for) to help the router choose between them depending on the specific query.  ```python from llama_index.tools.query_engine import QueryEngineTool  query_engine_tools = []\", \"add vector index tools for wiki_title in wiki_titles:     index = vector_indices[wiki_title]     summary = index_summaries[wiki_title]      query_engine = index.as_query_engine(service_context=service_context)     vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)     query_engine_tools.append(vector_tool)\", \"add graph tool graph_description = (     \\\\\"This tool contains Wikipedia articles about multiple cities. \\\\\"     \\\\\"Use this tool if you want to compare multiple cities. \\\\\" ) graph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description) query_engine_tools.append(graph_tool) ```  Now, we can define the routing logic and overall router query engine. Here, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.llm_selectors import LLMSingleSelector   router_query_engine = RouterQueryEngine(     selector=LLMSingleSelector.from_defaults(service_context=service_context),     query_engine_tools=query_engine_tools ) ```\", \"Querying our Unified Interface  The advantage of a unified query interface is that it can now handle different types of queries.  It can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.  Let\\'s take a look at a few examples!  **Asking a Compare/Contrast Question**  ```python\", \"ask a compare/contrast question response = router_query_engine.query(     \\\\\"Compare and contrast the arts and culture of Houston and Boston.\\\\\", ) print(str(response) ```  **Asking Questions about specific Cities**  ```python  response = router_query_engine.query(\\\\\"What are the sports teams in Toronto?\\\\\") print(str(response))  ```  This \\\\\"outer\\\\\" abstraction is able to handle different queries by routing to the right underlying abstractions.\", \"Q&A over Documents  At a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case, whether it\\'s question-answering, summarization, or a component in a chatbot.  This section describes the different ways you can query your data with LlamaIndex, roughly in order of simplest (top-k semantic search), to more advanced capabilities.\", \"Semantic Search  The most basic example usage of LlamaIndex is through semantic search. We provide a simple in-memory vector store for you to get started, but you can also choose to use any one of our vector store integrations:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response)  ```  **Tutorials** - Starter Tutorial - Basic Usage Pattern  **Guides** - Example (Notebook)\", \"Summarization  A summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer. For instance, a summarization query could look like one of the following:  - \\\\\"What is a summary of this collection of text?\\\\\" - \\\\\"Give me a summary of person X\\'s experience with the company.\\\\\"  In general, a list index would be suited for this use case. A list index by default goes through all the data.  Empirically, setting `response_mode=\\\\\"tree_summarize\\\\\"` also leads to better summarization results.  ```python index = ListIndex.from_documents(documents)  query_engine = index.as_query_engine(     response_mode=\\\\\"tree_summarize\\\\\" ) response = query_engine.query(\\\\\"\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"define query engine query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\", \"query the graph query_str = (     \\\\\"Compare and contrast the arts and culture of Houston and Boston. \\\\\" ) response_chatgpt = query_engine.query(query_str) ```\", \"Defining the Unified Query Interface  Now that we\\'ve defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface to our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph will be used for the job.  There are a few ways to do this, both within our framework as well as outside of it!  - Build a **router query engine** on top of your existing indexes/graphs - Define each index/graph as a Tool within an agent framework (e.g. LangChain).  For the purposes of this tutorial, we follow the former approach. If you want to take a look at how the latter approach works, take a look at our example tutorial here.  Let\\'s take a look at an example of building a router query engine to automatically \\\\\"route\\\\\" any query to the set of indexes/graphs that you have define under the hood.  First, we define the query engines for the set of indexes/graph that we want to route our query to. We also give each a description (about what data it holds and what it\\'s useful for) to help the router choose between them depending on the specific query.  ```python from llama_index.tools.query_engine import QueryEngineTool  query_engine_tools = []\", \"add vector index tools for wiki_title in wiki_titles:     index = vector_indices[wiki_title]     summary = index_summaries[wiki_title]      query_engine = index.as_query_engine(service_context=service_context)     vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)     query_engine_tools.append(vector_tool)\", \"add graph tool graph_description = (     \\\\\"This tool contains Wikipedia articles about multiple cities. \\\\\"     \\\\\"Use this tool if you want to compare multiple cities. \\\\\" ) graph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description) query_engine_tools.append(graph_tool) ```  Now, we can define the routing logic and overall router query engine. Here, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.  ```python from llama_index.query_engine.router_query_engine import RouterQueryEngine from llama_index.selectors.llm_selectors import LLMSingleSelector   router_query_engine = RouterQueryEngine(     selector=LLMSingleSelector.from_defaults(service_context=service_context),     query_engine_tools=query_engine_tools ) ```\", \"Querying our Unified Interface  The advantage of a unified query interface is that it can now handle different types of queries.  It can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.  Let\\'s take a look at a few examples!  **Asking a Compare/Contrast Question**  ```python\", \"ask a compare/contrast question response = router_query_engine.query(     \\\\\"Compare and contrast the arts and culture of Houston and Boston.\\\\\", ) print(str(response) ```  **Asking Questions about specific Cities**  ```python  response = router_query_engine.query(\\\\\"What are the sports teams in Toronto?\\\\\") print(str(response))  ```  This \\\\\"outer\\\\\" abstraction is able to handle different queries by routing to the right underlying abstractions.\", \"Q&A over Documents  At a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case, whether it\\'s question-answering, summarization, or a component in a chatbot.  This section describes the different ways you can query your data with LlamaIndex, roughly in order of simplest (top-k semantic search), to more advanced capabilities.\", \"Semantic Search  The most basic example usage of LlamaIndex is through semantic search. We provide a simple in-memory vector store for you to get started, but you can also choose to use any one of our vector store integrations:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response)  ```  **Tutorials** - Starter Tutorial - Basic Usage Pattern  **Guides** - Example (Notebook)\", \"Summarization  A summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer. For instance, a summarization query could look like one of the following:  - \\\\\"What is a summary of this collection of text?\\\\\" - \\\\\"Give me a summary of person X\\'s experience with the company.\\\\\"  In general, a list index would be suited for this use case. A list index by default goes through all the data.  Empirically, setting `response_mode=\\\\\"tree_summarize\\\\\"` also leads to better summarization results.  ```python index = ListIndex.from_documents(documents)  query_engine = index.as_query_engine(     response_mode=\\\\\"tree_summarize\\\\\" ) response = query_engine.query(\\\\\"\\\\\") ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=b961c8cea602d89984a36426cde759ca response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=b961c8cea602d89984a36426cde759ca response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Queries over Structured Data  LlamaIndex supports queries over structured data, whether that\\'s a Pandas DataFrame or a SQL Database.  Here are some relevant resources:  **Tutorials**  - Guide on Text-to-SQL  **Guides** - SQL Guide (Core) (Notebook) - Pandas Demo (Notebook)\", \"Synthesis over Heterogeneous Data  LlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data. Specifically, compose a list index over your subindices. A list index inherently combines information for each node; therefore it can synthesize information across your heterogeneous data sources.  ```python from llama_index import VectorStoreIndex, ListIndex from llama_index.indices.composability import ComposableGraph  index1 = VectorStoreIndex.from_documents(notion_docs) index2 = VectorStoreIndex.from_documents(slack_docs)  graph = ComposableGraph.from_indices(ListIndex, [index1, index2], index_summaries=[\\\\\"summary1\\\\\", \\\\\"summary2\\\\\"]) query_engine = graph.as_query_engine() response = query_engine.query(\\\\\"\\\\\")  ```  **Guides** - Composability - City Analysis (Notebook)\", \"Routing over Heterogeneous Data  LlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \\\\\"route\\\\\" a query to an  underlying Document or a sub-index.   To do this, first build the sub-indices over different data sources. Then construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.  ```python from llama_index import TreeIndex, VectorStoreIndex from llama_index.tools import QueryEngineTool  ...\", \"define sub-indices index1 = VectorStoreIndex.from_documents(notion_docs) index2 = VectorStoreIndex.from_documents(slack_docs)\", \"define query engines and tools tool1 = QueryEngineTool.from_defaults(     query_engine=index1.as_query_engine(),      description=\\\\\"Use this query engine to do...\\\\\", ) tool2 = QueryEngineTool.from_defaults(     query_engine=index2.as_query_engine(),      description=\\\\\"Use this query engine for something else...\\\\\", ) ```  Then, we define a `RouterQueryEngine` over them. By default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.  ```python from llama_index.query_engine import RouterQueryEngine  query_engine = RouterQueryEngine.from_defaults(     query_engine_tools=[tool1, tool2] )  response = query_engine.query(     \\\\\"In Notion, give me a summary of the product roadmap.\\\\\" )  ```  **Guides** - Router Query Engine Guide (Notebook) - City Analysis Unified Query Interface (Notebook)\", \"Compare/Contrast Queries You can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.  ```python from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     service_context.llm_predictor, verbose=True ) ```  This module will help break down a complex query into a simpler one over your existing index structure.  **Guides** - Query Transformations - City Analysis Compare/Contrast Example (Notebook)  You can also rely on the LLM to *infer* whether to perform compare/contrast queries (see Multi-Document Queries below).\", \"Multi-Document Queries  Besides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well.  It can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \\\\\"query plan\\\\\" containing sub-queries against sub-documents before synthesizing the final answer.  To do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):  ```python from llama_index.tools import QueryEngineTool, ToolMetadata  query_engine_tools = [     QueryEngineTool(         query_engine=sept_engine,          metadata=ToolMetadata(name=\\'sept_22\\', description=\\'Provides information about Uber quarterly financials ending September 2022\\')     ),     QueryEngineTool(         query_engine=june_engine,          metadata=ToolMetadata(name=\\'june_22\\', description=\\'Provides information about Uber quarterly financials ending June 2022\\')     ),     QueryEngineTool(         query_engine=march_engine,          metadata=ToolMetadata(name=\\'march_22\\', description=\\'Provides information about Uber quarterly financials ending March 2022\\')     ), ] ```  Then, we define a `SubQuestionQueryEngine` over these tools:  ```python from llama_index.query_engine import SubQuestionQueryEngine  query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)  ```  This query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer. This makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.  **Guides** - Sub Question Query Engine (Intro) - 10Q Analysis (Uber) - 10K Analysis (Uber and Lyft)\", \"Multi-Step Queries  LlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions, and sequentially generate subquestions based on returned answers until the final answer is returned.  For instance, given a question \\\\\"Who was in the first batch of the accelerator program the author started?\\\\\", the module will first decompose the query into a simpler initial question \\\\\"What was the accelerator program the author started?\\\\\", query the index, and then ask followup questions.  **Guides** - Query Transformations - Multi-Step Query Decomposition (Notebook)\", \"Temporal Queries  LlamaIndex can support queries that require an understanding of time. It can do this in two ways: - Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question. - Sort by recency and filter outdated context.  **Guides** - Second-Stage Postprocessing Guide - Prev/Next Postprocessing - Recency Postprocessing\", \"Additional Resources - A Guide to Creating a Unified Query Framework over your ndexes - A Guide to Extracting Terms and Definitions - SEC 10k Analysis\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Queries over Structured Data  LlamaIndex supports queries over structured data, whether that\\'s a Pandas DataFrame or a SQL Database.  Here are some relevant resources:  **Tutorials**  - Guide on Text-to-SQL  **Guides** - SQL Guide (Core) (Notebook) - Pandas Demo (Notebook)\", \"Synthesis over Heterogeneous Data  LlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data. Specifically, compose a list index over your subindices. A list index inherently combines information for each node; therefore it can synthesize information across your heterogeneous data sources.  ```python from llama_index import VectorStoreIndex, ListIndex from llama_index.indices.composability import ComposableGraph  index1 = VectorStoreIndex.from_documents(notion_docs) index2 = VectorStoreIndex.from_documents(slack_docs)  graph = ComposableGraph.from_indices(ListIndex, [index1, index2], index_summaries=[\\\\\"summary1\\\\\", \\\\\"summary2\\\\\"]) query_engine = graph.as_query_engine() response = query_engine.query(\\\\\"\\\\\")  ```  **Guides** - Composability - City Analysis (Notebook)\", \"Routing over Heterogeneous Data  LlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \\\\\"route\\\\\" a query to an  underlying Document or a sub-index.   To do this, first build the sub-indices over different data sources. Then construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.  ```python from llama_index import TreeIndex, VectorStoreIndex from llama_index.tools import QueryEngineTool  ...\", \"define sub-indices index1 = VectorStoreIndex.from_documents(notion_docs) index2 = VectorStoreIndex.from_documents(slack_docs)\", \"define query engines and tools tool1 = QueryEngineTool.from_defaults(     query_engine=index1.as_query_engine(),      description=\\\\\"Use this query engine to do...\\\\\", ) tool2 = QueryEngineTool.from_defaults(     query_engine=index2.as_query_engine(),      description=\\\\\"Use this query engine for something else...\\\\\", ) ```  Then, we define a `RouterQueryEngine` over them. By default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.  ```python from llama_index.query_engine import RouterQueryEngine  query_engine = RouterQueryEngine.from_defaults(     query_engine_tools=[tool1, tool2] )  response = query_engine.query(     \\\\\"In Notion, give me a summary of the product roadmap.\\\\\" )  ```  **Guides** - Router Query Engine Guide (Notebook) - City Analysis Unified Query Interface (Notebook)\", \"Compare/Contrast Queries You can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.  ```python from llama_index.indices.query.query_transform.base import DecomposeQueryTransform decompose_transform = DecomposeQueryTransform(     service_context.llm_predictor, verbose=True ) ```  This module will help break down a complex query into a simpler one over your existing index structure.  **Guides** - Query Transformations - City Analysis Compare/Contrast Example (Notebook)  You can also rely on the LLM to *infer* whether to perform compare/contrast queries (see Multi-Document Queries below).\", \"Multi-Document Queries  Besides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well.  It can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \\\\\"query plan\\\\\" containing sub-queries against sub-documents before synthesizing the final answer.  To do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):  ```python from llama_index.tools import QueryEngineTool, ToolMetadata  query_engine_tools = [     QueryEngineTool(         query_engine=sept_engine,          metadata=ToolMetadata(name=\\'sept_22\\', description=\\'Provides information about Uber quarterly financials ending September 2022\\')     ),     QueryEngineTool(         query_engine=june_engine,          metadata=ToolMetadata(name=\\'june_22\\', description=\\'Provides information about Uber quarterly financials ending June 2022\\')     ),     QueryEngineTool(         query_engine=march_engine,          metadata=ToolMetadata(name=\\'march_22\\', description=\\'Provides information about Uber quarterly financials ending March 2022\\')     ), ] ```  Then, we define a `SubQuestionQueryEngine` over these tools:  ```python from llama_index.query_engine import SubQuestionQueryEngine  query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)  ```  This query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer. This makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.  **Guides** - Sub Question Query Engine (Intro) - 10Q Analysis (Uber) - 10K Analysis (Uber and Lyft)\", \"Multi-Step Queries  LlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions, and sequentially generate subquestions based on returned answers until the final answer is returned.  For instance, given a question \\\\\"Who was in the first batch of the accelerator program the author started?\\\\\", the module will first decompose the query into a simpler initial question \\\\\"What was the accelerator program the author started?\\\\\", query the index, and then ask followup questions.  **Guides** - Query Transformations - Multi-Step Query Decomposition (Notebook)\", \"Temporal Queries  LlamaIndex can support queries that require an understanding of time. It can do this in two ways: - Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question. - Sort by recency and filter outdated context.  **Guides** - Second-Stage Postprocessing Guide - Prev/Next Postprocessing - Recency Postprocessing\", \"Additional Resources - A Guide to Creating a Unified Query Framework over your ndexes - A Guide to Extracting Terms and Definitions - SEC 10k Analysis\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=88f3deba9d0dac55eeee7402bf523dc9 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=134 request_id=88f3deba9d0dac55eeee7402bf523dc9 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"A Guide to LlamaIndex + Structured Data  A lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse. LlamaIndex provides a lot of advanced features, powered by LLM\\'s, to both create structured data from unstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.  This guide helps walk through each of these capabilities. Specifically, we cover the following topics: - **Setup**: Defining up our example SQL Table. - **Building our Table Index**: How to go from sql database to a Table Schema Index - **Using natural language SQL queries**: How to query our SQL database using natural language.  We will walk through a toy example table which contains city/population/country information. A notebook for this tutorial is available here.\", \"Setup  First, we use SQLAlchemy to setup a simple sqlite db: ```python from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column  engine = create_engine(\\\\\"sqlite:///:memory:\\\\\") metadata_obj = MetaData() ```  We then create a toy `city_stats` table: ```python\", \"create city SQL table table_name = \\\\\"city_stats\\\\\" city_stats_table = Table(     table_name,     metadata_obj,     Column(\\\\\"city_name\\\\\", String(16), primary_key=True),     Column(\\\\\"population\\\\\", Integer),     Column(\\\\\"country\\\\\", String(16), nullable=False), ) metadata_obj.create_all(engine) ```  Now it\\'s time to insert some datapoints!  If you want to look into filling into this table by inferring structured datapoints from unstructured data, take a look at the below section. Otherwise, you can choose to directly populate this table:  ```python from sqlalchemy import insert rows = [     {\\\\\"city_name\\\\\": \\\\\"Toronto\\\\\", \\\\\"population\\\\\": 2731571, \\\\\"country\\\\\": \\\\\"Canada\\\\\"},     {\\\\\"city_name\\\\\": \\\\\"Tokyo\\\\\", \\\\\"population\\\\\": 13929286, \\\\\"country\\\\\": \\\\\"Japan\\\\\"},     {\\\\\"city_name\\\\\": \\\\\"Berlin\\\\\", \\\\\"population\\\\\": 600000, \\\\\"country\\\\\": \\\\\"Germany\\\\\"}, ] for row in rows:     stmt = insert(city_stats_table).values(**row)     with engine.connect() as connection:         cursor = connection.execute(stmt) ```  Finally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper; this allows the db to be used within LlamaIndex:  ```python from llama_index import SQLDatabase  sql_database = SQLDatabase(engine, include_tables=[\\\\\"city_stats\\\\\"]) ```\", \"Natural language SQL Once we have constructed our SQL database, we can use the NLSQLTableQueryEngine to construct natural language queries that are synthesized into SQL queries.  Note that we need to specify the tables we want to use with this query engine. If we don\\'t the query engine will pull all the schema context, which could overflow the context window of the LLM.  ```python query_engine = NLSQLTableQueryEngine(     sql_database=sql_database,     tables=[\\\\\"city_stats\\\\\"], ) query_str = (     \\\\\"Which city has the highest population?\\\\\" ) response = query_engine.query(query_str) ```  This query engine should used in any case where you can specify the tables you want to query over beforehand, or the total size of all the table schema plus the rest of the prompt fits your context window.\", \"Building our Table Index If we don\\'t know ahead of time which table we would like to use, and the total size of the table schema overflows your context window size, we should store the table schema  in an index so that during query time we can retrieve the right schema.  The way we can do this is using the SQLTableNodeMapping object, which takes in a  SQLDatabase and produces a Node object for each SQLTableSchema object passed  into the ObjectIndex constructor.  ```python table_node_mapping = SQLTableNodeMapping(sql_database) table_schema_objs = [(SQLTableSchema(table_name=\\\\\"city_stats\\\\\")), ...] # one SQLTableSchema for each table  obj_index = ObjectIndex.from_objects(     table_schema_objs,     table_node_mapping,     VectorStoreIndex, ) ```  Here you can see we define our table_node_mapping, and a single SQLTableSchema with the \\\\\"city_stats\\\\\" table name. We pass these into the ObjectIndex constructor, along with the VectorStoreIndex class definition we want to use. This will give us a VectorStoreIndex where each Node contains table schema and other context information. You can also add any additional context information you\\'d like.  ```python\", \"manually set extra context text city_stats_text = (     \\\\\"This table gives information regarding the population and country of a given city.\\\\\\\\n\\\\\"     \\\\\"The user will query with codewords, where \\'foo\\' corresponds to population and \\'bar\\'\\\\\"     \\\\\"corresponds to city.\\\\\" )  table_node_mapping = SQLTableNodeMapping(sql_database) table_schema_objs = [(SQLTableSchema(table_name=\\\\\"city_stats\\\\\", context_str=city_stats_text))] ```\", \"Using natural language SQL queries Once we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine by passing in our SQLDatabase, and a retriever constructed from our object index.  ```python query_engine = SQLTableRetrieverQueryEngine(     sql_database, obj_index.as_retriever(similarity_top_k=1) ) response = query_engine.query(\\\\\"Which city has the highest population?\\\\\") print(response) ``` Now when we query the retriever query engine, it will retrieve the relevant table schema and synthesize a SQL query and a response from the results of that query.\", \"Concluding Thoughts  This is it for now! We\\'re constantly looking for ways to improve our structured data support. If you have any questions let us know in our Discord.\", \"Structured Data  Relevant Resources: - A Guide to LlamaIndex + Structured Data - Airbyte SQL Index Guide\", \"Basic Usage Pattern  The general usage pattern of LlamaIndex is as follows:  1. Load in documents (either manually, or through a data loader) 2. Parse the Documents into Nodes 3. Construct Index (from Nodes or Documents) 4. [Optional, Advanced] Building indices on top of other indices 5. Query the index\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"A Guide to LlamaIndex + Structured Data  A lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse. LlamaIndex provides a lot of advanced features, powered by LLM\\'s, to both create structured data from unstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.  This guide helps walk through each of these capabilities. Specifically, we cover the following topics: - **Setup**: Defining up our example SQL Table. - **Building our Table Index**: How to go from sql database to a Table Schema Index - **Using natural language SQL queries**: How to query our SQL database using natural language.  We will walk through a toy example table which contains city/population/country information. A notebook for this tutorial is available here.\", \"Setup  First, we use SQLAlchemy to setup a simple sqlite db: ```python from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column  engine = create_engine(\\\\\"sqlite:///:memory:\\\\\") metadata_obj = MetaData() ```  We then create a toy `city_stats` table: ```python\", \"create city SQL table table_name = \\\\\"city_stats\\\\\" city_stats_table = Table(     table_name,     metadata_obj,     Column(\\\\\"city_name\\\\\", String(16), primary_key=True),     Column(\\\\\"population\\\\\", Integer),     Column(\\\\\"country\\\\\", String(16), nullable=False), ) metadata_obj.create_all(engine) ```  Now it\\'s time to insert some datapoints!  If you want to look into filling into this table by inferring structured datapoints from unstructured data, take a look at the below section. Otherwise, you can choose to directly populate this table:  ```python from sqlalchemy import insert rows = [     {\\\\\"city_name\\\\\": \\\\\"Toronto\\\\\", \\\\\"population\\\\\": 2731571, \\\\\"country\\\\\": \\\\\"Canada\\\\\"},     {\\\\\"city_name\\\\\": \\\\\"Tokyo\\\\\", \\\\\"population\\\\\": 13929286, \\\\\"country\\\\\": \\\\\"Japan\\\\\"},     {\\\\\"city_name\\\\\": \\\\\"Berlin\\\\\", \\\\\"population\\\\\": 600000, \\\\\"country\\\\\": \\\\\"Germany\\\\\"}, ] for row in rows:     stmt = insert(city_stats_table).values(**row)     with engine.connect() as connection:         cursor = connection.execute(stmt) ```  Finally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper; this allows the db to be used within LlamaIndex:  ```python from llama_index import SQLDatabase  sql_database = SQLDatabase(engine, include_tables=[\\\\\"city_stats\\\\\"]) ```\", \"Natural language SQL Once we have constructed our SQL database, we can use the NLSQLTableQueryEngine to construct natural language queries that are synthesized into SQL queries.  Note that we need to specify the tables we want to use with this query engine. If we don\\'t the query engine will pull all the schema context, which could overflow the context window of the LLM.  ```python query_engine = NLSQLTableQueryEngine(     sql_database=sql_database,     tables=[\\\\\"city_stats\\\\\"], ) query_str = (     \\\\\"Which city has the highest population?\\\\\" ) response = query_engine.query(query_str) ```  This query engine should used in any case where you can specify the tables you want to query over beforehand, or the total size of all the table schema plus the rest of the prompt fits your context window.\", \"Building our Table Index If we don\\'t know ahead of time which table we would like to use, and the total size of the table schema overflows your context window size, we should store the table schema  in an index so that during query time we can retrieve the right schema.  The way we can do this is using the SQLTableNodeMapping object, which takes in a  SQLDatabase and produces a Node object for each SQLTableSchema object passed  into the ObjectIndex constructor.  ```python table_node_mapping = SQLTableNodeMapping(sql_database) table_schema_objs = [(SQLTableSchema(table_name=\\\\\"city_stats\\\\\")), ...] # one SQLTableSchema for each table  obj_index = ObjectIndex.from_objects(     table_schema_objs,     table_node_mapping,     VectorStoreIndex, ) ```  Here you can see we define our table_node_mapping, and a single SQLTableSchema with the \\\\\"city_stats\\\\\" table name. We pass these into the ObjectIndex constructor, along with the VectorStoreIndex class definition we want to use. This will give us a VectorStoreIndex where each Node contains table schema and other context information. You can also add any additional context information you\\'d like.  ```python\", \"manually set extra context text city_stats_text = (     \\\\\"This table gives information regarding the population and country of a given city.\\\\\\\\n\\\\\"     \\\\\"The user will query with codewords, where \\'foo\\' corresponds to population and \\'bar\\'\\\\\"     \\\\\"corresponds to city.\\\\\" )  table_node_mapping = SQLTableNodeMapping(sql_database) table_schema_objs = [(SQLTableSchema(table_name=\\\\\"city_stats\\\\\", context_str=city_stats_text))] ```\", \"Using natural language SQL queries Once we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine by passing in our SQLDatabase, and a retriever constructed from our object index.  ```python query_engine = SQLTableRetrieverQueryEngine(     sql_database, obj_index.as_retriever(similarity_top_k=1) ) response = query_engine.query(\\\\\"Which city has the highest population?\\\\\") print(response) ``` Now when we query the retriever query engine, it will retrieve the relevant table schema and synthesize a SQL query and a response from the results of that query.\", \"Concluding Thoughts  This is it for now! We\\'re constantly looking for ways to improve our structured data support. If you have any questions let us know in our Discord.\", \"Structured Data  Relevant Resources: - A Guide to LlamaIndex + Structured Data - Airbyte SQL Index Guide\", \"Basic Usage Pattern  The general usage pattern of LlamaIndex is as follows:  1. Load in documents (either manually, or through a data loader) 2. Parse the Documents into Nodes 3. Construct Index (from Nodes or Documents) 4. [Optional, Advanced] Building indices on top of other indices 5. Query the index\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1191 request_id=d7fbb167b7a0f9afb09b1eb733ef72eb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=1191 request_id=d7fbb167b7a0f9afb09b1eb733ef72eb response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"1. Load in Documents  The first step is to load in data. This data is represented in the form of `Document` objects. We provide a variety of data loaders which will load in Documents through the `load_data` function, e.g.:  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'./data\\').load_data() ```  You can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.  ```python from llama_index import Document  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list] ```  A Document represents a lightweight container around the data source. You can now choose to proceed with one of the following steps:  1. Feed the Document object directly into the index (see section 3). 2. First convert the Document into Node objects (see section 2).\", \"2. Parse the Documents into Nodes  The next step is to parse these Document objects into Node objects. Nodes represent \\\\\"chunks\\\\\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes.  For instance, you can do  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults()  nodes = parser.get_nodes_from_documents(documents) ```  You can also choose to construct Node objects manually and skip the first section. For instance,  ```python from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo  node1 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\") node2 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\")\", \"set relationships node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id) node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id) nodes = [node1, node2] ```  The `RelatedNodeInfo` class can also store additional `metadata` if needed:  ```python node2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\\\\\"key\\\\\": \\\\\"val\\\\\"}) ```\", \"3. Index Construction  We can now build an index over these Document objects. The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).  `from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(documents) ```  You can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex(nodes) ```  Depending on which index you use, LlamaIndex may make LLM calls in order to build the index.\", \"Reusing Nodes across Index Structures  If you have multiple Node objects defined, and wish to share these Node objects across multiple index structures, you can do that. Simply instantiate a StorageContext object, add the Node objects to the underlying DocumentStore, and pass the StorageContext around.  ```python from llama_index import StorageContext  storage_context = StorageContext.from_defaults() storage_context.docstore.add_documents(nodes)  index1 = VectorStoreIndex(nodes, storage_context=storage_context) index2 = ListIndex(nodes, storage_context=storage_context) ```  **NOTE**: If the `storage_context` argument isn\\'t specified, then it is implicitly created for each index during index construction. You can access the docstore associated with a given index through `index.storage_context`.\", \"Inserting Documents or Nodes  You can also take advantage of the `insert` capability of indices to insert Document objects one at a time instead of during index construction.  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex([]) for doc in documents:     index.insert(doc) ```  If you want to insert nodes on directly you can use `insert_nodes` function instead.  ```python from llama_index import VectorStoreIndex\", \"nodes: Sequence[Node] index = VectorStoreIndex([]) index.insert_nodes(nodes) ```  See the Document Management How-To for more details on managing documents and an example notebook.\", \"Customizing Documents  When creating documents, you can also attach useful metadata. Any metadata added to a document will be copied to the nodes that get created from their respective source document.  ```python document = Document(     text=\\'text\\',     metadata={         \\'filename\\': \\'\\',         \\'category\\': \\'\\'     } ) ```  More information and approaches to this are discussed in the section Customizing Documents.\", \"Customizing LLM\\'s  By default, we use OpenAI\\'s `text-davinci-003` model. You may choose to use another LLM when constructing an index.  ```python from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context from llama_index.llms import OpenAI  ...\", \"define LLM llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0, max_tokens=256)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"1. Load in Documents  The first step is to load in data. This data is represented in the form of `Document` objects. We provide a variety of data loaders which will load in Documents through the `load_data` function, e.g.:  ```python from llama_index import SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'./data\\').load_data() ```  You can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.  ```python from llama_index import Document  text_list = [text1, text2, ...] documents = [Document(text=t) for t in text_list] ```  A Document represents a lightweight container around the data source. You can now choose to proceed with one of the following steps:  1. Feed the Document object directly into the index (see section 3). 2. First convert the Document into Node objects (see section 2).\", \"2. Parse the Documents into Nodes  The next step is to parse these Document objects into Node objects. Nodes represent \\\\\"chunks\\\\\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information with other nodes and index structures.  Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \\\\\"parse\\\\\" source Documents into Nodes through our `NodeParser` classes.  For instance, you can do  ```python from llama_index.node_parser import SimpleNodeParser  parser = SimpleNodeParser.from_defaults()  nodes = parser.get_nodes_from_documents(documents) ```  You can also choose to construct Node objects manually and skip the first section. For instance,  ```python from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo  node1 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\") node2 = TextNode(text=\\\\\"\\\\\", id_=\\\\\"\\\\\")\", \"set relationships node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id) node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id) nodes = [node1, node2] ```  The `RelatedNodeInfo` class can also store additional `metadata` if needed:  ```python node2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\\\\\"key\\\\\": \\\\\"val\\\\\"}) ```\", \"3. Index Construction  We can now build an index over these Document objects. The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).  `from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex.from_documents(documents) ```  You can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex(nodes) ```  Depending on which index you use, LlamaIndex may make LLM calls in order to build the index.\", \"Reusing Nodes across Index Structures  If you have multiple Node objects defined, and wish to share these Node objects across multiple index structures, you can do that. Simply instantiate a StorageContext object, add the Node objects to the underlying DocumentStore, and pass the StorageContext around.  ```python from llama_index import StorageContext  storage_context = StorageContext.from_defaults() storage_context.docstore.add_documents(nodes)  index1 = VectorStoreIndex(nodes, storage_context=storage_context) index2 = ListIndex(nodes, storage_context=storage_context) ```  **NOTE**: If the `storage_context` argument isn\\'t specified, then it is implicitly created for each index during index construction. You can access the docstore associated with a given index through `index.storage_context`.\", \"Inserting Documents or Nodes  You can also take advantage of the `insert` capability of indices to insert Document objects one at a time instead of during index construction.  ```python from llama_index import VectorStoreIndex  index = VectorStoreIndex([]) for doc in documents:     index.insert(doc) ```  If you want to insert nodes on directly you can use `insert_nodes` function instead.  ```python from llama_index import VectorStoreIndex\", \"nodes: Sequence[Node] index = VectorStoreIndex([]) index.insert_nodes(nodes) ```  See the Document Management How-To for more details on managing documents and an example notebook.\", \"Customizing Documents  When creating documents, you can also attach useful metadata. Any metadata added to a document will be copied to the nodes that get created from their respective source document.  ```python document = Document(     text=\\'text\\',     metadata={         \\'filename\\': \\'\\',         \\'category\\': \\'\\'     } ) ```  More information and approaches to this are discussed in the section Customizing Documents.\", \"Customizing LLM\\'s  By default, we use OpenAI\\'s `text-davinci-003` model. You may choose to use another LLM when constructing an index.  ```python from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context from llama_index.llms import OpenAI  ...\", \"define LLM llm = OpenAI(model=\\\\\"gpt-4\\\\\", temperature=0, max_tokens=256)\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=52 request_id=a772bf490cc83d9267f3e54ea2d34f6e response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=52 request_id=a772bf490cc83d9267f3e54ea2d34f6e response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"configure service context service_context = ServiceContext.from_defaults(llm=llm) set_global_service_context(service_context)\", \"build index index = VectorStoreIndex.from_documents(     documents ) ```  To save costs, you may want to use a local model.  ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(llm=\\\\\"local\\\\\") ```  This will use llama2-chat-13B from with LlamaCPP, and assumes you have `llama-cpp-python` installed. Full LlamaCPP usage guide is available in a notebook here.  See the Custom LLM\\'s How-To for more details.\", \"Global ServiceContext  If you wanted the service context from the last section to always be the default, you can configure one like so:  ```python from llama_index import set_global_service_context set_global_service_context(service_context) ```  This service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.  For more details on the service context, including how to create a global service context, see the page Customizing the ServiceContext.\", \"Customizing Prompts  Depending on the index used, we used default prompt templates for constructing the index (and also insertion/querying). See Custom Prompts How-To for more details on how to customize your prompt.\", \"Customizing embeddings  For embedding-based indices, you can choose to pass in a custom embedding model. See Custom Embeddings How-To for more details.\", \"Cost Analysis  Creating an index, inserting to an index, and querying an index may use tokens. We can track token usage through the outputs of these operations. When running operations, the token usage will be printed.  You can also fetch the token usage through `TokenCountingCallback` handler. See Cost Analysis How-To for more details.\", \"[Optional] Save the index for future use  By default, data is stored in-memory. To persist to disk:  ```python index.storage_context.persist(persist_dir=\\\\\"\\\\\") ```  You may omit persist_dir to persist to `./storage` by default.  To reload from disk:  ```python from llama_index import StorageContext, load_index_from_storage\", \"rebuild storage context storage_context = StorageContext.from_defaults(persist_dir=\\\\\"\\\\\")\", \"load index index = load_index_from_storage(storage_context) ```  **NOTE**: If you had initialized the index with a custom `ServiceContext` object, you will also need to pass in the same ServiceContext during `load_index_from_storage` or ensure you have a global sevice context.  ```python  service_context = ServiceContext.from_defaults(llm=llm) set_global_service_context(service_context)\", \"when first building the index index = VectorStoreIndex.from_documents(     documents, # service_context=service_context -> optional if not using global )  ...\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"configure service context service_context = ServiceContext.from_defaults(llm=llm) set_global_service_context(service_context)\", \"build index index = VectorStoreIndex.from_documents(     documents ) ```  To save costs, you may want to use a local model.  ```python from llama_index import ServiceContext service_context = ServiceContext.from_defaults(llm=\\\\\"local\\\\\") ```  This will use llama2-chat-13B from with LlamaCPP, and assumes you have `llama-cpp-python` installed. Full LlamaCPP usage guide is available in a notebook here.  See the Custom LLM\\'s How-To for more details.\", \"Global ServiceContext  If you wanted the service context from the last section to always be the default, you can configure one like so:  ```python from llama_index import set_global_service_context set_global_service_context(service_context) ```  This service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.  For more details on the service context, including how to create a global service context, see the page Customizing the ServiceContext.\", \"Customizing Prompts  Depending on the index used, we used default prompt templates for constructing the index (and also insertion/querying). See Custom Prompts How-To for more details on how to customize your prompt.\", \"Customizing embeddings  For embedding-based indices, you can choose to pass in a custom embedding model. See Custom Embeddings How-To for more details.\", \"Cost Analysis  Creating an index, inserting to an index, and querying an index may use tokens. We can track token usage through the outputs of these operations. When running operations, the token usage will be printed.  You can also fetch the token usage through `TokenCountingCallback` handler. See Cost Analysis How-To for more details.\", \"[Optional] Save the index for future use  By default, data is stored in-memory. To persist to disk:  ```python index.storage_context.persist(persist_dir=\\\\\"\\\\\") ```  You may omit persist_dir to persist to `./storage` by default.  To reload from disk:  ```python from llama_index import StorageContext, load_index_from_storage\", \"rebuild storage context storage_context = StorageContext.from_defaults(persist_dir=\\\\\"\\\\\")\", \"load index index = load_index_from_storage(storage_context) ```  **NOTE**: If you had initialized the index with a custom `ServiceContext` object, you will also need to pass in the same ServiceContext during `load_index_from_storage` or ensure you have a global sevice context.  ```python  service_context = ServiceContext.from_defaults(llm=llm) set_global_service_context(service_context)\", \"when first building the index index = VectorStoreIndex.from_documents(     documents, # service_context=service_context -> optional if not using global )  ...\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=45 request_id=9db0f926afc312dc0d649414d1c5c5a3 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=45 request_id=9db0f926afc312dc0d649414d1c5c5a3 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"when loading the index from disk index = load_index_from_storage(     StorageContext.from_defaults(persist_dir=\\\\\"\\\\\")     # service_context=service_context -> optional if not using global )  ```\", \"4. [Optional, Advanced] Building indices on top of other indices  You can build indices on top of other indices! Composability gives you greater power in indexing your heterogeneous sources of data. For a discussion on relevant use cases, see our Query Use Cases. For technical details and examples, see our Composability How-To.\", \"5. Query the index.  After building the index, you can now query it with a `QueryEngine`. Note that a \\\\\"query\\\\\" is simply an input to an LLM - this means that you can use the index for question-answering, but you can also do more than that!\", \"High-level API  To start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:  ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response)  response = query_engine.query(\\\\\"Write an email to the user given their background information.\\\\\") print(response) ```\", \"Low-level API  We also support a low-level composition API that gives you more granular control over the query logic. Below we highlight a few of the possible customizations.  ```python from llama_index import (     VectorStoreIndex,     get_response_synthesizer, ) from llama_index.retrievers import VectorIndexRetriever from llama_index.query_engine import RetrieverQueryEngine from llama_index.indices.postprocessor import SimilarityPostprocessor\", \"build index index = VectorStoreIndex.from_documents(documents)\", \"configure retriever retriever = VectorIndexRetriever(     index=index,     similarity_top_k=2, )\", \"configure response synthesizer response_synthesizer = get_response_synthesizer()\", \"assemble query engine query_engine = RetrieverQueryEngine(     retriever=retriever,     response_synthesizer=response_synthesizer,     node_postprocessors=[         SimilarityPostprocessor(similarity_cutoff=0.7)     ]  )\", \"query response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```  You may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.  For a full list of implemented components and the supported configurations, please see the detailed reference docs.  In the following, we discuss some commonly used configurations in detail.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"when loading the index from disk index = load_index_from_storage(     StorageContext.from_defaults(persist_dir=\\\\\"\\\\\")     # service_context=service_context -> optional if not using global )  ```\", \"4. [Optional, Advanced] Building indices on top of other indices  You can build indices on top of other indices! Composability gives you greater power in indexing your heterogeneous sources of data. For a discussion on relevant use cases, see our Query Use Cases. For technical details and examples, see our Composability How-To.\", \"5. Query the index.  After building the index, you can now query it with a `QueryEngine`. Note that a \\\\\"query\\\\\" is simply an input to an LLM - this means that you can use the index for question-answering, but you can also do more than that!\", \"High-level API  To start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:  ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response)  response = query_engine.query(\\\\\"Write an email to the user given their background information.\\\\\") print(response) ```\", \"Low-level API  We also support a low-level composition API that gives you more granular control over the query logic. Below we highlight a few of the possible customizations.  ```python from llama_index import (     VectorStoreIndex,     get_response_synthesizer, ) from llama_index.retrievers import VectorIndexRetriever from llama_index.query_engine import RetrieverQueryEngine from llama_index.indices.postprocessor import SimilarityPostprocessor\", \"build index index = VectorStoreIndex.from_documents(documents)\", \"configure retriever retriever = VectorIndexRetriever(     index=index,     similarity_top_k=2, )\", \"configure response synthesizer response_synthesizer = get_response_synthesizer()\", \"assemble query engine query_engine = RetrieverQueryEngine(     retriever=retriever,     response_synthesizer=response_synthesizer,     node_postprocessors=[         SimilarityPostprocessor(similarity_cutoff=0.7)     ]  )\", \"query response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```  You may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.  For a full list of implemented components and the supported configurations, please see the detailed reference docs.  In the following, we discuss some commonly used configurations in detail.\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=101 request_id=1336269ad6517138243e7c68ea209ae7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=101 request_id=1336269ad6517138243e7c68ea209ae7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Configuring retriever  An index can have a variety of index-specific retrieval modes. For instance, a list index supports the default `ListIndexRetriever` that retrieves all nodes, and `ListIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.  For convienience, you can also use the following shorthand:  ```python     # ListIndexRetriever     retriever = index.as_retriever(retriever_mode=\\'default\\')     # ListIndexEmbeddingRetriever     retriever = index.as_retriever(retriever_mode=\\'embedding\\') ```  After choosing your desired retriever, you can construct your query engine:  ```python query_engine = RetrieverQueryEngine(retriever) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  The full list of retrievers for each index (and their shorthand) is documented in the Query Reference.  (setting-response-mode)=\", \"Configuring response synthesis After a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.  You can configure it via  ```python query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=) ```  Right now, we support the following options:  - `default`: \\\\\"create and refine\\\\\" an answer by sequentially going through each retrieved `Node`;   This makes a separate LLM call per Node. Good for more detailed answers. - `compact`: \\\\\"compact\\\\\" the prompt during each LLM call by stuffing as   many `Node` text chunks that can fit within the maximum prompt size. If there are   too many chunks to stuff in one prompt, \\\\\"create and refine\\\\\" an answer by going through   multiple prompts. - `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree   and return the root node as the response. Good for summarization purposes. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,   without actually sending them. Then can be inspected by checking `response.source_nodes`.   The response object is covered in more detail in Section 5. - `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text   chunk while accumulating the responses into an array. Returns a concatenated string of all   responses. Good for when you need to run the same query separately against each text   chunk.  ```python index = ListIndex.from_documents(documents) retriever = index.as_retriever()\", \"default query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'default\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"compact query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'compact\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"tree summarize query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'tree_summarize\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"no text query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'no_text\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```\", \"Configuring node postprocessors (i.e. filtering and augmentation)  We also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects. This can help reduce the time/number of LLM calls/cost or improve response quality.  For example:  - `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`. - `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers) - `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.  The full list of node postprocessors is documented in the Node Postprocessor Reference.  To configure the desired node postprocessors:  ```python node_postprocessors = [     KeywordNodePostprocessor(         required_keywords=[\\\\\"Combinator\\\\\"],         exclude_keywords=[\\\\\"Italy\\\\\"]     ) ] query_engine = RetrieverQueryEngine.from_args(     retriever, node_postprocessors=node_postprocessors ) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```\", \"5. Parsing the response  The object returned is a `Response` object. The object contains both the response text as well as the \\\\\"sources\\\\\" of the response:  ```python response = query_engine.query(\\\\\"\\\\\")\", \"get response str(response)\", \"get sources response.source_nodes\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Configuring retriever  An index can have a variety of index-specific retrieval modes. For instance, a list index supports the default `ListIndexRetriever` that retrieves all nodes, and `ListIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.  For convienience, you can also use the following shorthand:  ```python     # ListIndexRetriever     retriever = index.as_retriever(retriever_mode=\\'default\\')     # ListIndexEmbeddingRetriever     retriever = index.as_retriever(retriever_mode=\\'embedding\\') ```  After choosing your desired retriever, you can construct your query engine:  ```python query_engine = RetrieverQueryEngine(retriever) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```  The full list of retrievers for each index (and their shorthand) is documented in the Query Reference.  (setting-response-mode)=\", \"Configuring response synthesis After a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.  You can configure it via  ```python query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=) ```  Right now, we support the following options:  - `default`: \\\\\"create and refine\\\\\" an answer by sequentially going through each retrieved `Node`;   This makes a separate LLM call per Node. Good for more detailed answers. - `compact`: \\\\\"compact\\\\\" the prompt during each LLM call by stuffing as   many `Node` text chunks that can fit within the maximum prompt size. If there are   too many chunks to stuff in one prompt, \\\\\"create and refine\\\\\" an answer by going through   multiple prompts. - `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree   and return the root node as the response. Good for summarization purposes. - `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,   without actually sending them. Then can be inspected by checking `response.source_nodes`.   The response object is covered in more detail in Section 5. - `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text   chunk while accumulating the responses into an array. Returns a concatenated string of all   responses. Good for when you need to run the same query separately against each text   chunk.  ```python index = ListIndex.from_documents(documents) retriever = index.as_retriever()\", \"default query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'default\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"compact query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'compact\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"tree summarize query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'tree_summarize\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")\", \"no text query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\\'no_text\\') response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```\", \"Configuring node postprocessors (i.e. filtering and augmentation)  We also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects. This can help reduce the time/number of LLM calls/cost or improve response quality.  For example:  - `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`. - `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers) - `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.  The full list of node postprocessors is documented in the Node Postprocessor Reference.  To configure the desired node postprocessors:  ```python node_postprocessors = [     KeywordNodePostprocessor(         required_keywords=[\\\\\"Combinator\\\\\"],         exclude_keywords=[\\\\\"Italy\\\\\"]     ) ] query_engine = RetrieverQueryEngine.from_args(     retriever, node_postprocessors=node_postprocessors ) response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") ```\", \"5. Parsing the response  The object returned is a `Response` object. The object contains both the response text as well as the \\\\\"sources\\\\\" of the response:  ```python response = query_engine.query(\\\\\"\\\\\")\", \"get response str(response)\", \"get sources response.source_nodes\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=89 request_id=fa7d20e652c14ee611caf64a5efcece7 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=89 request_id=fa7d20e652c14ee611caf64a5efcece7 response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"formatted sources response.get_formatted_sources() ```  An example is shown below. !\", \"Use Cases  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/question_and_answer.md /end_to_end_tutorials/chatbots.md /end_to_end_tutorials/agents.md /end_to_end_tutorials/graphs.md /end_to_end_tutorials/structured_data.md /end_to_end_tutorials/apps.md /end_to_end_tutorials/privacy.md Finetuning Llama 2 for Text-to-SQL  Finetuning GPT-3.5 to Distill GPT-4  ```\", \"Data Connector Examples  Each of these notebooks showcase our readers which can read data from a variety of data sources.\", \"FAQ\", \"High-Level Concepts  ```{tip} If you haven\\'t, install and complete starter tutorial before you read this. It will make a lot more sense! ```  LlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.  In this high-level concepts guide, you will learn: * the retrieval augmented generation (RAG) paradigm for combining LLM with custom data, * key concepts and modules in LlamaIndex for composing your own RAG pipeline.\", \"Retrieval Augmented Generation (RAG) Retrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data. It generally consists of two stages:  1) **indexing stage**: preparing a knowledge base, and 2) **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question  !   LlamaIndex provides the essential toolkit for making both steps super easy. Let\\'s explore each stage in detail.\", \"Indexing Stage LlamaIndex help you prepare the knowledge base with a suite of data connectors and indexes. !   **Data Connectors**: A data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).  **Documents / Nodes**: A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \\\\\"chunk\\\\\" of a source `Document`. It\\'s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.  **Data Indexes**:  Once you\\'ve ingested your data, LlamaIndex will help you index the data into a format that\\'s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the VectorStoreIndex\", \"Querying Stage In the querying stage, the RAG pipeline retrieves the most relevant context given a user query, and pass that to the LLM (along with the query) to synthesize a response. This gives the LLM up-to-date knowledge that is not in its original training data, (also reducing hallucination). The key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.  LlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent. These building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.  !\", \"Building Blocks **Retrievers**:  A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.  **Node Postprocessors**: A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.   **Response Synthesizers**: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\", \"Pipelines  **Query Engines**: A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.   **Chat Engines**:  A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer).  **Agents**:  An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines.  The main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic. This gives it additional flexibility to tackle more complex tasks.  ```{admonition} Next Steps * tell me how to customize things. * curious about a specific module? Check out the module guides \\\\ud83d\\\\udc48 * have a use case in mind? Check out the end-to-end tutorials ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"formatted sources response.get_formatted_sources() ```  An example is shown below. !\", \"Use Cases  ```{toctree} --- maxdepth: 1 --- /end_to_end_tutorials/question_and_answer.md /end_to_end_tutorials/chatbots.md /end_to_end_tutorials/agents.md /end_to_end_tutorials/graphs.md /end_to_end_tutorials/structured_data.md /end_to_end_tutorials/apps.md /end_to_end_tutorials/privacy.md Finetuning Llama 2 for Text-to-SQL  Finetuning GPT-3.5 to Distill GPT-4  ```\", \"Data Connector Examples  Each of these notebooks showcase our readers which can read data from a variety of data sources.\", \"FAQ\", \"High-Level Concepts  ```{tip} If you haven\\'t, install and complete starter tutorial before you read this. It will make a lot more sense! ```  LlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.  In this high-level concepts guide, you will learn: * the retrieval augmented generation (RAG) paradigm for combining LLM with custom data, * key concepts and modules in LlamaIndex for composing your own RAG pipeline.\", \"Retrieval Augmented Generation (RAG) Retrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data. It generally consists of two stages:  1) **indexing stage**: preparing a knowledge base, and 2) **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question  !   LlamaIndex provides the essential toolkit for making both steps super easy. Let\\'s explore each stage in detail.\", \"Indexing Stage LlamaIndex help you prepare the knowledge base with a suite of data connectors and indexes. !   **Data Connectors**: A data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).  **Documents / Nodes**: A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \\\\\"chunk\\\\\" of a source `Document`. It\\'s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.  **Data Indexes**:  Once you\\'ve ingested your data, LlamaIndex will help you index the data into a format that\\'s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the VectorStoreIndex\", \"Querying Stage In the querying stage, the RAG pipeline retrieves the most relevant context given a user query, and pass that to the LLM (along with the query) to synthesize a response. This gives the LLM up-to-date knowledge that is not in its original training data, (also reducing hallucination). The key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.  LlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent. These building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.  !\", \"Building Blocks **Retrievers**:  A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.  **Node Postprocessors**: A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.   **Response Synthesizers**: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\", \"Pipelines  **Query Engines**: A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.   **Chat Engines**:  A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer).  **Agents**:  An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines.  The main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic. This gives it additional flexibility to tackle more complex tasks.  ```{admonition} Next Steps * tell me how to customize things. * curious about a specific module? Check out the module guides \\\\ud83d\\\\udc48 * have a use case in mind? Check out the end-to-end tutorials ```\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=61 request_id=442f6dcfc86468828d3f33db0f51b14a response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=61 request_id=442f6dcfc86468828d3f33db0f51b14a response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Customization Tutorial ====================== .. tip::     If you haven\\'t, `install <installation.html>`_, complete `starter tutorial <starter_example.html>`_, and learn the `high-level concepts <concepts.html>`_ before you read this.It will make a lot more sense!In this tutorial, we show the most common customizations with the `starter example <starter_example.html>`_:  .. code-block:: python      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to parse my documents into smaller chunks\\\\\"**  .. code-block:: python      from llama_index import ServiceContext     service_context = ServiceContext.from_defaults(chunk_size=1000)  .. tip::     `ServiceContext` is a bundle of services and configurations used across a LlamaIndex pipeline,     Learn more `here <../core_modules/supporting_modules/service_context.html>`_... code-block:: python     :emphasize-lines: 4      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents, service_context=service_context)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to use a different vector store\\\\\"**  .. code-block:: python      import chromadb     from llama_index.vector_stores import ChromaVectorStore     from llama_index import StorageContext      chroma_client = chromadb.PersistentClient()     chroma_collection = chroma_client.create_collection(\\\\\"quickstart\\\\\")     vector_store = ChromaVectorStore(chroma_collection=chroma_collection)     storage_context = StorageContext.from_defaults(vector_store=vector_store)  .. tip::     `StorageContext` defines the storage backend for where the documents, embeddings, and indexes are stored.Learn more `here <../core_modules/data_modules/storage/customization.html>`_... code-block:: python     :emphasize-lines: 4      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to retrieve more context when I query\\\\\"**  .. code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(similarity_top_k=5)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  .. tip::     `as_query_engine` builds a default retriever and query engine on top of the index.You can configure the retriever and query engine by passing in keyword arguments.Here, we configure the retriever to return the top 5 most similar documents (instead of the default of 2).Learn more about vector index `here <../core_modules/data_modules/index/vector_store_guide.html>`_.\", \"-----------------  **\\\\\"I want to use a different LLM\\\\\"**  .. code-block:: python      from llama_index import ServiceContext     from llama_index.llms import PaLM     service_context = ServiceContext.from_defaults(llm=PaLM())  .. tip::     Learn more about customizing LLMs `here <../core_modules/model_modules/llms/usage_custom.html>`_... code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(service_context=service_context)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to use a different response mode\\\\\"**   .. code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(response_mode=\\'tree_summarize\\')     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")     print(response)  .. tip::     Learn more about query engine usage pattern `here <../core_modules/query_modules/query_engine/usage_pattern.html>`_ and available response modes `here <../core_modules/query_modules/query_engine/response_modes.html>`_.  -----------------  **\\\\\"I want to stream the response back\\\\\"**   .. code-block:: python     :emphasize-lines: 5, 7      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(streaming=True)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")     response.print_response_stream()  .. tip::     Learn more about streaming `here <../core_modules/query_modules/query_engine/streaming.html>`_.  -----------------  **\\\\\"I want a chatbot instead of Q&A\\\\\"**  .. code-block:: python     :emphasize-lines: 5, 6, 9      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_chat_engine()     response = query_engine.chat(\\\\\"What did the author do growing up?\\\\\")     print(response)      response = query_engine.chat(\\\\\"Oh interesting, tell me more.\\\\\")     print(response)  .. tip::     Learn more about chat engine usage pattern `here <../core_modules/query_modules/chat_engines/usage_pattern.html>`_.  -----------------  .. admonition:: Next Steps      * want a thorough walkthrough of (almost) everything you can configure? Try the `end-to-end tutorial on basic usage pattern <../end_to_end_tutorials/usage_pattern.html>`_.     * want more in-depth understanding of specific modules? Check out the module guides \\\\ud83d\\\\udc48\", \"Installation and Setup\", \"Installation from Pip  You can simply do:  ``` pip install llama-index ```  **NOTE:** LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ...). Use the environment variable \\\\\"LLAMA_INDEX_CACHE_DIR\\\\\" to control where these files are saved.\", \"Installation from Source  Git clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:  - `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself. - `pip install -r requirements.txt` if you want to install optional dependencies + dependencies used for development (e.g. unit testing).\", \"OpenAI Environment Setup  By default, we use the OpenAI `gpt-3.5-turbo` model for text generation and `text-embedding-ada-002` for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY setup. You can register an API key by logging into OpenAI\\'s page and creating a new API token.  ```{tip} You can also customize the underlying LLM. You may need additional environment keys + tokens setup depending on the LLM provider. ```\", \"Local Environment Setup  If you don\\'t wish to use OpenAI, the environment will automatically fallback to using `LlamaCPP` and `llama2-chat-13B` for text generation and `BAAI/bge-small-en` for retrieval and embeddings. This models will all run locally.  In order to use `LlamaCPP`, follow the installation guide here. You\\'ll need to install the `llama-cpp-python` package, preferably compiled to support your GPU. This will use aronund 11.5GB of memory across the CPU and GPU.  In order to use the local embeddings, simply run `pip install sentence-transformers`. The local embedding model uses about 500MB of memory.\", \"Starter Tutorial  ```{tip} Make sure you\\'ve followed the installation steps first. ``` Here is a starter example for using LlamaIndex.\", \"Download  LlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository. We first want to download this `examples` folder. An easy way to do this is to just clone the repo:  ```bash $ git clone https://github.com/jerryjliu/llama_index.git ```  Next, navigate to your newly-cloned repository, and verify the contents:  ```bash $ cd llama_index $ ls LICENSE                data_requirements.txt  tests/ MANIFEST.in            examples/              pyproject.toml Makefile               experimental/          requirements.txt README.md              llama_index/             setup.py ```  We now want to navigate to the following folder:  ```bash $ cd examples/paul_graham_essay ```  This contains LlamaIndex examples around Paul Graham\\'s essay, \\\\\"What I Worked On\\\\\". A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.\", \"Build and Query Index  Create a new `.py` file with the following:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) ```  This builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following  ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```  You should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Customization Tutorial ====================== .. tip::     If you haven\\'t, `install <installation.html>`_, complete `starter tutorial <starter_example.html>`_, and learn the `high-level concepts <concepts.html>`_ before you read this.It will make a lot more sense!In this tutorial, we show the most common customizations with the `starter example <starter_example.html>`_:  .. code-block:: python      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to parse my documents into smaller chunks\\\\\"**  .. code-block:: python      from llama_index import ServiceContext     service_context = ServiceContext.from_defaults(chunk_size=1000)  .. tip::     `ServiceContext` is a bundle of services and configurations used across a LlamaIndex pipeline,     Learn more `here <../core_modules/supporting_modules/service_context.html>`_... code-block:: python     :emphasize-lines: 4      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents, service_context=service_context)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to use a different vector store\\\\\"**  .. code-block:: python      import chromadb     from llama_index.vector_stores import ChromaVectorStore     from llama_index import StorageContext      chroma_client = chromadb.PersistentClient()     chroma_collection = chroma_client.create_collection(\\\\\"quickstart\\\\\")     vector_store = ChromaVectorStore(chroma_collection=chroma_collection)     storage_context = StorageContext.from_defaults(vector_store=vector_store)  .. tip::     `StorageContext` defines the storage backend for where the documents, embeddings, and indexes are stored.Learn more `here <../core_modules/data_modules/storage/customization.html>`_... code-block:: python     :emphasize-lines: 4      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)     query_engine = index.as_query_engine()     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to retrieve more context when I query\\\\\"**  .. code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(similarity_top_k=5)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  .. tip::     `as_query_engine` builds a default retriever and query engine on top of the index.You can configure the retriever and query engine by passing in keyword arguments.Here, we configure the retriever to return the top 5 most similar documents (instead of the default of 2).Learn more about vector index `here <../core_modules/data_modules/index/vector_store_guide.html>`_.\", \"-----------------  **\\\\\"I want to use a different LLM\\\\\"**  .. code-block:: python      from llama_index import ServiceContext     from llama_index.llms import PaLM     service_context = ServiceContext.from_defaults(llm=PaLM())  .. tip::     Learn more about customizing LLMs `here <../core_modules/model_modules/llms/usage_custom.html>`_... code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(service_context=service_context)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")print(response)  -----------------  **\\\\\"I want to use a different response mode\\\\\"**   .. code-block:: python     :emphasize-lines: 5      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(response_mode=\\'tree_summarize\\')     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")     print(response)  .. tip::     Learn more about query engine usage pattern `here <../core_modules/query_modules/query_engine/usage_pattern.html>`_ and available response modes `here <../core_modules/query_modules/query_engine/response_modes.html>`_.  -----------------  **\\\\\"I want to stream the response back\\\\\"**   .. code-block:: python     :emphasize-lines: 5, 7      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_query_engine(streaming=True)     response = query_engine.query(\\\\\"What did the author do growing up?\\\\\")     response.print_response_stream()  .. tip::     Learn more about streaming `here <../core_modules/query_modules/query_engine/streaming.html>`_.  -----------------  **\\\\\"I want a chatbot instead of Q&A\\\\\"**  .. code-block:: python     :emphasize-lines: 5, 6, 9      from llama_index import VectorStoreIndex, SimpleDirectoryReader      documents = SimpleDirectoryReader(\\'data\\').load_data()     index = VectorStoreIndex.from_documents(documents)     query_engine = index.as_chat_engine()     response = query_engine.chat(\\\\\"What did the author do growing up?\\\\\")     print(response)      response = query_engine.chat(\\\\\"Oh interesting, tell me more.\\\\\")     print(response)  .. tip::     Learn more about chat engine usage pattern `here <../core_modules/query_modules/chat_engines/usage_pattern.html>`_.  -----------------  .. admonition:: Next Steps      * want a thorough walkthrough of (almost) everything you can configure? Try the `end-to-end tutorial on basic usage pattern <../end_to_end_tutorials/usage_pattern.html>`_.     * want more in-depth understanding of specific modules? Check out the module guides \\\\ud83d\\\\udc48\", \"Installation and Setup\", \"Installation from Pip  You can simply do:  ``` pip install llama-index ```  **NOTE:** LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ...). Use the environment variable \\\\\"LLAMA_INDEX_CACHE_DIR\\\\\" to control where these files are saved.\", \"Installation from Source  Git clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:  - `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself. - `pip install -r requirements.txt` if you want to install optional dependencies + dependencies used for development (e.g. unit testing).\", \"OpenAI Environment Setup  By default, we use the OpenAI `gpt-3.5-turbo` model for text generation and `text-embedding-ada-002` for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY setup. You can register an API key by logging into OpenAI\\'s page and creating a new API token.  ```{tip} You can also customize the underlying LLM. You may need additional environment keys + tokens setup depending on the LLM provider. ```\", \"Local Environment Setup  If you don\\'t wish to use OpenAI, the environment will automatically fallback to using `LlamaCPP` and `llama2-chat-13B` for text generation and `BAAI/bge-small-en` for retrieval and embeddings. This models will all run locally.  In order to use `LlamaCPP`, follow the installation guide here. You\\'ll need to install the `llama-cpp-python` package, preferably compiled to support your GPU. This will use aronund 11.5GB of memory across the CPU and GPU.  In order to use the local embeddings, simply run `pip install sentence-transformers`. The local embedding model uses about 500MB of memory.\", \"Starter Tutorial  ```{tip} Make sure you\\'ve followed the installation steps first. ``` Here is a starter example for using LlamaIndex.\", \"Download  LlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository. We first want to download this `examples` folder. An easy way to do this is to just clone the repo:  ```bash $ git clone https://github.com/jerryjliu/llama_index.git ```  Next, navigate to your newly-cloned repository, and verify the contents:  ```bash $ cd llama_index $ ls LICENSE                data_requirements.txt  tests/ MANIFEST.in            examples/              pyproject.toml Makefile               experimental/          requirements.txt README.md              llama_index/             setup.py ```  We now want to navigate to the following folder:  ```bash $ cd examples/paul_graham_essay ```  This contains LlamaIndex examples around Paul Graham\\'s essay, \\\\\"What I Worked On\\\\\". A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.\", \"Build and Query Index  Create a new `.py` file with the following:  ```python from llama_index import VectorStoreIndex, SimpleDirectoryReader  documents = SimpleDirectoryReader(\\'data\\').load_data() index = VectorStoreIndex.from_documents(documents) ```  This builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following  ```python query_engine = index.as_query_engine() response = query_engine.query(\\\\\"What did the author do growing up?\\\\\") print(response) ```  You should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=277 request_id=ca86b1d0a9b132dde81b990af577501d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=277 request_id=ca86b1d0a9b132dde81b990af577501d response_code=200\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"Viewing Queries and Events Using Logging  In a Jupyter notebook, you can view info and/or debugging logging using the following snippet:  ```python import logging import sys  logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) ```  You can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\", \"Saving and Loading  By default, data is stored in-memory. To persist to disk (under `./storage`):  ```python index.storage_context.persist() ```  To reload from disk: ```python from llama_index import StorageContext, load_index_from_storage\", \"rebuild storage context storage_context = StorageContext.from_defaults(persist_dir=\\\\\"./storage\\\\\")\", \"load index index = load_index_from_storage(storage_context) ```    ```{admonition} Next Steps * learn more about the high-level concepts. * tell me how to customize things. * curious about a specific module? check out the guides \\\\ud83d\\\\udc48 * have a use case in mind? check out the end-to-end tutorials ```\", \".. LlamaIndex documentation master file, created by    sphinx-quickstart on Sun Dec 11 14:30:34 2022.You can adapt this file completely to your liking, but it should at least    contain the root `toctree` directive.Welcome to LlamaIndex \\\\ud83e\\\\udd99 !##########################  LlamaIndex (formerly GPT Index) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data.\\\\ud83d\\\\ude80 Why LlamaIndex?******************  At their core, LLMs offer a natural language interface between humans and inferred data.Widely available models come pre-trained on huge amounts of publicly available data, from Wikipedia and mailing lists to textbooks and source code.Applications built on top of LLMs often require augmenting these models with private or domain-specific data.Unfortunately, that data can be distributed across siloed applications and data stores.It\\'s behind APIs, in SQL databases, or trapped in PDFs and slide decks.That\\'s where **LlamaIndex** comes in.\\\\ud83e\\\\udd99 How can LlamaIndex help?***************************  LlamaIndex provides the following tools:  - **Data connectors** ingest your existing data from their native source and format.These could be APIs, PDFs, SQL, and (much) more.- **Data indexes** structure your data in intermediate representations that are easy and performant for LLMs to consume.- **Engines** provide natural language access to your data.For example:    - Query engines are powerful retrieval interfaces for knowledge-augmented output.- Chat engines are conversational interfaces for multi-message, \\\\\"back and forth\\\\\" interactions with your data.- **Data agents** are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.- **Application integrations** tie LlamaIndex back into the rest of your ecosystem.This could be LangChain, Flask, Docker, ChatGPT, or\\\\u2026 anything else!\\\\ud83d\\\\udc68\\\\u200d\\\\ud83d\\\\udc69\\\\u200d\\\\ud83d\\\\udc67\\\\u200d\\\\ud83d\\\\udc66 Who is LlamaIndex for?*************************  LlamaIndex provides tools for beginners, advanced users, and everyone in between.Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.For more complex applications, our lower-level APIs allow advanced users to customize and extend any module\\\\u2014data connectors, indices, retrievers, query engines, reranking modules\\\\u2014to fit their needs.Getting Started **************** ``pip install llama-index``  Our documentation includes detailed `Installation Instructions <./getting_started/installation.html>`_ and a `Starter Tutorial <./getting_started/starter_example.html>`_ to build your first application (in five lines of code!)Once you\\'re up and running, `High-Level Concepts <./getting_started/concepts.html>`_ has an overview of LlamaIndex\\'s modular architecture.For more hands-on practical examples, look through our `End-to-End Tutorials <./end_to_end_tutorials/use_cases.html>`_ or learn how to `customize <./getting_started/customization.html>`_ components to fit your specific needs.**NOTE**: We have a Typescript package too!`Repo <https://github.com/run-llama/LlamaIndexTS>`_, `Docs <https://ts.llamaindex.ai/>`_  \\\\ud83d\\\\uddfa\\\\ufe0f Ecosystem ************  To download or contribute, find LlamaIndex on:  - Github: https://github.com/jerryjliu/llama_index - PyPi:    - LlamaIndex: https://pypi.org/project/llama-index/.- GPT Index (duplicate): https://pypi.org/project/gpt-index/.- NPM (Typescript/Javascript):    - Github: https://github.com/run-llama/LlamaIndexTS    - Docs: https://ts.llamaindex.ai/    - LlamaIndex.TS: https://www.npmjs.com/package/llamaindex  Community --------- Need help?Have a feature suggestion?Join the LlamaIndex community:  - Twitter: https://twitter.com/llama_index - Discord https://discord.gg/dGcwcsnxhU  Associated projects -------------------  - \\\\ud83c\\\\udfe1 LlamaHub: https://llamahub.ai | A large (and growing!)\", \"collection of custom data connectors - \\\\ud83e\\\\uddea LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex  .. toctree::    :maxdepth: 1    :caption: Getting Started    :hidden:     getting_started/installation.md    getting_started/starter_example.md    getting_started/concepts.md    getting_started/customization.rst  .. toctree::    :maxdepth: 2    :caption: End-to-End Tutorials    :hidden:     end_to_end_tutorials/usage_pattern.md    end_to_end_tutorials/one_click_observability.md    end_to_end_tutorials/principled_dev_practices.md    end_to_end_tutorials/discover_llamaindex.md    end_to_end_tutorials/finetuning.md    end_to_end_tutorials/use_cases.md     .. toctree::    :maxdepth: 1    :caption: Index/Data Modules    :hidden:     core_modules/data_modules/connector/root.md    core_modules/data_modules/documents_and_nodes/root.md    core_modules/data_modules/node_parsers/root.md    core_modules/data_modules/storage/root.md    core_modules/data_modules/index/root.md  .. toctree::    :maxdepth: 1    :caption: Query Modules    :hidden:     core_modules/query_modules/query_engine/root.md    core_modules/query_modules/chat_engines/root.md    core_modules/query_modules/retriever/root.md    core_modules/query_modules/router/root.md    core_modules/query_modules/node_postprocessors/root.md    core_modules/query_modules/response_synthesizers/root.md    core_modules/query_modules/structured_outputs/root.md  .. toctree::    :maxdepth: 1    :caption: Agent Modules    :hidden:     core_modules/agent_modules/agents/root.md    core_modules/agent_modules/tools/root.md  .. toctree::    :maxdepth: 1    :caption: Model Modules    :hidden:     core_modules/model_modules/llms/root.md    core_modules/model_modules/embeddings/root.md    core_modules/model_modules/prompts.md  .. toctree::    :maxdepth: 1    :caption: Supporting Modules    :hidden:     core_modules/supporting_modules/service_context.md    core_modules/supporting_modules/callbacks/root.md    core_modules/supporting_modules/evaluation/root.md    core_modules/supporting_modules/cost_analysis/root.md    core_modules/supporting_modules/playground/root.md  .. toctree::    :maxdepth: 2    :caption: Development    :hidden:     development/contributing.rst    development/documentation.rst    development/privacy.md    development/changelog.rst  .. toctree::    :maxdepth: 2    :caption: Community    :hidden:     community/integrations.md    community/app_showcase.md  .. toctree::    :maxdepth: 1    :caption: API Reference    :hidden:     api_reference/index.rst  .. toctree::    :maxdepth: 1    :hidden:     deprecated_terms.md\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"Viewing Queries and Events Using Logging  In a Jupyter notebook, you can view info and/or debugging logging using the following snippet:  ```python import logging import sys  logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout)) ```  You can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\", \"Saving and Loading  By default, data is stored in-memory. To persist to disk (under `./storage`):  ```python index.storage_context.persist() ```  To reload from disk: ```python from llama_index import StorageContext, load_index_from_storage\", \"rebuild storage context storage_context = StorageContext.from_defaults(persist_dir=\\\\\"./storage\\\\\")\", \"load index index = load_index_from_storage(storage_context) ```    ```{admonition} Next Steps * learn more about the high-level concepts. * tell me how to customize things. * curious about a specific module? check out the guides \\\\ud83d\\\\udc48 * have a use case in mind? check out the end-to-end tutorials ```\", \".. LlamaIndex documentation master file, created by    sphinx-quickstart on Sun Dec 11 14:30:34 2022.You can adapt this file completely to your liking, but it should at least    contain the root `toctree` directive.Welcome to LlamaIndex \\\\ud83e\\\\udd99 !##########################  LlamaIndex (formerly GPT Index) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data.\\\\ud83d\\\\ude80 Why LlamaIndex?******************  At their core, LLMs offer a natural language interface between humans and inferred data.Widely available models come pre-trained on huge amounts of publicly available data, from Wikipedia and mailing lists to textbooks and source code.Applications built on top of LLMs often require augmenting these models with private or domain-specific data.Unfortunately, that data can be distributed across siloed applications and data stores.It\\'s behind APIs, in SQL databases, or trapped in PDFs and slide decks.That\\'s where **LlamaIndex** comes in.\\\\ud83e\\\\udd99 How can LlamaIndex help?***************************  LlamaIndex provides the following tools:  - **Data connectors** ingest your existing data from their native source and format.These could be APIs, PDFs, SQL, and (much) more.- **Data indexes** structure your data in intermediate representations that are easy and performant for LLMs to consume.- **Engines** provide natural language access to your data.For example:    - Query engines are powerful retrieval interfaces for knowledge-augmented output.- Chat engines are conversational interfaces for multi-message, \\\\\"back and forth\\\\\" interactions with your data.- **Data agents** are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.- **Application integrations** tie LlamaIndex back into the rest of your ecosystem.This could be LangChain, Flask, Docker, ChatGPT, or\\\\u2026 anything else!\\\\ud83d\\\\udc68\\\\u200d\\\\ud83d\\\\udc69\\\\u200d\\\\ud83d\\\\udc67\\\\u200d\\\\ud83d\\\\udc66 Who is LlamaIndex for?*************************  LlamaIndex provides tools for beginners, advanced users, and everyone in between.Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.For more complex applications, our lower-level APIs allow advanced users to customize and extend any module\\\\u2014data connectors, indices, retrievers, query engines, reranking modules\\\\u2014to fit their needs.Getting Started **************** ``pip install llama-index``  Our documentation includes detailed `Installation Instructions <./getting_started/installation.html>`_ and a `Starter Tutorial <./getting_started/starter_example.html>`_ to build your first application (in five lines of code!)Once you\\'re up and running, `High-Level Concepts <./getting_started/concepts.html>`_ has an overview of LlamaIndex\\'s modular architecture.For more hands-on practical examples, look through our `End-to-End Tutorials <./end_to_end_tutorials/use_cases.html>`_ or learn how to `customize <./getting_started/customization.html>`_ components to fit your specific needs.**NOTE**: We have a Typescript package too!`Repo <https://github.com/run-llama/LlamaIndexTS>`_, `Docs <https://ts.llamaindex.ai/>`_  \\\\ud83d\\\\uddfa\\\\ufe0f Ecosystem ************  To download or contribute, find LlamaIndex on:  - Github: https://github.com/jerryjliu/llama_index - PyPi:    - LlamaIndex: https://pypi.org/project/llama-index/.- GPT Index (duplicate): https://pypi.org/project/gpt-index/.- NPM (Typescript/Javascript):    - Github: https://github.com/run-llama/LlamaIndexTS    - Docs: https://ts.llamaindex.ai/    - LlamaIndex.TS: https://www.npmjs.com/package/llamaindex  Community --------- Need help?Have a feature suggestion?Join the LlamaIndex community:  - Twitter: https://twitter.com/llama_index - Discord https://discord.gg/dGcwcsnxhU  Associated projects -------------------  - \\\\ud83c\\\\udfe1 LlamaHub: https://llamahub.ai | A large (and growing!)\", \"collection of custom data connectors - \\\\ud83e\\\\uddea LlamaLab: https://github.com/run-llama/llama-lab | Ambitious projects built on top of LlamaIndex  .. toctree::    :maxdepth: 1    :caption: Getting Started    :hidden:     getting_started/installation.md    getting_started/starter_example.md    getting_started/concepts.md    getting_started/customization.rst  .. toctree::    :maxdepth: 2    :caption: End-to-End Tutorials    :hidden:     end_to_end_tutorials/usage_pattern.md    end_to_end_tutorials/one_click_observability.md    end_to_end_tutorials/principled_dev_practices.md    end_to_end_tutorials/discover_llamaindex.md    end_to_end_tutorials/finetuning.md    end_to_end_tutorials/use_cases.md     .. toctree::    :maxdepth: 1    :caption: Index/Data Modules    :hidden:     core_modules/data_modules/connector/root.md    core_modules/data_modules/documents_and_nodes/root.md    core_modules/data_modules/node_parsers/root.md    core_modules/data_modules/storage/root.md    core_modules/data_modules/index/root.md  .. toctree::    :maxdepth: 1    :caption: Query Modules    :hidden:     core_modules/query_modules/query_engine/root.md    core_modules/query_modules/chat_engines/root.md    core_modules/query_modules/retriever/root.md    core_modules/query_modules/router/root.md    core_modules/query_modules/node_postprocessors/root.md    core_modules/query_modules/response_synthesizers/root.md    core_modules/query_modules/structured_outputs/root.md  .. toctree::    :maxdepth: 1    :caption: Agent Modules    :hidden:     core_modules/agent_modules/agents/root.md    core_modules/agent_modules/tools/root.md  .. toctree::    :maxdepth: 1    :caption: Model Modules    :hidden:     core_modules/model_modules/llms/root.md    core_modules/model_modules/embeddings/root.md    core_modules/model_modules/prompts.md  .. toctree::    :maxdepth: 1    :caption: Supporting Modules    :hidden:     core_modules/supporting_modules/service_context.md    core_modules/supporting_modules/callbacks/root.md    core_modules/supporting_modules/evaluation/root.md    core_modules/supporting_modules/cost_analysis/root.md    core_modules/supporting_modules/playground/root.md  .. toctree::    :maxdepth: 2    :caption: Development    :hidden:     development/contributing.rst    development/documentation.rst    development/privacy.md    development/changelog.rst  .. toctree::    :maxdepth: 2    :caption: Community    :hidden:     community/integrations.md    community/app_showcase.md  .. toctree::    :maxdepth: 1    :caption: API Reference    :hidden:     api_reference/index.rst  .. toctree::    :maxdepth: 1    :hidden:     deprecated_terms.md\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=90 request_id=edee00bb7ee708e594a151a75375b5f1 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=90 request_id=edee00bb7ee708e594a151a75375b5f1 response_code=200\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "system_prompt = '''You are an senior software developer. \n",
    "your job is to answer technical questions based on Llama-index's documentation. \n",
    "Keep your answers technical and based on the given facts – do not hallucinate features.'''\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0, system_prompt=system_prompt))\n",
    "index = VectorStoreIndex.from_documents(lidocs, service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4b953f-a49f-418d-8779-57a633d9c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to files\n",
    "index.storage_context.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
